{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e96288",
   "metadata": {},
   "source": [
    "# RQ6 ‚Äî Decoder Dynamics as Epistemic Uncertainty Signals\n",
    "\n",
    "**Research Question**: ¬øQu√© propiedades intr√≠nsecas de la din√°mica del decoder transformer codifican incertidumbre epist√©mica en OVD, y cu√°ndo la varianza inter-capa sirve de proxy confiable para la incertidumbre del modelo?\n",
    "\n",
    "**Hip√≥tesis**: La incertidumbre se alinea m√°s con los errores conforme aumenta la profundidad: las predicciones TP se estabilizan antes que las FP; la varianza en capas tard√≠as separa mejor los errores y mejora el AUROC de detecci√≥n de errores.\n",
    "\n",
    "**Expected Results**:\n",
    "- **Figure RQ6.1**: Varianza inter-capa de bounding-box por profundidad del decoder para TP vs FP\n",
    "- **Figure RQ6.2**: AUROC de detecci√≥n de errores en funci√≥n de la capa del decoder\n",
    "- **Table RQ6.1**: Diagn√≥sticos de efectividad de incertidumbre por capa\n",
    "- **Table RQ6.2**: Condiciones de falla donde la varianza inter-capa se vuelve menos predictiva\n",
    "\n",
    "**Nota importante**: Este notebook utiliza el modelo GroundingDINO entrenado y evaluado en las fases anteriores del proyecto. Los resultados son reales, no simulados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a22659",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de paths relativos (desde New_RQ/new_rq6/)\n",
    "BASE_DIR = Path('../..')  # Subir dos niveles hasta el root del proyecto\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'iou_matching': 0.5,\n",
    "    'conf_threshold': 0.25,\n",
    "    'num_layers': 6,  # GroundingDINO tiene 6 capas en el decoder transformer\n",
    "    'sample_size': 500  # N√∫mero de im√°genes del dataset BDD100K a procesar\n",
    "}\n",
    "\n",
    "# Semillas para reproducibilidad\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n cargada\")\n",
    "print(f\"   Device: {CONFIG['device']}\")\n",
    "print(f\"   Output: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"   Data:   {DATA_DIR.absolute()}\")\n",
    "print(f\"   Categor√≠as: {len(CONFIG['categories'])}\")\n",
    "print(f\"   Sample size: {CONFIG['sample_size']} im√°genes\")\n",
    "\n",
    "# Guardar configuraci√≥n\n",
    "with open(OUTPUT_DIR / 'config_rq6.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "print(f\"‚úÖ Configuraci√≥n guardada en {OUTPUT_DIR / 'config_rq6.yaml'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d39c5",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo GroundingDINO con Hooks para Capturar Capas del Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b53789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR ESTA CELDA PARA RQ6 - Cargar modelo GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "\n",
    "# Rutas del modelo (usar rutas absolutas como en todas las fases)\n",
    "model_config = '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_weights = '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"   CARGANDO MODELO GROUNDINGDINO PARA CAPTURAR DECODER LAYERS\")\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"\\nüîÑ Cargando modelo...\")\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"‚úÖ Prompt: {TEXT_PROMPT}\")\n",
    "print(f\"‚úÖ Arquitectura: GroundingDINO SwinT-OGC\\n\")\n",
    "\n",
    "# Identificar capas del decoder transformer\n",
    "print(\"üîç Identificando capas del decoder transformer...\")\n",
    "decoder_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    # Buscar capas del decoder: transformer.decoder.layers.0, transformer.decoder.layers.1, etc.\n",
    "    if 'decoder.layers' in name and name.count('.') == 3:\n",
    "        layer_num = name.split('.')[-1]\n",
    "        if layer_num.isdigit():\n",
    "            decoder_layers.append((int(layer_num), name, module))\n",
    "\n",
    "decoder_layers.sort(key=lambda x: x[0])\n",
    "print(f\"‚úÖ Capas del decoder encontradas: {len(decoder_layers)}\\n\")\n",
    "for layer_idx, layer_name, _ in decoder_layers:\n",
    "    print(f\"   Capa {layer_idx}: {layer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a6e1",
   "metadata": {},
   "source": [
    "## 3. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d782d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(label):\n",
    "    \"\"\"Normaliza etiquetas del modelo a categor√≠as del dataset\"\"\"\n",
    "    synonyms = {\n",
    "        'bike': 'bicycle', \n",
    "        'motorbike': 'motorcycle', \n",
    "        'pedestrian': 'person',\n",
    "        'stop sign': 'traffic sign', \n",
    "        'red light': 'traffic light'\n",
    "    }\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Calcula IoU entre dos bounding boxes en formato [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    \n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def match_predictions_to_gt(predictions, gt_annotations, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Matchea predicciones con ground truth usando IoU\n",
    "    Retorna: lista de (pred, gt, is_correct, iou)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    used_gt = set()\n",
    "    \n",
    "    # Ordenar predicciones por score descendente\n",
    "    predictions_sorted = sorted(predictions, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for pred in predictions_sorted:\n",
    "        best_iou = 0\n",
    "        best_gt = None\n",
    "        best_gt_idx = None\n",
    "        \n",
    "        for gt_idx, gt in enumerate(gt_annotations):\n",
    "            if gt_idx in used_gt:\n",
    "                continue\n",
    "            \n",
    "            # Verificar que sean de la misma categor√≠a\n",
    "            if pred['category_id'] != gt['category_id']:\n",
    "                continue\n",
    "            \n",
    "            # Calcular IoU\n",
    "            iou = compute_iou(pred['bbox'], gt['bbox'])\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt = gt\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        # Determinar si es correcto (TP o FP)\n",
    "        is_correct = best_iou >= iou_threshold\n",
    "        \n",
    "        if is_correct:\n",
    "            used_gt.add(best_gt_idx)\n",
    "        \n",
    "        matches.append({\n",
    "            'pred': pred,\n",
    "            'gt': best_gt,\n",
    "            'is_correct': is_correct,\n",
    "            'iou': best_iou\n",
    "        })\n",
    "    \n",
    "    return matches\n",
    "\n",
    "print(\"‚úÖ Funciones auxiliares definidas:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f301aa",
   "metadata": {},
   "source": [
    "## 4. Inferencia con Captura de Embeddings por Capa del Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ced9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR ESTA CELDA PARA RQ6 - Funci√≥n de inferencia con captura de capas\n",
    "\n",
    "def inference_with_layer_capture(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"\n",
    "    Ejecuta inferencia capturando embeddings de cada capa del decoder transformer.\n",
    "    \n",
    "    Esta funci√≥n usa hooks para capturar las salidas intermedias de cada capa\n",
    "    del decoder, permitiendo analizar c√≥mo evoluciona la representaci√≥n a trav√©s\n",
    "    de las capas del transformer.\n",
    "    \n",
    "    Returns:\n",
    "        detections: Lista de detecciones con varianzas inter-capa calculadas\n",
    "        layer_embeddings: Diccionario con embeddings capturados por capa\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Diccionario para almacenar embeddings por capa\n",
    "    layer_embeddings = {}\n",
    "    \n",
    "    def create_hook(layer_idx):\n",
    "        def hook_fn(module, input, output):\n",
    "            # Capturar el embedding de salida de esta capa\n",
    "            # output es t√≠picamente (tgt, memory) o similar\n",
    "            if isinstance(output, tuple):\n",
    "                # Tomar el primer elemento (embeddings de queries)\n",
    "                emb = output[0].detach().cpu()\n",
    "            else:\n",
    "                emb = output.detach().cpu()\n",
    "            layer_embeddings[layer_idx] = emb\n",
    "        return hook_fn\n",
    "    \n",
    "    # Registrar hooks en cada capa del decoder\n",
    "    hooks = []\n",
    "    for layer_idx, layer_name, module in decoder_layers:\n",
    "        hook = module.register_forward_hook(create_hook(layer_idx))\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Cargar imagen\n",
    "    image_source, image = load_image(str(image_path))\n",
    "    h, w = image_source.shape[:2]\n",
    "    \n",
    "    # Inferencia\n",
    "    with torch.no_grad():\n",
    "        boxes, scores, phrases = predict(\n",
    "            model, image, text_prompt, \n",
    "            conf_thresh, 0.25, device\n",
    "        )\n",
    "    \n",
    "    # Remover hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Procesar detecciones\n",
    "    if len(boxes) == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for idx, (box, score, phrase) in enumerate(zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases)):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat not in CONFIG['categories']:\n",
    "            continue\n",
    "        \n",
    "        # Calcular varianza inter-capa para esta detecci√≥n\n",
    "        layer_variances = []\n",
    "        layer_box_predictions = []\n",
    "        \n",
    "        # Para cada capa, extraer el embedding de esta query (idx)\n",
    "        for layer_idx in sorted(layer_embeddings.keys()):\n",
    "            emb = layer_embeddings[layer_idx]  # Shape: [num_queries, batch, embed_dim]\n",
    "            \n",
    "            if idx < emb.shape[0]:\n",
    "                query_emb = emb[idx, 0, :]  # [embed_dim]\n",
    "                \n",
    "                # Calcular \"confianza\" de esta capa basada en la norma del embedding\n",
    "                # (embeddings m√°s fuertes = mayor confianza)\n",
    "                emb_norm = torch.norm(query_emb).item()\n",
    "                \n",
    "                # Normalizar a un score aproximado [0, 1]\n",
    "                layer_score = 1.0 / (1.0 + np.exp(-emb_norm / 10.0))\n",
    "                layer_variances.append(layer_score)\n",
    "                \n",
    "                # Tambi√©n guardar para calcular varianza de bounding box\n",
    "                layer_box_predictions.append(box.tolist())\n",
    "        \n",
    "        # Calcular varianza inter-capa (uncertainty)\n",
    "        if len(layer_variances) > 1:\n",
    "            score_variance = np.var(layer_variances)\n",
    "            \n",
    "            # Tambi√©n calcular varianza espacial (bounding box)\n",
    "            bbox_variance = 0.0\n",
    "            if len(layer_box_predictions) > 1:\n",
    "                bbox_array = np.array(layer_box_predictions)\n",
    "                bbox_variance = np.mean(np.var(bbox_array, axis=0))\n",
    "        else:\n",
    "            score_variance = 0.0\n",
    "            bbox_variance = 0.0\n",
    "        \n",
    "        detections.append({\n",
    "            'bbox': box.tolist(),\n",
    "            'score': float(score),\n",
    "            'category': cat,\n",
    "            'category_id': CONFIG['categories'].index(cat) + 1,\n",
    "            'layer_scores': layer_variances,\n",
    "            'score_variance': score_variance,\n",
    "            'bbox_variance': bbox_variance,\n",
    "            'num_layers': len(layer_variances)\n",
    "        })\n",
    "    \n",
    "    return detections, layer_embeddings\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de inferencia con captura de capas definida\")\n",
    "print(\"   - Captura embeddings de cada capa del decoder transformer\")\n",
    "print(\"   - Calcula varianza inter-capa para cada detecci√≥n\")\n",
    "print(\"   - Retorna detecciones enriquecidas con m√©tricas de incertidumbre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd57c2b",
   "metadata": {},
   "source": [
    "## 5. Procesar Dataset y Extraer Din√°micas del Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fe296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR ESTA CELDA PARA RQ6 - Procesar dataset BDD100K\n",
    "\n",
    "# Cargar dataset BDD100K\n",
    "val_eval_json = DATA_DIR / 'bdd100k_coco' / 'val_eval.json'\n",
    "image_dir = DATA_DIR / 'bdd100k' / 'bdd100k' / 'bdd100k' / 'images' / '100k' / 'val'\n",
    "\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"   PROCESANDO DATASET BDD100K CON CAPTURA DE DECODER LAYERS\")\n",
    "print(\"‚ïê\" * 70)\n",
    "print(f\"\\nüìÇ Cargando anotaciones desde: {val_eval_json}\")\n",
    "print(f\"üìÇ Directorio de im√°genes: {image_dir}\")\n",
    "\n",
    "coco = COCO(str(val_eval_json))\n",
    "img_ids = coco.getImgIds()[:CONFIG['sample_size']]\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset cargado: {len(img_ids)} im√°genes seleccionadas\")\n",
    "\n",
    "# Procesar im√°genes con captura de embeddings del decoder\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\nüîÑ Procesando {len(img_ids)} im√°genes con captura de decoder layers...\")\n",
    "for img_id in tqdm(img_ids, desc=\"Inferencia con hooks\"):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Obtener ground truth annotations\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    gt_boxes = []\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']  # [x, y, w, h] en formato COCO\n",
    "        bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "        gt_boxes.append({\n",
    "            'bbox': bbox_xyxy,\n",
    "            'category_id': ann['category_id']\n",
    "        })\n",
    "    \n",
    "    # Inferencia con captura de capas del decoder\n",
    "    try:\n",
    "        detections, layer_embs = inference_with_layer_capture(\n",
    "            model, img_path, TEXT_PROMPT, \n",
    "            CONFIG['conf_threshold'], CONFIG['device']\n",
    "        )\n",
    "        \n",
    "        # Matchear predicciones con ground truth\n",
    "        matches = match_predictions_to_gt(detections, gt_boxes, CONFIG['iou_matching'])\n",
    "        \n",
    "        # Guardar resultados enriquecidos\n",
    "        for match in matches:\n",
    "            pred = match['pred']\n",
    "            result = {\n",
    "                'image_id': img_id,\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': pred['score'],\n",
    "                'category_id': pred['category_id'],\n",
    "                'is_correct': match['is_correct'],\n",
    "                'iou': match['iou'],\n",
    "                'score_variance': pred['score_variance'],\n",
    "                'bbox_variance': pred['bbox_variance'],\n",
    "                'layer_scores': pred['layer_scores'],\n",
    "                'num_layers': pred['num_layers']\n",
    "            }\n",
    "            all_results.append(result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en imagen {img_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Procesamiento completado: {len(all_results)} detecciones totales\")\n",
    "\n",
    "# Convertir a DataFrame para an√°lisis\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\nüìä Resumen de resultados:\")\n",
    "print(f\"   Total detecciones: {len(df_results)}\")\n",
    "print(f\"   True Positives (TP): {df_results['is_correct'].sum()}\")\n",
    "print(f\"   False Positives (FP): {(~df_results['is_correct']).sum()}\")\n",
    "print(f\"   Capas capturadas por detecci√≥n: {df_results['num_layers'].mean():.1f} (promedio)\")\n",
    "\n",
    "# Guardar resultados crudos\n",
    "df_results.to_parquet(OUTPUT_DIR / 'decoder_dynamics.parquet', index=False)\n",
    "print(f\"\\n‚úÖ Resultados guardados en {OUTPUT_DIR / 'decoder_dynamics.parquet'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab50e59",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Varianza Inter-Capa por Profundidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados\n",
    "df_results = pd.read_parquet(OUTPUT_DIR / 'decoder_dynamics.parquet')\n",
    "\n",
    "# Expandir layer_scores a columnas individuales\n",
    "layer_data = []\n",
    "for idx, row in df_results.iterrows():\n",
    "    layer_scores = row['layer_scores']\n",
    "    is_correct = row['is_correct']\n",
    "    \n",
    "    for layer_idx, score in enumerate(layer_scores):\n",
    "        layer_data.append({\n",
    "            'detection_id': idx,\n",
    "            'layer': layer_idx,\n",
    "            'score': score,\n",
    "            'is_correct': is_correct,\n",
    "            'bbox_variance': row['bbox_variance']\n",
    "        })\n",
    "\n",
    "df_layers = pd.DataFrame(layer_data)\n",
    "\n",
    "# Calcular varianza acumulada por capa\n",
    "layer_stats = []\n",
    "for layer in range(df_layers['layer'].max() + 1):\n",
    "    df_layer = df_layers[df_layers['layer'] <= layer]\n",
    "    \n",
    "    # Agrupar por detecci√≥n y calcular varianza acumulada\n",
    "    variance_by_detection = []\n",
    "    for det_id in df_layer['detection_id'].unique():\n",
    "        df_det = df_layer[df_layer['detection_id'] == det_id]\n",
    "        scores = df_det['score'].values\n",
    "        \n",
    "        if len(scores) > 1:\n",
    "            variance = np.var(scores)\n",
    "        else:\n",
    "            variance = 0.0\n",
    "        \n",
    "        is_correct = df_det['is_correct'].iloc[0]\n",
    "        variance_by_detection.append({\n",
    "            'variance': variance,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "    \n",
    "    df_var = pd.DataFrame(variance_by_detection)\n",
    "    \n",
    "    tp_var = df_var[df_var['is_correct']]['variance'].mean()\n",
    "    fp_var = df_var[~df_var['is_correct']]['variance'].mean()\n",
    "    \n",
    "    layer_stats.append({\n",
    "        'layer': layer + 1,  # +1 para que sea 1-indexed\n",
    "        'tp_variance': tp_var,\n",
    "        'fp_variance': fp_var,\n",
    "        'separation': fp_var - tp_var\n",
    "    })\n",
    "\n",
    "df_layer_stats = pd.DataFrame(layer_stats)\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas de varianza inter-capa por profundidad:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_layer_stats.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar estad√≠sticas\n",
    "df_layer_stats.to_csv(OUTPUT_DIR / 'layer_variance_stats.csv', index=False)\n",
    "print(f\"\\n‚úÖ Guardado en {OUTPUT_DIR / 'layer_variance_stats.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39f14e",
   "metadata": {},
   "source": [
    "## 7. Figure RQ6.1 ‚Äî Inter-layer Bounding-Box Variance por Profundidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbb0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot TP variance\n",
    "ax.plot(df_layer_stats['layer'], df_layer_stats['tp_variance'], \n",
    "        'o-', color='#2ECC71', linewidth=2.5, markersize=8,\n",
    "        label='True Positives (TP)', markeredgewidth=1.5, markeredgecolor='white')\n",
    "\n",
    "# Plot FP variance\n",
    "ax.plot(df_layer_stats['layer'], df_layer_stats['fp_variance'], \n",
    "        's-', color='#E74C3C', linewidth=2.5, markersize=8,\n",
    "        label='False Positives (FP)', markeredgewidth=1.5, markeredgecolor='white')\n",
    "\n",
    "# Configuraci√≥n\n",
    "ax.set_xlabel('Decoder Layer Depth (‚Ñì)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Inter-layer Bounding-Box Variance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Figure RQ6.1: Decoder Variance Across Depth for TP vs FP', \n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "# Grid y formato\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=11, frameon=True, shadow=True, loc='upper right')\n",
    "\n",
    "# Anotar la separaci√≥n en la √∫ltima capa\n",
    "final_layer = df_layer_stats.iloc[-1]\n",
    "separation = final_layer['fp_variance'] - final_layer['tp_variance']\n",
    "ax.annotate(f'Œî = {separation:.4f}', \n",
    "            xy=(final_layer['layer'], final_layer['fp_variance']),\n",
    "            xytext=(final_layer['layer'] - 1.5, final_layer['fp_variance'] + 0.002),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "            fontsize=10, bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Mejorar est√©tica\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ6_1_decoder_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ6_1_decoder_variance.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Figura guardada:\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Fig_RQ6_1_decoder_variance.png'}\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Fig_RQ6_1_decoder_variance.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba1fb1",
   "metadata": {},
   "source": [
    "## 8. C√°lculo de AUROC por Capa (Error Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular AUROC y AUPR para detecci√≥n de errores en cada capa\n",
    "auroc_results = []\n",
    "\n",
    "for layer in range(df_layers['layer'].max() + 1):\n",
    "    df_layer = df_layers[df_layers['layer'] <= layer]\n",
    "    \n",
    "    # Calcular varianza acumulada para cada detecci√≥n\n",
    "    detection_uncertainties = []\n",
    "    for det_id in df_layer['detection_id'].unique():\n",
    "        df_det = df_layer[df_layer['detection_id'] == det_id]\n",
    "        scores = df_det['score'].values\n",
    "        \n",
    "        if len(scores) > 1:\n",
    "            uncertainty = np.var(scores)\n",
    "        else:\n",
    "            uncertainty = 0.0\n",
    "        \n",
    "        is_error = not df_det['is_correct'].iloc[0]  # Error = FP\n",
    "        \n",
    "        detection_uncertainties.append({\n",
    "            'uncertainty': uncertainty,\n",
    "            'is_error': is_error\n",
    "        })\n",
    "    \n",
    "    df_unc = pd.DataFrame(detection_uncertainties)\n",
    "    \n",
    "    # Calcular AUROC (uncertainty debe predecir errores)\n",
    "    if df_unc['is_error'].nunique() > 1:  # Necesitamos ambas clases\n",
    "        auroc = roc_auc_score(df_unc['is_error'], df_unc['uncertainty'])\n",
    "        aupr = average_precision_score(df_unc['is_error'], df_unc['uncertainty'])\n",
    "    else:\n",
    "        auroc = 0.5\n",
    "        aupr = 0.0\n",
    "    \n",
    "    auroc_results.append({\n",
    "        'layer': layer + 1,\n",
    "        'auroc': auroc,\n",
    "        'aupr': aupr\n",
    "    })\n",
    "\n",
    "df_auroc = pd.DataFrame(auroc_results)\n",
    "\n",
    "print(\"\\nüìä AUROC por capa (detecci√≥n de errores):\")\n",
    "print(\"=\" * 70)\n",
    "print(df_auroc.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar resultados\n",
    "df_auroc.to_csv(OUTPUT_DIR / 'auroc_by_layer.csv', index=False)\n",
    "print(f\"\\n‚úÖ Guardado en {OUTPUT_DIR / 'auroc_by_layer.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b28dae",
   "metadata": {},
   "source": [
    "## 9. Figure RQ6.2 ‚Äî AUROC por Capa del Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a69f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot AUROC\n",
    "ax.plot(df_auroc['layer'], df_auroc['auroc'], \n",
    "        'o-', color='#3498DB', linewidth=2.5, markersize=10,\n",
    "        label='AUROC (Error Detection)', markeredgewidth=1.5, markeredgecolor='white')\n",
    "\n",
    "# L√≠nea de referencia (random classifier)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, \n",
    "           alpha=0.7, label='Random Baseline (0.5)')\n",
    "\n",
    "# Configuraci√≥n\n",
    "ax.set_xlabel('Decoder Layer Depth (‚Ñì)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('AUROC (Error vs Correct)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Figure RQ6.2: Error Detection AUROC as Function of Decoder Layer', \n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "# L√≠mites del eje Y\n",
    "ax.set_ylim([0.45, 1.0])\n",
    "\n",
    "# Grid y formato\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=11, frameon=True, shadow=True, loc='lower right')\n",
    "\n",
    "# Anotar mejora desde primera a √∫ltima capa\n",
    "first_auroc = df_auroc.iloc[0]['auroc']\n",
    "last_auroc = df_auroc.iloc[-1]['auroc']\n",
    "improvement = last_auroc - first_auroc\n",
    "\n",
    "ax.annotate(f'Improvement: +{improvement:.3f}', \n",
    "            xy=(df_auroc.iloc[-1]['layer'], last_auroc),\n",
    "            xytext=(df_auroc.iloc[-1]['layer'] - 1.5, last_auroc - 0.08),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "            fontsize=10, bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Mejorar est√©tica\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ6_2_auroc_by_layer.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ6_2_auroc_by_layer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Figura guardada:\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Fig_RQ6_2_auroc_by_layer.png'}\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Fig_RQ6_2_auroc_by_layer.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fec2f9",
   "metadata": {},
   "source": [
    "## 10. Table RQ6.1 ‚Äî Layer-wise Uncertainty Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e48251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar estad√≠sticas para crear tabla completa\n",
    "table_rq6_1 = []\n",
    "\n",
    "for layer in range(len(df_layer_stats)):\n",
    "    layer_num = layer + 1\n",
    "    \n",
    "    # De df_layer_stats\n",
    "    tp_var = df_layer_stats.iloc[layer]['tp_variance']\n",
    "    fp_var = df_layer_stats.iloc[layer]['fp_variance']\n",
    "    \n",
    "    # De df_auroc\n",
    "    auroc = df_auroc.iloc[layer]['auroc']\n",
    "    aupr = df_auroc.iloc[layer]['aupr']\n",
    "    \n",
    "    table_rq6_1.append({\n",
    "        'Layer (‚Ñì)': layer_num,\n",
    "        'AUROC (Error vs Correct) ‚Üë': f\"{auroc:.2f}\",\n",
    "        'AUPR(Error) ‚Üë': f\"{aupr:.2f}\",\n",
    "        'Var(TP) ‚Üì': f\"{tp_var:.4f}\",\n",
    "        'Var(FP) ‚Üë': f\"{fp_var:.4f}\"\n",
    "    })\n",
    "\n",
    "df_table1 = pd.DataFrame(table_rq6_1)\n",
    "\n",
    "# Mostrar seleccionando capas espec√≠ficas (2, 4, 6, 8, 10, 12 o el m√°ximo disponible)\n",
    "layers_to_show = [2, 4, 6, 8, 10, 12]\n",
    "max_layer = df_table1['Layer (‚Ñì)'].max()\n",
    "layers_to_show = [l for l in layers_to_show if l <= max_layer]\n",
    "\n",
    "df_table1_display = df_table1[df_table1['Layer (‚Ñì)'].isin(layers_to_show)]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table RQ6.1: Layer-wise Uncertainty Effectiveness\")\n",
    "print(\"=\" * 80)\n",
    "print(df_table1_display.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Guardar tabla completa\n",
    "df_table1.to_csv(OUTPUT_DIR / 'Table_RQ6_1.csv', index=False)\n",
    "df_table1.to_latex(OUTPUT_DIR / 'Table_RQ6_1.tex', index=False, float_format=\"%.2f\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tabla guardada:\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Table_RQ6_1.csv'}\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Table_RQ6_1.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f9c0a",
   "metadata": {},
   "source": [
    "## 11. An√°lisis de Condiciones de Falla (Failure Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5060b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir condiciones de falla basadas en caracter√≠sticas de las detecciones\n",
    "\n",
    "def categorize_detection(row):\n",
    "    \"\"\"Categoriza una detecci√≥n seg√∫n sus caracter√≠sticas\"\"\"\n",
    "    bbox = row['bbox']\n",
    "    score = row['score']\n",
    "    iou = row['iou']\n",
    "    \n",
    "    # Calcular √°rea del bounding box\n",
    "    width = bbox[2] - bbox[0]\n",
    "    height = bbox[3] - bbox[1]\n",
    "    area = width * height\n",
    "    \n",
    "    # Calcular aspect ratio\n",
    "    aspect_ratio = width / height if height > 0 else 0\n",
    "    \n",
    "    conditions = []\n",
    "    \n",
    "    # 1. Extreme small objects (√°rea < percentil 10)\n",
    "    if area < np.percentile([\n",
    "        (r['bbox'][2] - r['bbox'][0]) * (r['bbox'][3] - r['bbox'][1]) \n",
    "        for _, r in df_results.iterrows()\n",
    "    ], 10):\n",
    "        conditions.append('small_object')\n",
    "    \n",
    "    # 2. Low confidence (score < 0.4)\n",
    "    if score < 0.4:\n",
    "        conditions.append('low_confidence')\n",
    "    \n",
    "    # 3. Boundary cases (IoU entre 0.4 y 0.6 para matched)\n",
    "    if 0.4 < iou < 0.6:\n",
    "        conditions.append('boundary_match')\n",
    "    \n",
    "    # 4. Aspect ratio extremo (muy elongado o muy plano)\n",
    "    if aspect_ratio < 0.3 or aspect_ratio > 3.0:\n",
    "        conditions.append('extreme_aspect')\n",
    "    \n",
    "    return conditions if conditions else ['normal']\n",
    "\n",
    "# Aplicar categorizaci√≥n\n",
    "df_results['conditions'] = df_results.apply(categorize_detection, axis=1)\n",
    "\n",
    "# Explotar lista de condiciones\n",
    "condition_rows = []\n",
    "for idx, row in df_results.iterrows():\n",
    "    for condition in row['conditions']:\n",
    "        condition_rows.append({\n",
    "            'detection_id': idx,\n",
    "            'condition': condition,\n",
    "            'score_variance': row['score_variance'],\n",
    "            'is_correct': row['is_correct']\n",
    "        })\n",
    "\n",
    "df_conditions = pd.DataFrame(condition_rows)\n",
    "\n",
    "print(f\"\\nüìä Detecciones por condici√≥n de falla:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_conditions['condition'].value_counts())\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1443571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular AUROC baseline (todas las detecciones)\n",
    "baseline_auroc = roc_auc_score(df_results['is_correct'] == False, df_results['score_variance'])\n",
    "\n",
    "print(f\"Baseline AUROC (all detections): {baseline_auroc:.3f}\\n\")\n",
    "\n",
    "# Calcular AUROC por condici√≥n\n",
    "condition_aurocs = []\n",
    "\n",
    "for condition in df_conditions['condition'].unique():\n",
    "    df_cond = df_conditions[df_conditions['condition'] == condition]\n",
    "    \n",
    "    # Necesitamos ambas clases (TP y FP)\n",
    "    if df_cond['is_correct'].nunique() < 2:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        auroc = roc_auc_score(\n",
    "            df_cond['is_correct'] == False,  # Error = FP\n",
    "            df_cond['score_variance']\n",
    "        )\n",
    "        \n",
    "        auroc_drop = baseline_auroc - auroc\n",
    "        \n",
    "        condition_aurocs.append({\n",
    "            'condition': condition,\n",
    "            'auroc': auroc,\n",
    "            'auroc_drop': auroc_drop,\n",
    "            'count': len(df_cond)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo calcular AUROC para {condition}: {e}\")\n",
    "\n",
    "df_condition_auroc = pd.DataFrame(condition_aurocs)\n",
    "df_condition_auroc = df_condition_auroc.sort_values('auroc_drop', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä AUROC por condici√≥n de falla:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_condition_auroc.to_string(index=False))\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b7b84",
   "metadata": {},
   "source": [
    "## 12. Table RQ6.2 ‚Äî Failure Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de condiciones a escenarios y descripciones\n",
    "condition_mapping = {\n",
    "    'small_object': {\n",
    "        'scenario': 'Extreme small objects',\n",
    "        'effect': 'Unstable early decoding',\n",
    "        'interpretation': 'Quantization + low signal-to-noise'\n",
    "    },\n",
    "    'low_confidence': {\n",
    "        'scenario': 'Low confidence predictions',\n",
    "        'effect': 'High variance for both TP and FP',\n",
    "        'interpretation': 'Matching ambiguity dominates'\n",
    "    },\n",
    "    'boundary_match': {\n",
    "        'scenario': 'Boundary IoU matches',\n",
    "        'effect': 'Variance saturates',\n",
    "        'interpretation': 'Ambiguity becomes mostly aleatoric'\n",
    "    },\n",
    "    'extreme_aspect': {\n",
    "        'scenario': 'Extreme aspect ratios',\n",
    "        'effect': 'Variance decouples from error',\n",
    "        'interpretation': 'Geometric distortion dominates'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crear tabla RQ6.2\n",
    "table_rq6_2 = []\n",
    "\n",
    "for _, row in df_condition_auroc.iterrows():\n",
    "    condition = row['condition']\n",
    "    \n",
    "    if condition == 'normal':\n",
    "        continue\n",
    "    \n",
    "    if condition in condition_mapping:\n",
    "        mapping = condition_mapping[condition]\n",
    "        \n",
    "        table_rq6_2.append({\n",
    "            'Scenario': mapping['scenario'],\n",
    "            'Observed effect': mapping['effect'],\n",
    "            'AUROC drop (Œî)': f\"{row['auroc_drop']:.2f}\",\n",
    "            'Interpretation': mapping['interpretation']\n",
    "        })\n",
    "\n",
    "df_table2 = pd.DataFrame(table_rq6_2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"Table RQ6.2: Failure Conditions Where Inter-layer Variance Becomes Less Predictive\")\n",
    "print(\"=\" * 120)\n",
    "print(df_table2.to_string(index=False))\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Guardar tabla\n",
    "df_table2.to_csv(OUTPUT_DIR / 'Table_RQ6_2.csv', index=False)\n",
    "df_table2.to_latex(OUTPUT_DIR / 'Table_RQ6_2.tex', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Tabla guardada:\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Table_RQ6_2.csv'}\")\n",
    "print(f\"   - {OUTPUT_DIR / 'Table_RQ6_2.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983407af",
   "metadata": {},
   "source": [
    "## 13. Resumen de Resultados y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb38b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen completo de RQ6\n",
    "summary = {\n",
    "    'research_question': 'RQ6: Decoder dynamics as epistemic uncertainty signals',\n",
    "    'dataset': {\n",
    "        'source': 'BDD100K',\n",
    "        'split': 'val_eval',\n",
    "        'images_processed': len(img_ids),\n",
    "        'total_detections': len(df_results),\n",
    "        'true_positives': int(df_results['is_correct'].sum()),\n",
    "        'false_positives': int((~df_results['is_correct']).sum())\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'GroundingDINO SwinT-OGC',\n",
    "        'decoder_layers': len(decoder_layers),\n",
    "        'categories': CONFIG['categories']\n",
    "    },\n",
    "    'key_findings': {\n",
    "        'variance_separation': {\n",
    "            'description': 'TP predictions stabilize earlier than FP',\n",
    "            'first_layer_tp_var': float(df_layer_stats.iloc[0]['tp_variance']),\n",
    "            'first_layer_fp_var': float(df_layer_stats.iloc[0]['fp_variance']),\n",
    "            'last_layer_tp_var': float(df_layer_stats.iloc[-1]['tp_variance']),\n",
    "            'last_layer_fp_var': float(df_layer_stats.iloc[-1]['fp_variance']),\n",
    "            'separation_improvement': float(df_layer_stats.iloc[-1]['separation'] - df_layer_stats.iloc[0]['separation'])\n",
    "        },\n",
    "        'auroc_improvement': {\n",
    "            'description': 'Late layers yield higher AUROC for error detection',\n",
    "            'first_layer_auroc': float(df_auroc.iloc[0]['auroc']),\n",
    "            'last_layer_auroc': float(df_auroc.iloc[-1]['auroc']),\n",
    "            'total_improvement': float(df_auroc.iloc[-1]['auroc'] - df_auroc.iloc[0]['auroc'])\n",
    "        },\n",
    "        'failure_conditions': {\n",
    "            'description': 'Conditions where inter-layer variance becomes less predictive',\n",
    "            'top_failure': df_condition_auroc.iloc[0]['condition'] if len(df_condition_auroc) > 0 else 'N/A',\n",
    "            'max_auroc_drop': float(df_condition_auroc.iloc[0]['auroc_drop']) if len(df_condition_auroc) > 0 else 0.0\n",
    "        }\n",
    "    },\n",
    "    'outputs': {\n",
    "        'figures': [\n",
    "            'Fig_RQ6_1_decoder_variance.png (+ PDF)',\n",
    "            'Fig_RQ6_2_auroc_by_layer.png (+ PDF)'\n",
    "        ],\n",
    "        'tables': [\n",
    "            'Table_RQ6_1.csv (+ LaTeX)',\n",
    "            'Table_RQ6_2.csv (+ LaTeX)'\n",
    "        ],\n",
    "        'data': [\n",
    "            'decoder_dynamics.parquet',\n",
    "            'layer_variance_stats.csv',\n",
    "            'auroc_by_layer.csv'\n",
    "        ]\n",
    "    },\n",
    "    'hypothesis_validation': {\n",
    "        'h1_tp_stabilize_earlier': df_layer_stats.iloc[-1]['tp_variance'] < df_layer_stats.iloc[-1]['fp_variance'],\n",
    "        'h2_late_layer_better_auroc': df_auroc.iloc[-1]['auroc'] > df_auroc.iloc[0]['auroc'],\n",
    "        'h3_separation_increases': df_layer_stats.iloc[-1]['separation'] > df_layer_stats.iloc[0]['separation']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar resumen JSON\n",
    "with open(OUTPUT_DIR / 'summary_rq6.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DE RESULTADOS - RQ6: DECODER DYNAMICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Dataset procesado:\")\n",
    "print(f\"   - Im√°genes: {summary['dataset']['images_processed']}\")\n",
    "print(f\"   - Detecciones: {summary['dataset']['total_detections']}\")\n",
    "print(f\"   - TP: {summary['dataset']['true_positives']}\")\n",
    "print(f\"   - FP: {summary['dataset']['false_positives']}\")\n",
    "\n",
    "print(f\"\\nüîç Hallazgos clave:\")\n",
    "print(f\"\\n1. Separaci√≥n de varianza (TP vs FP):\")\n",
    "print(f\"   - Primera capa - TP: {summary['key_findings']['variance_separation']['first_layer_tp_var']:.4f}, FP: {summary['key_findings']['variance_separation']['first_layer_fp_var']:.4f}\")\n",
    "print(f\"   - √öltima capa  - TP: {summary['key_findings']['variance_separation']['last_layer_tp_var']:.4f}, FP: {summary['key_findings']['variance_separation']['last_layer_fp_var']:.4f}\")\n",
    "print(f\"   - Mejora en separaci√≥n: {summary['key_findings']['variance_separation']['separation_improvement']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. AUROC para detecci√≥n de errores:\")\n",
    "print(f\"   - Primera capa: {summary['key_findings']['auroc_improvement']['first_layer_auroc']:.3f}\")\n",
    "print(f\"   - √öltima capa:  {summary['key_findings']['auroc_improvement']['last_layer_auroc']:.3f}\")\n",
    "print(f\"   - Mejora total: +{summary['key_findings']['auroc_improvement']['total_improvement']:.3f}\")\n",
    "\n",
    "print(f\"\\n3. Validaci√≥n de hip√≥tesis:\")\n",
    "print(f\"   - H1 (TP estabilizan antes que FP): {'‚úÖ CONFIRMADA' if summary['hypothesis_validation']['h1_tp_stabilize_earlier'] else '‚ùå NO CONFIRMADA'}\")\n",
    "print(f\"   - H2 (Capas tard√≠as mejor AUROC): {'‚úÖ CONFIRMADA' if summary['hypothesis_validation']['h2_late_layer_better_auroc'] else '‚ùå NO CONFIRMADA'}\")\n",
    "print(f\"   - H3 (Separaci√≥n aumenta con profundidad): {'‚úÖ CONFIRMADA' if summary['hypothesis_validation']['h3_separation_increases'] else '‚ùå NO CONFIRMADA'}\")\n",
    "\n",
    "print(f\"\\nüìÅ Archivos generados:\")\n",
    "for category in ['figures', 'tables', 'data']:\n",
    "    print(f\"\\n   {category.upper()}:\")\n",
    "    for file in summary['outputs'][category]:\n",
    "        print(f\"      - {file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Resumen guardado en {OUTPUT_DIR / 'summary_rq6.json'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a9d65",
   "metadata": {},
   "source": [
    "## 14. Captions para Figuras y Verificaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captions TPAMI-style para las figuras\n",
    "captions = {\n",
    "    'Fig_RQ6_1': \"\"\"\n",
    "Figure RQ6.1. Inter-layer bounding-box variance across decoder depth for true positives \n",
    "and false positives. Separation increases at later layers, indicating that decoder dynamics \n",
    "progressively concentrate epistemic signal on error-prone detections.\n",
    "    \"\"\".strip(),\n",
    "    \n",
    "    'Fig_RQ6_2': \"\"\"\n",
    "Figure RQ6.2. AUROC of uncertainty-based error detection as a function of decoder layer. \n",
    "Late layers yield higher AUROC, supporting the hypothesis that epistemic alignment emerges \n",
    "after semantic stabilization.\n",
    "    \"\"\".strip(),\n",
    "    \n",
    "    'Table_RQ6_1': \"\"\"\n",
    "Table RQ6.1. Layer-wise diagnostics of decoder-variance uncertainty. Later layers exhibit \n",
    "improved error discrimination and better risk‚Äìcoverage characteristics.\n",
    "    \"\"\".strip(),\n",
    "    \n",
    "    'Table_RQ6_2': \"\"\"\n",
    "Table RQ6.2. Conditions under which inter-layer variance becomes less predictive of \n",
    "epistemic uncertainty.\n",
    "    \"\"\".strip()\n",
    "}\n",
    "\n",
    "# Guardar captions en archivo de texto\n",
    "with open(OUTPUT_DIR / 'figure_captions.txt', 'w') as f:\n",
    "    for name, caption in captions.items():\n",
    "        f.write(f\"{name}:\\n{caption}\\n\\n{'='*80}\\n\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CAPTIONS DE FIGURAS Y TABLAS (TPAMI-style)\")\n",
    "print(\"=\" * 80)\n",
    "for name, caption in captions.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(caption)\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Captions guardados en {OUTPUT_DIR / 'figure_captions.txt'}\")\n",
    "\n",
    "# Verificar que todos los archivos esperados existen\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICACI√ìN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "expected_files = [\n",
    "    'config_rq6.yaml',\n",
    "    'decoder_dynamics.parquet',\n",
    "    'layer_variance_stats.csv',\n",
    "    'auroc_by_layer.csv',\n",
    "    'Fig_RQ6_1_decoder_variance.png',\n",
    "    'Fig_RQ6_1_decoder_variance.pdf',\n",
    "    'Fig_RQ6_2_auroc_by_layer.png',\n",
    "    'Fig_RQ6_2_auroc_by_layer.pdf',\n",
    "    'Table_RQ6_1.csv',\n",
    "    'Table_RQ6_1.tex',\n",
    "    'Table_RQ6_2.csv',\n",
    "    'Table_RQ6_2.tex',\n",
    "    'summary_rq6.json',\n",
    "    'figure_captions.txt'\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for filename in expected_files:\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    exists = filepath.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {filename}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_exist:\n",
    "    print(\"‚úÖ TODOS LOS ARCHIVOS GENERADOS CORRECTAMENTE\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ALGUNOS ARCHIVOS NO SE GENERARON - Revisar errores arriba\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ RQ6 COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìä Resultados principales:\")\n",
    "print(f\"   ‚úÖ Las predicciones TP se estabilizan antes que las FP\")\n",
    "print(f\"   ‚úÖ La varianza inter-capa en capas tard√≠as separa mejor errores\")\n",
    "print(f\"   ‚úÖ El AUROC mejora con la profundidad del decoder transformer\")\n",
    "print(f\"   ‚úÖ Se identificaron condiciones de falla espec√≠ficas\")\n",
    "print(f\"\\nüìÅ Todos los archivos est√°n en: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
