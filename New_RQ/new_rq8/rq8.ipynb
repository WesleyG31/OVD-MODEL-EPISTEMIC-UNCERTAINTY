{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512cccd8",
   "metadata": {},
   "source": [
    "# RQ8 ‚Äî Joint Semantic‚ÄìGeometric Calibration for Reliability\n",
    "\n",
    "**Research Question**: How can semantic confidence and localization quality be jointly calibrated to yield meaningful scores for ranking/selection?\n",
    "\n",
    "**Hip√≥tesis**: Los scores sem√°nticos crudos est√°n desalineados con la calidad geom√©trica (IoU); una calibraci√≥n conjunta restaura la monotonicidad y mejora m√©tricas de ranking (e.g., Precision@K) incluso cuando el mAP cambia poco.\n",
    "\n",
    "**Expected Results**:\n",
    "- **Figure RQ8.1**: Confiabilidad de los scores de detecci√≥n respecto a la calidad geom√©trica (mean IoU por bin de confianza). La calibraci√≥n conjunta mejora sustancialmente la alineaci√≥n monot√≥nica entre score y precisi√≥n de localizaci√≥n.\n",
    "- **Figure RQ8.2**: Precision@K para ranking de detecciones bajo scores crudos y calibrados (escala log K). La calibraci√≥n mejora la calidad del ranking, soportando selecci√≥n confiable m√°s all√° del mAP.\n",
    "- **Table RQ8.1**: Correlaci√≥n entre detection score e IoU antes y despu√©s de calibraci√≥n conjunta\n",
    "- **Table RQ8.2**: Mejoras en ranking y selecci√≥n inducidas por calibraci√≥n conjunta con presupuesto fijo de propuestas\n",
    "\n",
    "**Nota**: Este notebook utiliza resultados REALES del modelo GroundingDINO evaluado en las fases anteriores. Los scores, IoUs y m√©tricas son aut√©nticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051d91",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7568cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n cargada\n",
      "   Device: cuda\n",
      "   Output: /workspace/New_RQ/new_rq8/output\n",
      "   Data:   /workspace/New_RQ/new_rq8/../../data\n",
      "   Sample size: 500 im√°genes\n",
      "‚úÖ Configuraci√≥n guardada en output/config_rq8.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de paths relativos (desde New_RQ/new_rq8/)\n",
    "BASE_DIR = Path('../..')  # Subir dos niveles hasta el root del proyecto\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'iou_matching': 0.5,\n",
    "    'conf_threshold': 0.25,\n",
    "    'sample_size': 500,  # N√∫mero de im√°genes a procesar\n",
    "    'n_bins': 10,  # Para reliability diagrams\n",
    "    'top_k_values': [100, 200, 400]  # Presupuestos para Precision@K\n",
    "}\n",
    "\n",
    "# Semillas para reproducibilidad\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n cargada\")\n",
    "print(f\"   Device: {CONFIG['device']}\")\n",
    "print(f\"   Output: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"   Data:   {DATA_DIR.absolute()}\")\n",
    "print(f\"   Sample size: {CONFIG['sample_size']} im√°genes\")\n",
    "\n",
    "# Guardar configuraci√≥n\n",
    "with open(OUTPUT_DIR / 'config_rq8.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "print(f\"‚úÖ Configuraci√≥n guardada en {OUTPUT_DIR / 'config_rq8.yaml'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9c327",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo GroundingDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7a96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "   CARGANDO MODELO GROUNDINGDINO PARA CALIBRACI√ìN CONJUNTA\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "üìç GroundingDINO instalado en: /opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino\n",
      "‚úÖ Config encontrado: /opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\n",
      "‚úÖ Pesos encontrados: /opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth\n",
      "\n",
      "üîÑ Cargando modelo desde:\n",
      "   Config:  /opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\n",
      "   Weights: /opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "‚úÖ Modelo cargado en cuda\n",
      "‚úÖ Prompt: person. rider. car. truck. bus. train. motorcycle. bicycle. traffic light. traffic sign.\n",
      "‚úÖ Arquitectura: GroundingDINO SwinT-OGC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ8 - Cargar modelo GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "import groundingdino\n",
    "import os\n",
    "\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"   CARGANDO MODELO GROUNDINGDINO PARA CALIBRACI√ìN CONJUNTA\")\n",
    "print(\"‚ïê\" * 70)\n",
    "\n",
    "# Detectar ubicaci√≥n de GroundingDINO\n",
    "gdino_path = os.path.dirname(groundingdino.__file__)\n",
    "print(f\"\\nüìç GroundingDINO instalado en: {gdino_path}\")\n",
    "\n",
    "# Buscar archivos de configuraci√≥n y pesos en ubicaciones comunes\n",
    "possible_configs = [\n",
    "    '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py',\n",
    "    os.path.join(gdino_path, 'config', 'GroundingDINO_SwinT_OGC.py'),\n",
    "    str(BASE_DIR / 'installing_dino' / 'GroundingDINO' / 'groundingdino' / 'config' / 'GroundingDINO_SwinT_OGC.py'),\n",
    "    'C:/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py',  # Windows absoluta\n",
    "]\n",
    "\n",
    "possible_weights = [\n",
    "    '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth',\n",
    "    os.path.join(os.path.dirname(gdino_path), 'weights', 'groundingdino_swint_ogc.pth'),\n",
    "    str(BASE_DIR / 'installing_dino' / 'GroundingDINO' / 'weights' / 'groundingdino_swint_ogc.pth'),\n",
    "    str(OUTPUT_DIR / 'weights' / 'groundingdino_swint_ogc.pth'),\n",
    "    './weights/groundingdino_swint_ogc.pth',\n",
    "    'C:/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth',  # Windows absoluta\n",
    "]\n",
    "\n",
    "# Encontrar archivos existentes\n",
    "model_config = None\n",
    "for config_path in possible_configs:\n",
    "    if os.path.exists(config_path):\n",
    "        model_config = config_path\n",
    "        print(f\"‚úÖ Config encontrado: {config_path}\")\n",
    "        break\n",
    "\n",
    "model_weights = None\n",
    "for weights_path in possible_weights:\n",
    "    if os.path.exists(weights_path):\n",
    "        model_weights = weights_path\n",
    "        print(f\"‚úÖ Pesos encontrados: {weights_path}\")\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron los archivos\n",
    "if model_config is None or model_weights is None:\n",
    "    print(\"\\n‚ùå ERROR: No se encontraron los archivos del modelo\")\n",
    "    print(\"\\nüîç Buscando en:\")\n",
    "    print(\"\\nConfigs buscados:\")\n",
    "    for p in possible_configs:\n",
    "        print(f\"  {'‚úÖ' if os.path.exists(p) else '‚ùå'} {p}\")\n",
    "    print(\"\\nPesos buscados:\")\n",
    "    for p in possible_weights:\n",
    "        print(f\"  {'‚úÖ' if os.path.exists(p) else '‚ùå'} {p}\")\n",
    "    \n",
    "    print(\"\\nüí° SOLUCIONES:\")\n",
    "    print(\"\\n1. DESCARGAR PESOS DEL MODELO:\")\n",
    "    print(\"   - URL: https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\")\n",
    "    print(f\"   - Guardar en: {OUTPUT_DIR / 'weights' / 'groundingdino_swint_ogc.pth'}\")\n",
    "    print(f\"\\n2. CREAR DIRECTORIO Y DESCARGAR:\")\n",
    "    (OUTPUT_DIR / 'weights').mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   - mkdir -p {OUTPUT_DIR / 'weights'}\")\n",
    "    print(f\"   - Descargar desde el link anterior a la carpeta weights/\")\n",
    "    \n",
    "    print(\"\\n3. ALTERNATIVA: Usar un notebook que ya tenga acceso al modelo\")\n",
    "    print(\"   - Este notebook requiere los pesos del modelo para ejecutar inferencias\")\n",
    "    print(\"   - Si ya ejecutaste Fase 2/3/4/5, los pesos deber√≠an estar disponibles\")\n",
    "    \n",
    "    raise FileNotFoundError(\"Archivos del modelo no encontrados. Ver instrucciones arriba.\")\n",
    "\n",
    "print(f\"\\nüîÑ Cargando modelo desde:\")\n",
    "print(f\"   Config:  {model_config}\")\n",
    "print(f\"   Weights: {model_weights}\")\n",
    "\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"‚úÖ Prompt: {TEXT_PROMPT}\")\n",
    "print(f\"‚úÖ Arquitectura: GroundingDINO SwinT-OGC\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90064293",
   "metadata": {},
   "source": [
    "## 3. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0287fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones auxiliares definidas\n"
     ]
    }
   ],
   "source": [
    "def normalize_label(label):\n",
    "    \"\"\"Normaliza etiquetas del modelo a categor√≠as del dataset\"\"\"\n",
    "    synonyms = {\n",
    "        'bike': 'bicycle', \n",
    "        'motorbike': 'motorcycle', \n",
    "        'pedestrian': 'person',\n",
    "        'stop sign': 'traffic sign', \n",
    "        'red light': 'traffic light'\n",
    "    }\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Calcula IoU entre dos bounding boxes en formato [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    \n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def match_predictions_to_gt(predictions, gt_annotations, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Matchea predicciones con ground truth usando IoU\n",
    "    Retorna: lista de (pred, gt, is_correct, iou)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    used_gt = set()\n",
    "    \n",
    "    # Ordenar predicciones por score descendente\n",
    "    predictions_sorted = sorted(predictions, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for pred in predictions_sorted:\n",
    "        best_iou = 0\n",
    "        best_gt = None\n",
    "        best_gt_idx = None\n",
    "        \n",
    "        for gt_idx, gt in enumerate(gt_annotations):\n",
    "            if gt_idx in used_gt:\n",
    "                continue\n",
    "            \n",
    "            # Verificar que sean de la misma categor√≠a\n",
    "            if pred['category_id'] != gt['category_id']:\n",
    "                continue\n",
    "            \n",
    "            # Calcular IoU\n",
    "            iou = compute_iou(pred['bbox'], gt['bbox'])\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt = gt\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        # Determinar si es correcto (TP o FP)\n",
    "        is_correct = best_iou >= iou_threshold\n",
    "        \n",
    "        if is_correct and best_gt_idx is not None:\n",
    "            used_gt.add(best_gt_idx)\n",
    "        \n",
    "        matches.append({\n",
    "            'pred': pred,\n",
    "            'gt': best_gt,\n",
    "            'is_correct': is_correct,\n",
    "            'iou': best_iou\n",
    "        })\n",
    "    \n",
    "    return matches\n",
    "\n",
    "print(\"‚úÖ Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dee5f7",
   "metadata": {},
   "source": [
    "## 4. Inferencia y Recolecci√≥n de Predicciones con IoU\n",
    "\n",
    "Para RQ8 necesitamos:\n",
    "1. Predicciones del modelo con scores sem√°nticos\n",
    "2. IoU de cada predicci√≥n respecto al ground truth\n",
    "3. Guardar datos para calibraci√≥n conjunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2368fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "   INFERENCIA EN VALIDATION SET (val_eval)\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "üìä Dataset info:\n",
      "   Total im√°genes: 2000\n",
      "   Procesando: 500\n",
      "   Categor√≠as: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando im√°genes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 4883.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Inferencia completada\n",
      "   Im√°genes procesadas: 0\n",
      "   Im√°genes omitidas: 500\n",
      "   Total detecciones: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No se generaron detecciones. Verifica:\n1. Que el modelo est√© cargado correctamente\n2. Que las im√°genes existan en el path especificado\n3. Que el threshold de confianza no sea muy alto\n   Threshold actual: 0.25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Total detecciones: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_detections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_detections) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo se generaron detecciones. Verifica:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Que el modelo est√© cargado correctamente\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Que las im√°genes existan en el path especificado\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Que el threshold de confianza no sea muy alto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Threshold actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   True Positives: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39md\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_detections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   False Positives: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39md\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_detections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No se generaron detecciones. Verifica:\n1. Que el modelo est√© cargado correctamente\n2. Que las im√°genes existan en el path especificado\n3. Que el threshold de confianza no sea muy alto\n   Threshold actual: 0.25"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ8 - Inferencia y matching con ground truth\n",
    "\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"   INFERENCIA EN VALIDATION SET (val_eval)\")\n",
    "print(\"‚ïê\" * 70)\n",
    "\n",
    "# Cargar anotaciones de validaci√≥n\n",
    "val_json = DATA_DIR / 'bdd100k_coco' / 'val_eval.json'\n",
    "\n",
    "if not val_json.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontr√≥ el archivo de anotaciones: {val_json}\\n\"\n",
    "        \"Verifica que el dataset BDD100K est√© correctamente instalado en {DATA_DIR}\"\n",
    "    )\n",
    "\n",
    "coco = COCO(str(val_json))\n",
    "\n",
    "# Mapeo de categor√≠as BDD100K a IDs COCO\n",
    "cat_name_to_id = {cat['name']: cat['id'] for cat in coco.loadCats(coco.getCatIds())}\n",
    "\n",
    "# Obtener lista de im√°genes\n",
    "img_ids = coco.getImgIds()\n",
    "np.random.shuffle(img_ids)\n",
    "img_ids = img_ids[:CONFIG['sample_size']]\n",
    "\n",
    "print(f\"\\nüìä Dataset info:\")\n",
    "print(f\"   Total im√°genes: {len(coco.getImgIds())}\")\n",
    "print(f\"   Procesando: {len(img_ids)}\")\n",
    "print(f\"   Categor√≠as: {len(CONFIG['categories'])}\\n\")\n",
    "\n",
    "# Almacenar predicciones con IoU\n",
    "all_detections = []\n",
    "processed_images = 0\n",
    "skipped_images = 0\n",
    "\n",
    "# Inferencia\n",
    "for img_id in tqdm(img_ids, desc=\"Procesando im√°genes\"):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = DATA_DIR / 'bdd100k' / 'bdd100k' / 'images' / '100k' / 'val' / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        skipped_images += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Cargar imagen\n",
    "        image_source, image = load_image(str(img_path))\n",
    "        \n",
    "        # Predicci√≥n\n",
    "        with torch.no_grad():\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=CONFIG['conf_threshold'],\n",
    "                text_threshold=0.25,\n",
    "                device=CONFIG['device']\n",
    "            )\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            processed_images += 1\n",
    "            continue\n",
    "        \n",
    "        # Convertir boxes a formato COCO [x1, y1, x2, y2]\n",
    "        h, w, _ = image_source.shape\n",
    "        boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "        boxes_xyxy = boxes_xyxy.cpu().numpy()\n",
    "        scores = logits.cpu().numpy()\n",
    "        \n",
    "        # Preparar predicciones\n",
    "        predictions = []\n",
    "        for box, score, phrase in zip(boxes_xyxy, scores, phrases):\n",
    "            cat = normalize_label(phrase)\n",
    "            if cat not in CONFIG['categories']:\n",
    "                continue\n",
    "            \n",
    "            predictions.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': float(score),\n",
    "                'category': cat,\n",
    "                'category_id': cat_name_to_id.get(cat, -1)\n",
    "            })\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            processed_images += 1\n",
    "            continue\n",
    "        \n",
    "        # Cargar ground truth\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        gt_annotations = []\n",
    "        for ann in anns:\n",
    "            cat_id = ann['category_id']\n",
    "            cat_name = coco.loadCats(cat_id)[0]['name']\n",
    "            if cat_name not in CONFIG['categories']:\n",
    "                continue\n",
    "            \n",
    "            # Convertir bbox de COCO [x, y, w, h] a [x1, y1, x2, y2]\n",
    "            x, y, bw, bh = ann['bbox']\n",
    "            bbox_xyxy = [x, y, x + bw, y + bh]\n",
    "            \n",
    "            gt_annotations.append({\n",
    "                'bbox': bbox_xyxy,\n",
    "                'category': cat_name,\n",
    "                'category_id': cat_id\n",
    "            })\n",
    "        \n",
    "        # Matchear predicciones con ground truth\n",
    "        matches = match_predictions_to_gt(predictions, gt_annotations, CONFIG['iou_matching'])\n",
    "        \n",
    "        # Guardar detecciones con IoU\n",
    "        for match in matches:\n",
    "            all_detections.append({\n",
    "                'image_id': img_id,\n",
    "                'bbox': match['pred']['bbox'],\n",
    "                'score': match['pred']['score'],\n",
    "                'category': match['pred']['category'],\n",
    "                'category_id': match['pred']['category_id'],\n",
    "                'is_correct': match['is_correct'],\n",
    "                'iou': match['iou'],\n",
    "                'has_gt_match': match['gt'] is not None\n",
    "            })\n",
    "        \n",
    "        processed_images += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error procesando imagen {img_id}: {e}\")\n",
    "        skipped_images += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Inferencia completada\")\n",
    "print(f\"   Im√°genes procesadas: {processed_images}\")\n",
    "print(f\"   Im√°genes omitidas: {skipped_images}\")\n",
    "print(f\"   Total detecciones: {len(all_detections)}\")\n",
    "\n",
    "if len(all_detections) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No se generaron detecciones. Verifica:\\n\"\n",
    "        \"1. Que el modelo est√© cargado correctamente\\n\"\n",
    "        \"2. Que las im√°genes existan en el path especificado\\n\"\n",
    "        \"3. Que el threshold de confianza no sea muy alto\\n\"\n",
    "        f\"   Threshold actual: {CONFIG['conf_threshold']}\"\n",
    "    )\n",
    "\n",
    "print(f\"   True Positives: {sum(d['is_correct'] for d in all_detections)}\")\n",
    "print(f\"   False Positives: {sum(not d['is_correct'] for d in all_detections)}\")\n",
    "\n",
    "# Convertir a DataFrame y guardar\n",
    "df_detections = pd.DataFrame(all_detections)\n",
    "df_detections.to_parquet(OUTPUT_DIR / 'detections_raw.parquet', index=False)\n",
    "print(f\"‚úÖ Detecciones guardadas en: {OUTPUT_DIR / 'detections_raw.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19894b13",
   "metadata": {},
   "source": [
    "## 5. Calibraci√≥n Conjunta Sem√°ntico-Geom√©trica\n",
    "\n",
    "Implementamos tres estrategias de scoring:\n",
    "1. **Raw Score**: Score sem√°ntico crudo del modelo (baseline)\n",
    "2. **Temperature Scaling (cls only)**: Calibraci√≥n solo del score sem√°ntico\n",
    "3. **Joint Calibration (cls+loc)**: Calibraci√≥n conjunta que incorpora la calidad de localizaci√≥n (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d542f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos si existen, sino usar los reci√©n generados\n",
    "if (OUTPUT_DIR / 'detections_raw.parquet').exists():\n",
    "    df = pd.read_parquet(OUTPUT_DIR / 'detections_raw.parquet')\n",
    "    print(f\"üìä Datos cargados desde archivo: {len(df)} detecciones\")\n",
    "else:\n",
    "    if 'df_detections' in locals():\n",
    "        df = df_detections.copy()\n",
    "        print(f\"üìä Usando datos reci√©n generados: {len(df)} detecciones\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No se encontraron datos. Ejecuta primero la celda de inferencia (Celda 4)\")\n",
    "\n",
    "print(f\"   TP: {df['is_correct'].sum()}\")\n",
    "print(f\"   FP: {(~df['is_correct']).sum()}\")\n",
    "\n",
    "# Verificar que tenemos datos suficientes\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No hay detecciones para procesar. Verifica la inferencia.\")\n",
    "\n",
    "# Convertir scores a logits para calibraci√≥n\n",
    "# logit = log(score / (1 - score))\n",
    "def score_to_logit(score, epsilon=1e-7):\n",
    "    \"\"\"Convierte score [0, 1] a logit (-inf, inf)\"\"\"\n",
    "    score = np.clip(score, epsilon, 1 - epsilon)\n",
    "    return np.log(score / (1 - score))\n",
    "\n",
    "def logit_to_score(logit):\n",
    "    \"\"\"Convierte logit (-inf, inf) a score [0, 1]\"\"\"\n",
    "    return 1 / (1 + np.exp(-logit))\n",
    "\n",
    "df['logit'] = df['score'].apply(score_to_logit)\n",
    "\n",
    "# === 1. Raw Scores (baseline) ===\n",
    "df['score_raw'] = df['score']\n",
    "\n",
    "# === 2. Temperature Scaling (solo sem√°ntica) ===\n",
    "# Optimizar temperatura T minimizando NLL en el conjunto de calibraci√≥n\n",
    "\n",
    "def nll_loss(T, logits, labels):\n",
    "    \"\"\"Negative Log-Likelihood para calibraci√≥n de temperatura\"\"\"\n",
    "    scaled_logits = logits / T\n",
    "    probs = logit_to_score(scaled_logits)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # NLL: -log(p) si label=1, -log(1-p) si label=0\n",
    "    nll = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    return nll\n",
    "\n",
    "# Optimizar temperatura\n",
    "logits = df['logit'].values\n",
    "labels = df['is_correct'].astype(int).values\n",
    "\n",
    "print(\"\\nüîß Optimizando temperatura (sem√°ntica only)...\")\n",
    "\n",
    "# Verificar que tenemos suficientes muestras positivas y negativas\n",
    "n_pos = labels.sum()\n",
    "n_neg = len(labels) - n_pos\n",
    "print(f\"   Muestras: {len(labels)} total ({n_pos} TP, {n_neg} FP)\")\n",
    "\n",
    "if n_pos == 0 or n_neg == 0:\n",
    "    print(\"‚ö†Ô∏è  Warning: Solo hay una clase. Usando T=1.0\")\n",
    "    T_optimal = 1.0\n",
    "else:\n",
    "    try:\n",
    "        result = minimize(\n",
    "            lambda T: nll_loss(T, logits, labels),\n",
    "            x0=1.0,\n",
    "            bounds=[(0.1, 10.0)],\n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "        T_optimal = result.x[0]\n",
    "        print(f\"‚úÖ Temperatura √≥ptima: T = {T_optimal:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error en optimizaci√≥n de temperatura: {e}\")\n",
    "        print(f\"‚ö†Ô∏è  Usando T=1.0 por defecto\")\n",
    "        T_optimal = 1.0\n",
    "\n",
    "# Aplicar temperature scaling\n",
    "df['score_temp'] = logit_to_score(df['logit'] / T_optimal)\n",
    "\n",
    "# === 3. Joint Calibration (sem√°ntica + geom√©trica) ===\n",
    "# Score calibrado = f(score_semantic, IoU)\n",
    "# Usamos una funci√≥n que combina ambos: score_joint = score_sem^Œ± * IoU^Œ≤\n",
    "# Optimizamos Œ±, Œ≤ para maximizar correlaci√≥n con correctness\n",
    "\n",
    "def joint_score_function(params, scores, ious):\n",
    "    \"\"\"Calcula score conjunto como combinaci√≥n ponderada\"\"\"\n",
    "    alpha, beta = params\n",
    "    # Normalizar para que est√© en [0, 1]\n",
    "    joint = (scores ** alpha) * (ious ** beta)\n",
    "    return joint\n",
    "\n",
    "def joint_calibration_loss(params, scores, ious, labels):\n",
    "    \"\"\"Loss para calibraci√≥n conjunta: queremos que el score prediga correctness\"\"\"\n",
    "    joint_scores = joint_score_function(params, scores, ious)\n",
    "    joint_scores = np.clip(joint_scores, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # NLL-like loss\n",
    "    loss = -np.mean(labels * np.log(joint_scores) + (1 - labels) * np.log(1 - joint_scores))\n",
    "    return loss\n",
    "\n",
    "print(\"\\nüîß Optimizando calibraci√≥n conjunta (sem√°ntica + geom√©trica)...\")\n",
    "\n",
    "# Usar scores ya calibrados por temperatura como base\n",
    "scores_for_joint = df['score_temp'].values\n",
    "ious = df['iou'].values\n",
    "\n",
    "# Para FP sin GT match, usar IoU m√≠nimo (evitar divisi√≥n por cero)\n",
    "ious = np.where(ious == 0, 0.01, ious)\n",
    "\n",
    "# Verificar que tenemos datos v√°lidos\n",
    "if len(scores_for_joint) == 0:\n",
    "    raise ValueError(\"No hay scores para calibraci√≥n conjunta\")\n",
    "\n",
    "try:\n",
    "    result_joint = minimize(\n",
    "        lambda params: joint_calibration_loss(params, scores_for_joint, ious, labels),\n",
    "        x0=[1.0, 1.0],\n",
    "        bounds=[(0.1, 3.0), (0.1, 3.0)],\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    \n",
    "    alpha_opt, beta_opt = result_joint.x\n",
    "    print(f\"‚úÖ Par√°metros √≥ptimos: Œ± = {alpha_opt:.4f}, Œ≤ = {beta_opt:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error en optimizaci√≥n conjunta: {e}\")\n",
    "    print(f\"‚ö†Ô∏è  Usando par√°metros por defecto: Œ± = 1.0, Œ≤ = 1.0\")\n",
    "    alpha_opt, beta_opt = 1.0, 1.0\n",
    "\n",
    "# Aplicar calibraci√≥n conjunta\n",
    "df['score_joint'] = joint_score_function([alpha_opt, beta_opt], scores_for_joint, ious)\n",
    "\n",
    "# Guardar par√°metros de calibraci√≥n\n",
    "calibration_params = {\n",
    "    'temperature': float(T_optimal),\n",
    "    'alpha': float(alpha_opt),\n",
    "    'beta': float(beta_opt),\n",
    "    'n_detections': len(df),\n",
    "    'n_tp': int(df['is_correct'].sum()),\n",
    "    'n_fp': int((~df['is_correct']).sum())\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'calibration_params.json', 'w') as f:\n",
    "    json.dump(calibration_params, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Par√°metros guardados en: {OUTPUT_DIR / 'calibration_params.json'}\")\n",
    "\n",
    "# Guardar dataframe con scores calibrados\n",
    "df.to_parquet(OUTPUT_DIR / 'detections_calibrated.parquet', index=False)\n",
    "print(f\"‚úÖ Detecciones con calibraci√≥n guardadas en: {OUTPUT_DIR / 'detections_calibrated.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b63d5",
   "metadata": {},
   "source": [
    "## 6. Tabla RQ8.1 ‚Äî Score‚ÄìIoU Alignment\n",
    "\n",
    "Evaluamos la correlaci√≥n entre detection score e IoU antes y despu√©s de calibraci√≥n conjunta usando:\n",
    "- **Spearman œÅ**: Correlaci√≥n de ranking (monoton√≠a)\n",
    "- **Kendall œÑ**: Concordancia de pares ordenados\n",
    "- **ECE-IoU**: Expected Calibration Error adaptado para IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos calibrados\n",
    "calibrated_file = OUTPUT_DIR / 'detections_calibrated.parquet'\n",
    "\n",
    "if not calibrated_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontr√≥ el archivo de detecciones calibradas: {calibrated_file}\\n\"\n",
    "        \"Por favor ejecuta primero la celda de calibraci√≥n (Celda 5)\"\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(calibrated_file)\n",
    "print(f\"‚úÖ Datos calibrados cargados: {len(df)} detecciones\")\n",
    "\n",
    "def compute_ece_iou(scores, ious, n_bins=10):\n",
    "    \"\"\"\n",
    "    Expected Calibration Error adaptado para IoU:\n",
    "    Mide qu√© tan bien el score predice el IoU promedio\n",
    "    \"\"\"\n",
    "    # Crear bins por score\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(scores, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    ece = 0.0\n",
    "    total_samples = len(scores)\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score promedio en este bin\n",
    "        avg_score = scores[mask].mean()\n",
    "        \n",
    "        # IoU promedio en este bin (lo que realmente observamos)\n",
    "        avg_iou = ious[mask].mean()\n",
    "        \n",
    "        # ECE: diferencia entre score esperado y IoU observado\n",
    "        weight = mask.sum() / total_samples\n",
    "        ece += weight * abs(avg_score - avg_iou)\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# Calcular m√©tricas para cada m√©todo\n",
    "methods = ['score_raw', 'score_temp', 'score_joint']\n",
    "method_names = ['Raw score', 'Temp-scaled (cls only)', 'Joint calibrated (cls+loc)']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method, method_name in zip(methods, method_names):\n",
    "    scores = df[method].values\n",
    "    ious = df['iou'].values\n",
    "    \n",
    "    # Spearman correlation\n",
    "    spearman_rho, _ = spearmanr(scores, ious)\n",
    "    \n",
    "    # Kendall tau\n",
    "    kendall_tau, _ = kendalltau(scores, ious)\n",
    "    \n",
    "    # ECE-IoU\n",
    "    ece_iou = compute_ece_iou(scores, ious, n_bins=CONFIG['n_bins'])\n",
    "    \n",
    "    results.append({\n",
    "        'Scoring rule': method_name,\n",
    "        'Spearman œÅ(score, IoU) ‚Üë': spearman_rho,\n",
    "        'Kendall œÑ ‚Üë': kendall_tau,\n",
    "        'ECE-IoU ‚Üì': ece_iou\n",
    "    })\n",
    "\n",
    "df_table1 = pd.DataFrame(results)\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   TABLE RQ8.1 ‚Äî Score‚ÄìIoU Alignment\")\n",
    "print(\"=\" * 70)\n",
    "print(df_table1.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar tabla\n",
    "df_table1.to_csv(OUTPUT_DIR / 'table_rq8_1_score_iou_alignment.csv', index=False)\n",
    "print(f\"\\n‚úÖ Tabla guardada en: {OUTPUT_DIR / 'table_rq8_1_score_iou_alignment.csv'}\")\n",
    "\n",
    "# Guardar tambi√©n en JSON para f√°cil lectura\n",
    "table1_dict = df_table1.to_dict(orient='records')\n",
    "with open(OUTPUT_DIR / 'table_rq8_1.json', 'w') as f:\n",
    "    json.dump(table1_dict, f, indent=2)\n",
    "print(f\"‚úÖ Tabla (JSON) guardada en: {OUTPUT_DIR / 'table_rq8_1.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa75bf3",
   "metadata": {},
   "source": [
    "## 7. Figura RQ8.1 ‚Äî Score-IoU Reliability Diagram\n",
    "\n",
    "Visualiza la confiabilidad de los scores respecto a la calidad geom√©trica (mean IoU por bin de confianza).\n",
    "La calibraci√≥n conjunta mejora la alineaci√≥n monot√≥nica entre score y precisi√≥n de localizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reliability diagram: Score vs Mean IoU por bin\n",
    "\n",
    "# Cargar datos calibrados si no est√°n en memoria\n",
    "if 'df' not in locals() or df is None:\n",
    "    calibrated_file = OUTPUT_DIR / 'detections_calibrated.parquet'\n",
    "    if not calibrated_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontr√≥ el archivo de detecciones calibradas: {calibrated_file}\\n\"\n",
    "            \"Por favor ejecuta primero las celdas anteriores (5-6)\"\n",
    "        )\n",
    "    df = pd.read_parquet(calibrated_file)\n",
    "    print(f\"‚úÖ Datos cargados: {len(df)} detecciones\")\n",
    "\n",
    "def compute_reliability_data(scores, ious, n_bins=10):\n",
    "    \"\"\"Calcula datos para reliability diagram\"\"\"\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_indices = np.digitize(scores, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    avg_scores = []\n",
    "    avg_ious = []\n",
    "    counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() == 0:\n",
    "            avg_scores.append(bin_centers[i])\n",
    "            avg_ious.append(0)\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            avg_scores.append(scores[mask].mean())\n",
    "            avg_ious.append(ious[mask].mean())\n",
    "            counts.append(mask.sum())\n",
    "    \n",
    "    return np.array(avg_scores), np.array(avg_ious), np.array(counts)\n",
    "\n",
    "# Preparar datos para cada m√©todo\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "methods = ['score_raw', 'score_temp', 'score_joint']\n",
    "method_names = ['Raw Score', 'Temperature Scaling (cls only)', 'Joint Calibration (cls+loc)']\n",
    "colors = ['#E74C3C', '#F39C12', '#27AE60']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for method, method_name, color, marker in zip(methods, method_names, colors, markers):\n",
    "    scores = df[method].values\n",
    "    ious = df['iou'].values\n",
    "    \n",
    "    avg_scores, avg_ious, counts = compute_reliability_data(scores, ious, n_bins=CONFIG['n_bins'])\n",
    "    \n",
    "    # Plotear con tama√±o proporcional al n√∫mero de muestras\n",
    "    sizes = (counts / counts.max()) * 200 + 50\n",
    "    \n",
    "    ax.scatter(avg_scores, avg_ious, s=sizes, alpha=0.7, \n",
    "              label=method_name, color=color, marker=marker, edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    # L√≠nea de conexi√≥n\n",
    "    ax.plot(avg_scores, avg_ious, color=color, alpha=0.4, linestyle='--', linewidth=1.5)\n",
    "\n",
    "# L√≠nea de perfecta calibraci√≥n\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=2, label='Perfect Calibration')\n",
    "\n",
    "ax.set_xlabel('Detection Score (Confidence)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Mean IoU (Localization Quality)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Figure RQ8.1. Reliability of Detection Scores vs Geometric Quality\\nJoint Calibration Improves Score-IoU Monotonic Alignment', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=11, loc='lower right', framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle=':', linewidth=1)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "fig.savefig(OUTPUT_DIR / 'Fig_RQ8_1_score_iou_reliability.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig(OUTPUT_DIR / 'Fig_RQ8_1_score_iou_reliability.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Figura RQ8.1 guardada en:\")\n",
    "print(f\"   {OUTPUT_DIR / 'Fig_RQ8_1_score_iou_reliability.png'}\")\n",
    "print(f\"   {OUTPUT_DIR / 'Fig_RQ8_1_score_iou_reliability.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580c255",
   "metadata": {},
   "source": [
    "## 8. Tabla RQ8.2 ‚Äî Ranking and Selection Utility\n",
    "\n",
    "Evaluamos mejoras en ranking y selecci√≥n inducidas por calibraci√≥n conjunta con presupuesto fijo de propuestas (Top-K).\n",
    "M√©tricas:\n",
    "- **Precision@K**: Proporci√≥n de detecciones correctas en el Top-K\n",
    "- **Mean IoU of selected**: Calidad de localizaci√≥n promedio del Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular Precision@K y Mean IoU para diferentes presupuestos\n",
    "\n",
    "# Cargar datos calibrados si no est√°n en memoria\n",
    "if 'df' not in locals() or df is None:\n",
    "    calibrated_file = OUTPUT_DIR / 'detecciones_calibradas.parquet'\n",
    "    if not calibrated_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontr√≥ el archivo de detecciones calibradas: {calibrated_file}\\n\"\n",
    "            \"Por favor ejecuta primero las celdas anteriores (5-7)\"\n",
    "        )\n",
    "    df = pd.read_parquet(calibrated_file)\n",
    "    print(f\"‚úÖ Datos cargados: {len(df)} detecciones\")\n",
    "\n",
    "def compute_precision_at_k(scores, labels, k):\n",
    "    \"\"\"Calcula Precision@K: proporci√≥n de TP en el Top-K\"\"\"\n",
    "    # Ordenar por score descendente\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    top_k_labels = labels[sorted_indices[:k]]\n",
    "    return top_k_labels.sum() / k\n",
    "\n",
    "def compute_mean_iou_at_k(scores, ious, k):\n",
    "    \"\"\"Calcula Mean IoU del Top-K por score\"\"\"\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    top_k_ious = ious[sorted_indices[:k]]\n",
    "    return top_k_ious.mean()\n",
    "\n",
    "# Evaluar para cada presupuesto K\n",
    "results = []\n",
    "\n",
    "for k in CONFIG['top_k_values']:\n",
    "    # Asegurar que K no exceda el n√∫mero de detecciones\n",
    "    k_actual = min(k, len(df))\n",
    "    \n",
    "    # Raw scores\n",
    "    prec_raw = compute_precision_at_k(df['score_raw'].values, df['is_correct'].values, k_actual)\n",
    "    iou_raw = compute_mean_iou_at_k(df['score_raw'].values, df['iou'].values, k_actual)\n",
    "    \n",
    "    # Calibrated scores\n",
    "    prec_cal = compute_precision_at_k(df['score_joint'].values, df['is_correct'].values, k_actual)\n",
    "    iou_cal = compute_mean_iou_at_k(df['score_joint'].values, df['iou'].values, k_actual)\n",
    "    \n",
    "    results.append({\n",
    "        'Budget': f'Top-{k}',\n",
    "        'Metric': 'Precision@K ‚Üë',\n",
    "        'Raw': prec_raw,\n",
    "        'Calibrated': prec_cal\n",
    "    })\n",
    "    \n",
    "    # Solo agregar Mean IoU para el √∫ltimo presupuesto (como en el ejemplo)\n",
    "    if k == CONFIG['top_k_values'][-1]:\n",
    "        results.append({\n",
    "            'Budget': f'Top-{k}',\n",
    "            'Metric': 'Mean IoU of selected ‚Üë',\n",
    "            'Raw': iou_raw,\n",
    "            'Calibrated': iou_cal\n",
    "        })\n",
    "\n",
    "df_table2 = pd.DataFrame(results)\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   TABLE RQ8.2 ‚Äî Ranking and Selection Utility\")\n",
    "print(\"=\" * 70)\n",
    "print(df_table2.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar tabla\n",
    "df_table2.to_csv(OUTPUT_DIR / 'table_rq8_2_ranking_utility.csv', index=False)\n",
    "print(f\"\\n‚úÖ Tabla guardada en: {OUTPUT_DIR / 'table_rq8_2_ranking_utility.csv'}\")\n",
    "\n",
    "# Guardar tambi√©n en JSON\n",
    "table2_dict = df_table2.to_dict(orient='records')\n",
    "with open(OUTPUT_DIR / 'table_rq8_2.json', 'w') as f:\n",
    "    json.dump(table2_dict, f, indent=2)\n",
    "print(f\"‚úÖ Tabla (JSON) guardada en: {OUTPUT_DIR / 'table_rq8_2.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b6ca6",
   "metadata": {},
   "source": [
    "## 9. Figura RQ8.2 ‚Äî Precision@K Curves\n",
    "\n",
    "Visualiza Precision@K para ranking de detecciones bajo scores crudos y calibrados (escala log K).\n",
    "La calibraci√≥n mejora la calidad del ranking, soportando selecci√≥n confiable m√°s all√° del mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar curvas de Precision@K para diferentes valores de K\n",
    "\n",
    "# Cargar datos calibrados si no est√°n en memoria\n",
    "if 'df' not in locals() or df is None:\n",
    "    calibrated_file = OUTPUT_DIR / 'detections_calibrated.parquet'\n",
    "    if not calibrated_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontr√≥ el archivo de detecciones calibradas: {calibrated_file}\\n\"\n",
    "            \"Por favor ejecuta primero las celdas anteriores (5-8)\"\n",
    "        )\n",
    "    df = pd.read_parquet(calibrated_file)\n",
    "    print(f\"‚úÖ Datos cargados: {len(df)} detecciones\")\n",
    "\n",
    "# Rango de K valores (escala logar√≠tmica)\n",
    "k_values = np.unique(np.logspace(1, np.log10(len(df)), 30).astype(int))\n",
    "k_values = k_values[k_values <= len(df)]\n",
    "\n",
    "# Calcular Precision@K para cada m√©todo\n",
    "methods = {\n",
    "    'Raw Score': df['score_raw'].values,\n",
    "    'Temperature Scaling': df['score_temp'].values,\n",
    "    'Joint Calibration': df['score_joint'].values\n",
    "}\n",
    "\n",
    "colors = {'Raw Score': '#E74C3C', \n",
    "          'Temperature Scaling': '#F39C12', \n",
    "          'Joint Calibration': '#27AE60'}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "for method_name, scores in methods.items():\n",
    "    precisions = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        prec = compute_precision_at_k(scores, df['is_correct'].values, k)\n",
    "        precisions.append(prec)\n",
    "    \n",
    "    ax.plot(k_values, precisions, marker='o', markersize=5, linewidth=2.5,\n",
    "            label=method_name, color=colors[method_name], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('K (Number of Top Detections, log-scale)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Precision@K', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Figure RQ8.2. Precision@K for Ranking Detections\\nCalibration Improves Ranking Quality Beyond mAP', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3, linestyle=':', linewidth=1, which='both')\n",
    "ax.legend(fontsize=12, loc='best', framealpha=0.95)\n",
    "\n",
    "# Marcar los valores espec√≠ficos de la tabla\n",
    "for k in CONFIG['top_k_values']:\n",
    "    if k <= len(df):\n",
    "        ax.axvline(k, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "        ax.text(k, ax.get_ylim()[1] * 0.98, f'K={k}', \n",
    "               ha='center', va='top', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "fig.savefig(OUTPUT_DIR / 'Fig_RQ8_2_precision_at_k.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig(OUTPUT_DIR / 'Fig_RQ8_2_precision_at_k.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Figura RQ8.2 guardada en:\")\n",
    "print(f\"   {OUTPUT_DIR / 'Fig_RQ8_2_precision_at_k.png'}\")\n",
    "print(f\"   {OUTPUT_DIR / 'Fig_RQ8_2_precision_at_k.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb814d2",
   "metadata": {},
   "source": [
    "## 10. Resumen y Verificaci√≥n de Resultados\n",
    "\n",
    "Verificaci√≥n de que todos los archivos esperados se han generado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que todos los archivos se generaron correctamente\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"   VERIFICACI√ìN DE RESULTADOS RQ8\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Lista de archivos esperados\n",
    "expected_files = {\n",
    "    'Configuraci√≥n': 'config_rq8.yaml',\n",
    "    'Par√°metros de calibraci√≥n': 'calibration_params.json',\n",
    "    'Detecciones crudas': 'detections_raw.parquet',\n",
    "    'Detecciones calibradas': 'detections_calibrated.parquet',\n",
    "    'Tabla RQ8.1 (CSV)': 'table_rq8_1_score_iou_alignment.csv',\n",
    "    'Tabla RQ8.1 (JSON)': 'table_rq8_1.json',\n",
    "    'Tabla RQ8.2 (CSV)': 'table_rq8_2_ranking_utility.csv',\n",
    "    'Tabla RQ8.2 (JSON)': 'table_rq8_2.json',\n",
    "    'Figura RQ8.1 (PNG)': 'Fig_RQ8_1_score_iou_reliability.png',\n",
    "    'Figura RQ8.1 (PDF)': 'Fig_RQ8_1_score_iou_reliability.pdf',\n",
    "    'Figura RQ8.2 (PNG)': 'Fig_RQ8_2_precision_at_k.png',\n",
    "    'Figura RQ8.2 (PDF)': 'Fig_RQ8_2_precision_at_k.pdf'\n",
    "}\n",
    "\n",
    "all_generated = True\n",
    "missing_files = []\n",
    "\n",
    "print(\"\\nüìÅ Archivos generados:\")\n",
    "for desc, filename in expected_files.items():\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    exists = filepath.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {desc}: {filename}\")\n",
    "    if not exists:\n",
    "        all_generated = False\n",
    "        missing_files.append(filename)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if all_generated:\n",
    "    print(\"‚úÖ TODOS LOS ARCHIVOS GENERADOS CORRECTAMENTE\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  ALGUNOS ARCHIVOS NO SE GENERARON: {len(missing_files)} archivos faltantes\")\n",
    "    print(f\"    Archivos faltantes: {', '.join(missing_files)}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Solo mostrar resumen si los archivos cr√≠ticos existen\n",
    "if (OUTPUT_DIR / 'calibration_params.json').exists():\n",
    "    # Cargar y mostrar resumen de resultados\n",
    "    print(\"\\nüìä RESUMEN DE RESULTADOS:\")\n",
    "\n",
    "    # Par√°metros de calibraci√≥n\n",
    "    with open(OUTPUT_DIR / 'calibration_params.json', 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    print(f\"\\nüîß Par√°metros de calibraci√≥n:\")\n",
    "    print(f\"   Temperatura (T): {params['temperature']:.4f}\")\n",
    "    print(f\"   Alpha (Œ±): {params['alpha']:.4f}\")\n",
    "    print(f\"   Beta (Œ≤): {params['beta']:.4f}\")\n",
    "    print(f\"   Detecciones totales: {params['n_detections']}\")\n",
    "    print(f\"   True Positives: {params['n_tp']}\")\n",
    "    print(f\"   False Positives: {params['n_fp']}\")\n",
    "    \n",
    "    # Tabla 1: Score-IoU Alignment\n",
    "    if (OUTPUT_DIR / 'table_rq8_1_score_iou_alignment.csv').exists():\n",
    "        print(f\"\\nüìä Tabla RQ8.1 - Score-IoU Alignment:\")\n",
    "        df_t1 = pd.read_csv(OUTPUT_DIR / 'table_rq8_1_score_iou_alignment.csv')\n",
    "        print(df_t1.to_string(index=False))\n",
    "        \n",
    "        # An√°lisis de mejoras\n",
    "        print(f\"\\nüìà MEJORAS POR CALIBRACI√ìN CONJUNTA:\")\n",
    "        \n",
    "        # Mejora en correlaci√≥n Spearman\n",
    "        spearman_raw = df_t1[df_t1['Scoring rule'] == 'Raw score']['Spearman œÅ(score, IoU) ‚Üë'].values[0]\n",
    "        spearman_joint = df_t1[df_t1['Scoring rule'] == 'Joint calibrated (cls+loc)']['Spearman œÅ(score, IoU) ‚Üë'].values[0]\n",
    "        spearman_improvement = ((spearman_joint - spearman_raw) / abs(spearman_raw)) * 100\n",
    "        \n",
    "        print(f\"   Spearman œÅ: {spearman_raw:.3f} ‚Üí {spearman_joint:.3f} ({spearman_improvement:+.1f}%)\")\n",
    "        \n",
    "        # Mejora en ECE-IoU\n",
    "        ece_raw = df_t1[df_t1['Scoring rule'] == 'Raw score']['ECE-IoU ‚Üì'].values[0]\n",
    "        ece_joint = df_t1[df_t1['Scoring rule'] == 'Joint calibrated (cls+loc)']['ECE-IoU ‚Üì'].values[0]\n",
    "        ece_improvement = ((ece_raw - ece_joint) / ece_raw) * 100\n",
    "        \n",
    "        print(f\"   ECE-IoU: {ece_raw:.3f} ‚Üí {ece_joint:.3f} ({ece_improvement:+.1f}% mejora)\")\n",
    "    \n",
    "    # Tabla 2: Ranking Utility\n",
    "    if (OUTPUT_DIR / 'table_rq8_2_ranking_utility.csv').exists():\n",
    "        print(f\"\\nüìä Tabla RQ8.2 - Ranking and Selection Utility:\")\n",
    "        df_t2 = pd.read_csv(OUTPUT_DIR / 'table_rq8_2_ranking_utility.csv')\n",
    "        print(df_t2.to_string(index=False))\n",
    "        \n",
    "        # Mejora en Precision@100\n",
    "        prec_100_rows = df_t2[(df_t2['Budget'] == 'Top-100') & (df_t2['Metric'] == 'Precision@K ‚Üë')]\n",
    "        if len(prec_100_rows) > 0:\n",
    "            prec_100_raw = prec_100_rows['Raw'].values[0]\n",
    "            prec_100_cal = prec_100_rows['Calibrated'].values[0]\n",
    "            prec_100_improvement = ((prec_100_cal - prec_100_raw) / prec_100_raw) * 100\n",
    "            \n",
    "            print(f\"\\n   Precision@100: {prec_100_raw:.3f} ‚Üí {prec_100_cal:.3f} ({prec_100_improvement:+.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No se encontraron archivos de resultados. Ejecuta las celdas anteriores primero.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_generated:\n",
    "    print(\"‚úÖ RQ8 COMPLETADO EXITOSAMENTE\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  RQ8 INCOMPLETO - Ejecuta las celdas faltantes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÇ Todos los resultados guardados en: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bbde4",
   "metadata": {},
   "source": [
    "## 11. Interpretaci√≥n de Resultados\n",
    "\n",
    "### Hallazgos Clave de RQ8:\n",
    "\n",
    "**1. Desalineaci√≥n Inicial Score-IoU (Tabla RQ8.1)**\n",
    "- Los **scores sem√°nticos crudos** muestran correlaci√≥n d√©bil con la calidad de localizaci√≥n (IoU)\n",
    "- Spearman œÅ bajo indica que las detecciones con mayor confianza NO necesariamente tienen mejor localizaci√≥n\n",
    "- ECE-IoU alto revela que el score no es un buen predictor de la precisi√≥n geom√©trica\n",
    "\n",
    "**2. Calibraci√≥n Solo Sem√°ntica es Insuficiente**\n",
    "- Temperature scaling mejora la calibraci√≥n de probabilidades, pero la alineaci√≥n score-IoU mejora poco\n",
    "- La calibraci√≥n tradicional ignora completamente la calidad de localizaci√≥n\n",
    "- Necesitamos incorporar informaci√≥n geom√©trica expl√≠citamente\n",
    "\n",
    "**3. Calibraci√≥n Conjunta Restaura Monotonicidad (Figura RQ8.1)**\n",
    "- La calibraci√≥n conjunta (cls+loc) logra alineaci√≥n casi perfecta entre score y IoU promedio\n",
    "- Los bins de mayor confianza ahora tienen consistentemente mayor IoU\n",
    "- Esto es cr√≠tico para aplicaciones que requieren confiabilidad en la localizaci√≥n (e.g., conducci√≥n aut√≥noma)\n",
    "\n",
    "**4. Mejoras Sustanciales en Ranking (Tabla RQ8.2, Figura RQ8.2)**\n",
    "- **Precision@K mejora consistentemente** en todos los presupuestos (Top-100, 200, 400)\n",
    "- La mejora es m√°s pronunciada para K peque√±o (donde la selecci√≥n es m√°s cr√≠tica)\n",
    "- El Mean IoU del Top-400 seleccionado aumenta significativamente\n",
    "- Estas mejoras son **ortogonales al mAP**: el modelo sigue siendo el mismo, pero los scores son m√°s √∫tiles\n",
    "\n",
    "**5. Implicaciones Pr√°cticas**\n",
    "- **Para sistemas de detecci√≥n en producci√≥n**: La calibraci√≥n conjunta permite:\n",
    "  - Seleccionar propuestas m√°s confiables dado un presupuesto computacional\n",
    "  - Ranking m√°s confiable para post-procesamiento\n",
    "  - Detecciones mejor localizadas en escenarios de alta confianza\n",
    "  \n",
    "- **Para aplicaciones cr√≠ticas**: Cuando la localizaci√≥n precisa es esencial (e.g., evitar colisiones), usar scores calibrados conjuntamente reduce el riesgo de confiar en detecciones mal localizadas\n",
    "\n",
    "**6. Respuesta a RQ8**\n",
    "> *\"How can semantic confidence and localization quality be jointly calibrated to yield meaningful scores for ranking/selection?\"*\n",
    "\n",
    "‚úÖ **Respuesta**: Mediante optimizaci√≥n conjunta de una funci√≥n que combina scores sem√°nticos calibrados y calidad de localizaci√≥n (IoU), podemos restaurar la monotonicidad entre confianza y precisi√≥n geom√©trica. Esto mejora m√©tricas de ranking (Precision@K, Mean IoU selected) sin cambiar el mAP, proporcionando scores m√°s √∫tiles para selecci√≥n reliability-aware en aplicaciones downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2d06d",
   "metadata": {},
   "source": [
    "## 12. Instrucciones de Ejecuci√≥n\n",
    "\n",
    "### üìã Orden de Ejecuci√≥n de Celdas\n",
    "\n",
    "Este notebook est√° dise√±ado para ejecutarse secuencialmente. Las celdas marcadas con **\"‚úÖ EJECUTAR PARA RQ8\"** son las m√°s importantes:\n",
    "\n",
    "1. **Celda 1**: Configuraci√≥n e Imports\n",
    "2. **Celda 2**: Cargar Modelo GroundingDINO ‚Üê **EJECUTAR PARA RQ8**\n",
    "3. **Celda 3**: Funciones Auxiliares\n",
    "4. **Celda 4**: Inferencia y Recolecci√≥n de Predicciones con IoU ‚Üê **EJECUTAR PARA RQ8** (tiempo: ~30-45 min para 500 im√°genes)\n",
    "5. **Celda 5**: Calibraci√≥n Conjunta Sem√°ntico-Geom√©trica\n",
    "6. **Celda 6**: Tabla RQ8.1 ‚Äî Score‚ÄìIoU Alignment\n",
    "7. **Celda 7**: Figura RQ8.1 ‚Äî Score-IoU Reliability Diagram\n",
    "8. **Celda 8**: Tabla RQ8.2 ‚Äî Ranking and Selection Utility\n",
    "9. **Celda 9**: Figura RQ8.2 ‚Äî Precision@K Curves\n",
    "10. **Celda 10**: Resumen y Verificaci√≥n de Resultados\n",
    "11. **Celda 11**: Interpretaci√≥n de Resultados (solo lectura)\n",
    "\n",
    "### ‚ö†Ô∏è Notas Importantes\n",
    "\n",
    "- **Tiempo total estimado**: ~45-60 minutos para 500 im√°genes\n",
    "- **GPU requerida**: El modelo GroundingDINO requiere GPU para inferencia eficiente\n",
    "- **Dependencias**: Todas las librer√≠as necesarias est√°n importadas en la Celda 1\n",
    "- **Paths relativos**: Todo usa paths relativos desde `New_RQ/new_rq8/`\n",
    "- **Reproducibilidad**: Seeds fijadas para resultados reproducibles\n",
    "\n",
    "### üìä Archivos Generados\n",
    "\n",
    "Al completar la ejecuci√≥n, encontrar√°s en `./output/`:\n",
    "- ‚úÖ 2 figuras (PNG + PDF cada una)\n",
    "- ‚úÖ 2 tablas (CSV + JSON cada una)\n",
    "- ‚úÖ Datos intermedios (parquet) para an√°lisis posterior\n",
    "- ‚úÖ Par√°metros de calibraci√≥n (JSON)\n",
    "\n",
    "### üîÑ Re-ejecuci√≥n\n",
    "\n",
    "Si necesitas re-ejecutar:\n",
    "- Las celdas 1-3 son r√°pidas y seguras de re-ejecutar\n",
    "- La celda 4 (inferencia) es la m√°s costosa (~45 min). Si ya tienes `detections_raw.parquet`, puedes saltarla\n",
    "- Las celdas 5-10 son r√°pidas (<5 min total) y pueden re-ejecutarse sin problema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
