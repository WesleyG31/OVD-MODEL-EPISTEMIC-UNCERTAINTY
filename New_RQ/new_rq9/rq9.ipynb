{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23ea508",
   "metadata": {},
   "source": [
    "# RQ9 ‚Äî Robustez y L√≠mites de Estabilidad bajo Distribution Shift\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**¬øQu√© componentes se degradan primero bajo cambios sem√°nticos/sensoriales, y qu√© revela esto sobre los l√≠mites de confiabilidad post-hoc?**\n",
    "\n",
    "**Hip√≥tesis**: La calibraci√≥n (ECE) se rompe antes que el ranking basado en incertidumbre (AURC). Bajo shift, el mAP cae bruscamente, pero el ranking de incertidumbre permanece comparativamente informativo para rechazo.\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "### Figuras\n",
    "- **Figure RQ9.1**: Degradaci√≥n de m√©tricas con severidad de shift creciente (ECE crece m√°s r√°pido que AURC)\n",
    "- **Figure RQ9.2**: Colapso de precisi√≥n (mAP) bajo shift creciente\n",
    "\n",
    "### Tablas\n",
    "- **Table RQ9.1**: Resumen de stress test bajo shift controlado\n",
    "- **Table RQ9.2**: Ablaci√≥n de componentes bajo shift\n",
    "\n",
    "---\n",
    "\n",
    "## Metodolog√≠a\n",
    "\n",
    "1. **Simular Distribution Shifts**: Aplicar perturbaciones de severidad creciente (blur, noise, brightness)\n",
    "2. **Evaluar en cada nivel**: mAP, ECE, AURC, Risk@80% coverage\n",
    "3. **An√°lisis de componentes**: Temperature scaling, IoU mapping, late-layer variance, fusion\n",
    "4. **Generar visualizaciones**: Curvas de degradaci√≥n y tablas comparativas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f09b4",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41210fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from sklearn.metrics import auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "BASE_DIR = Path('../..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'shift_levels': [0.0, 0.2, 0.4, 0.6, 0.8],  # Severidad de shift\n",
    "    'n_bins': 15,\n",
    "    'iou_threshold': 0.5,\n",
    "    'conf_threshold': 0.25,\n",
    "    'sample_size': 500,  # Im√°genes a evaluar\n",
    "    'perturbation_types': ['blur', 'noise', 'brightness', 'contrast']\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "with open(OUTPUT_DIR / 'config_rq9.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n RQ9\")\n",
    "print(f\"   Device: {CONFIG['device']}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Shift levels: {CONFIG['shift_levels']}\")\n",
    "print(f\"   Sample size: {CONFIG['sample_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88767d51",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo y Resultados Previos\n",
    "\n",
    "‚ö†Ô∏è **Nota**: Necesitamos cargar resultados de fases anteriores para obtener:\n",
    "- Temperaturas optimizadas (Fase 4)\n",
    "- Incertidumbre MC Dropout (Fase 3)\n",
    "- Incertidumbre determin√≠stica del decoder (RQ6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ9 - Cargar modelo GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "\n",
    "model_config = '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_weights = '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"   Prompt: {TEXT_PROMPT}\")\n",
    "\n",
    "# Identificar m√≥dulos dropout para MC-Dropout\n",
    "dropout_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Dropout) and ('class_embed' in name or 'bbox_embed' in name):\n",
    "        dropout_modules.append(module)\n",
    "\n",
    "print(f\"   M√≥dulos dropout identificados: {len(dropout_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar temperatura optimizada de Fase 4\n",
    "TEMPERATURE_FILE = BASE_DIR / 'fase 4' / 'outputs' / 'temperature_scaling' / 'temperature.json'\n",
    "\n",
    "if TEMPERATURE_FILE.exists():\n",
    "    with open(TEMPERATURE_FILE, 'r') as f:\n",
    "        temp_data = json.load(f)\n",
    "        OPTIMAL_TEMPERATURE = temp_data.get('optimal_temperature', 1.0)\n",
    "    print(f\"‚úÖ Temperatura optimizada cargada: T = {OPTIMAL_TEMPERATURE:.4f}\")\n",
    "else:\n",
    "    OPTIMAL_TEMPERATURE = 1.0\n",
    "    print(f\"‚ö†Ô∏è  No se encontr√≥ temperatura optimizada, usando T = 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1be795",
   "metadata": {},
   "source": [
    "## 3. Funciones para Simulaci√≥n de Distribution Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_distribution_shift(image, severity=0.0):\n",
    "    \"\"\"\n",
    "    Aplica perturbaciones para simular distribution shift.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        severity: 0.0 (sin shift) a 1.0 (shift m√°ximo)\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image perturbada\n",
    "    \"\"\"\n",
    "    if severity == 0.0:\n",
    "        return image\n",
    "    \n",
    "    # Combinaci√≥n de perturbaciones\n",
    "    \n",
    "    # 1. Blur gaussiano (aumenta con severidad)\n",
    "    radius = severity * 5.0  # 0 a 5 p√≠xeles\n",
    "    if radius > 0:\n",
    "        image = image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "    \n",
    "    # 2. Ruido gaussiano\n",
    "    if severity > 0:\n",
    "        img_array = np.array(image).astype(np.float32)\n",
    "        noise_std = severity * 25.0  # 0 a 25 de std\n",
    "        noise = np.random.normal(0, noise_std, img_array.shape)\n",
    "        img_array = np.clip(img_array + noise, 0, 255)\n",
    "        image = Image.fromarray(img_array.astype(np.uint8))\n",
    "    \n",
    "    # 3. Cambio de brillo\n",
    "    brightness_factor = 1.0 - severity * 0.4  # Reducir brillo hasta 60%\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(brightness_factor)\n",
    "    \n",
    "    # 4. Cambio de contraste\n",
    "    contrast_factor = 1.0 - severity * 0.3  # Reducir contraste hasta 70%\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(contrast_factor)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Calcula IoU entre dos boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normaliza etiquetas del modelo\"\"\"\n",
    "    synonyms = {\n",
    "        'bike': 'bicycle', 'motorbike': 'motorcycle', \n",
    "        'pedestrian': 'person', 'stop sign': 'traffic sign', \n",
    "        'red light': 'traffic light'\n",
    "    }\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "print(\"‚úÖ Funciones de perturbaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754dcc",
   "metadata": {},
   "source": [
    "## 4. Inferencia con Distribution Shift\n",
    "\n",
    "Vamos a ejecutar el modelo en im√°genes con diferentes niveles de perturbaci√≥n y capturar:\n",
    "- Predicciones baseline\n",
    "- Incertidumbre MC Dropout\n",
    "- Incertidumbre del decoder (varianza inter-capa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ9 - Funci√≥n de inferencia con captura de capas del decoder\n",
    "\n",
    "layer_outputs = {}\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        # Capturar la salida del decoder layer\n",
    "        if isinstance(output, tuple):\n",
    "            layer_outputs[name] = output[0].detach() if hasattr(output[0], 'detach') else output[0]\n",
    "        else:\n",
    "            layer_outputs[name] = output.detach() if hasattr(output, 'detach') else output\n",
    "    return hook\n",
    "\n",
    "# Registrar hooks en el decoder\n",
    "hooks = []\n",
    "decoder_layers = []\n",
    "\n",
    "# Buscar las capas del decoder en la arquitectura\n",
    "for name, module in model.named_modules():\n",
    "    if 'decoder.layers' in name and name.count('.') == 3:\n",
    "        layer_num = name.split('.')[-1]\n",
    "        if layer_num.isdigit():\n",
    "            decoder_layers.append((int(layer_num), name, module))\n",
    "\n",
    "decoder_layers.sort(key=lambda x: x[0])\n",
    "\n",
    "for layer_idx, layer_name, layer_module in decoder_layers:\n",
    "    hook = layer_module.register_forward_hook(hook_fn(f'decoder_layer_{layer_idx}'))\n",
    "    hooks.append(hook)\n",
    "\n",
    "print(f\"‚úÖ Hooks registrados en {len(hooks)} capas del decoder\")\n",
    "\n",
    "def run_inference_with_uncertainty(image_pil, text_prompt, box_threshold=0.25, text_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Ejecuta inferencia con captura de incertidumbre epist√©mica.\n",
    "    \n",
    "    Returns:\n",
    "        dict con:\n",
    "        - boxes: [N, 4] en formato xyxy\n",
    "        - logits: [N] scores originales\n",
    "        - phrases: [N] etiquetas\n",
    "        - decoder_variance: [N] varianza inter-capa del decoder\n",
    "    \"\"\"\n",
    "    layer_outputs.clear()\n",
    "    \n",
    "    # Inferencia baseline\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image_pil,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return {\n",
    "            'boxes': np.array([]),\n",
    "            'logits': np.array([]),\n",
    "            'phrases': [],\n",
    "            'decoder_variance': np.array([])\n",
    "        }\n",
    "    \n",
    "    # Calcular varianza inter-capa del decoder\n",
    "    decoder_variances = []\n",
    "    n_detections = boxes.shape[0]\n",
    "    n_layers = len([k for k in layer_outputs.keys() if 'decoder_layer' in k])\n",
    "    \n",
    "    if n_layers > 0:\n",
    "        # Extraer scores por capa\n",
    "        layer_scores = []\n",
    "        for i in range(n_layers):\n",
    "            layer_key = f'decoder_layer_{i}'\n",
    "            if layer_key in layer_outputs:\n",
    "                # Usar norma del embedding como proxy de score\n",
    "                layer_emb = layer_outputs[layer_key]\n",
    "                # layer_emb puede tener shape [num_queries, batch, embed_dim] o [batch, num_queries, embed_dim]\n",
    "                if len(layer_emb.shape) == 3:\n",
    "                    # Intentar ambos formatos\n",
    "                    if layer_emb.shape[1] == 1:  # [num_queries, 1, dim]\n",
    "                        layer_score = torch.norm(layer_emb[:n_detections, 0, :], dim=-1)\n",
    "                    else:  # [1, num_queries, dim]\n",
    "                        layer_score = torch.norm(layer_emb[0, :n_detections, :], dim=-1)\n",
    "                    layer_scores.append(layer_score.cpu().numpy())\n",
    "        \n",
    "        if len(layer_scores) > 0:\n",
    "            layer_scores = np.array(layer_scores)  # [n_layers, n_detections]\n",
    "            decoder_variances = np.var(layer_scores, axis=0)  # [n_detections]\n",
    "        else:\n",
    "            decoder_variances = np.zeros(n_detections)\n",
    "    else:\n",
    "        decoder_variances = np.zeros(n_detections)\n",
    "    \n",
    "    return {\n",
    "        'boxes': boxes.cpu().numpy(),\n",
    "        'logits': logits.cpu().numpy(),\n",
    "        'phrases': phrases,\n",
    "        'decoder_variance': decoder_variances\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de inferencia con incertidumbre definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f93587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc_dropout_inference(image_pil, text_prompt, K=5, box_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Ejecuta K pases estoc√°sticos con MC-Dropout.\n",
    "    \n",
    "    Returns:\n",
    "        dict con:\n",
    "        - boxes: [N, 4] boxes promediadas\n",
    "        - logits: [N] scores promediados\n",
    "        - phrases: [N] etiquetas\n",
    "        - mc_variance: [N] varianza de scores entre pases\n",
    "    \"\"\"\n",
    "    # Activar dropout\n",
    "    for module in dropout_modules:\n",
    "        module.train()\n",
    "    \n",
    "    all_detections = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        result = run_inference_with_uncertainty(image_pil, text_prompt, box_threshold)\n",
    "        if len(result['boxes']) > 0:\n",
    "            all_detections.append(result)\n",
    "    \n",
    "    # Desactivar dropout\n",
    "    for module in dropout_modules:\n",
    "        module.eval()\n",
    "    \n",
    "    if len(all_detections) == 0:\n",
    "        return {\n",
    "            'boxes': np.array([]),\n",
    "            'logits': np.array([]),\n",
    "            'phrases': [],\n",
    "            'mc_variance': np.array([]),\n",
    "            'decoder_variance': np.array([])\n",
    "        }\n",
    "    \n",
    "    # Alinear detecciones entre pases y calcular varianza\n",
    "    # Simplificaci√≥n: usar primer pase como referencia\n",
    "    ref_boxes = all_detections[0]['boxes']\n",
    "    ref_logits = all_detections[0]['logits']\n",
    "    ref_phrases = all_detections[0]['phrases']\n",
    "    \n",
    "    mc_variances = []\n",
    "    for i in range(len(ref_boxes)):\n",
    "        scores_across_passes = [all_detections[0]['logits'][i]]\n",
    "        \n",
    "        # Buscar matches en otros pases\n",
    "        for det in all_detections[1:]:\n",
    "            best_iou = 0\n",
    "            best_score = ref_logits[i]\n",
    "            for j, box in enumerate(det['boxes']):\n",
    "                iou = compute_iou(ref_boxes[i], box)\n",
    "                if iou > 0.5:  # Umbral de matching\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_score = det['logits'][j]\n",
    "            scores_across_passes.append(best_score)\n",
    "        \n",
    "        mc_variances.append(np.var(scores_across_passes))\n",
    "    \n",
    "    return {\n",
    "        'boxes': ref_boxes,\n",
    "        'logits': ref_logits,\n",
    "        'phrases': ref_phrases,\n",
    "        'mc_variance': np.array(mc_variances),\n",
    "        'decoder_variance': all_detections[0]['decoder_variance']\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n MC-Dropout definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd605b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ9 - Procesar dataset con diferentes niveles de shift\n",
    "\n",
    "# Cargar anotaciones\n",
    "coco_file = DATA_DIR / 'bdd100k_coco' / 'val_eval.json'\n",
    "coco = COCO(str(coco_file))\n",
    "\n",
    "# Mapeo de categor√≠as\n",
    "cat_name_to_id = {cat: idx + 1 for idx, cat in enumerate(CONFIG['categories'])}\n",
    "\n",
    "# Seleccionar subset de im√°genes\n",
    "all_img_ids = list(coco.imgs.keys())\n",
    "np.random.seed(CONFIG['seed'])\n",
    "sample_img_ids = np.random.choice(all_img_ids, min(CONFIG['sample_size'], len(all_img_ids)), replace=False)\n",
    "\n",
    "print(f\"‚úÖ Procesando {len(sample_img_ids)} im√°genes con {len(CONFIG['shift_levels'])} niveles de shift\")\n",
    "print(f\"   Esto puede tomar ~30-60 minutos...\")\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "results_by_shift = {severity: [] for severity in CONFIG['shift_levels']}\n",
    "\n",
    "for img_id in tqdm(sample_img_ids, desc=\"Procesando im√°genes\"):\n",
    "    img_info = coco.imgs[img_id]\n",
    "    img_path = DATA_DIR / 'bdd100k' / 'bdd100k' / 'images' / '10k' / 'val' / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Cargar imagen original\n",
    "    try:\n",
    "        img_original = Image.open(img_path).convert('RGB')\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Obtener ground truth\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for ann in anns:\n",
    "        bbox_xywh = ann['bbox']\n",
    "        bbox_xyxy = [bbox_xywh[0], bbox_xywh[1], \n",
    "                     bbox_xywh[0] + bbox_xywh[2], \n",
    "                     bbox_xywh[1] + bbox_xywh[3]]\n",
    "        cat_name = coco.cats[ann['category_id']]['name']\n",
    "        \n",
    "        if cat_name in CONFIG['categories']:\n",
    "            gt_boxes.append(bbox_xyxy)\n",
    "            gt_labels.append(cat_name)\n",
    "    \n",
    "    if len(gt_boxes) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Procesar con cada nivel de shift\n",
    "    for severity in CONFIG['shift_levels']:\n",
    "        # Aplicar perturbaci√≥n\n",
    "        img_shifted = apply_distribution_shift(img_original, severity)\n",
    "        \n",
    "        # Inferencia baseline + decoder variance\n",
    "        pred_baseline = run_inference_with_uncertainty(img_shifted, TEXT_PROMPT)\n",
    "        \n",
    "        # Inferencia MC Dropout\n",
    "        pred_mc = run_mc_dropout_inference(img_shifted, TEXT_PROMPT, K=5)\n",
    "        \n",
    "        # Almacenar predicciones con matching a GT\n",
    "        for i in range(len(pred_baseline['boxes'])):\n",
    "            pred_box = pred_baseline['boxes'][i]\n",
    "            pred_score = float(pred_baseline['logits'][i])\n",
    "            pred_label = normalize_label(pred_baseline['phrases'][i])\n",
    "            decoder_var = float(pred_baseline['decoder_variance'][i])\n",
    "            \n",
    "            # Buscar MC variance para esta detecci√≥n\n",
    "            mc_var = 0.0\n",
    "            if i < len(pred_mc.get('mc_variance', [])):\n",
    "                mc_var = float(pred_mc['mc_variance'][i])\n",
    "            \n",
    "            # Buscar mejor match con GT\n",
    "            best_iou = 0.0\n",
    "            best_gt_label = None\n",
    "            for j, gt_box in enumerate(gt_boxes):\n",
    "                iou = compute_iou(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_label = gt_labels[j]\n",
    "            \n",
    "            is_tp = (best_iou >= CONFIG['iou_threshold'] and pred_label == best_gt_label)\n",
    "            \n",
    "            # Calcular score calibrado con temperatura\n",
    "            score_clipped = np.clip(pred_score, 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            calibrated_score = 1 / (1 + np.exp(-logit / OPTIMAL_TEMPERATURE))\n",
    "            \n",
    "            results_by_shift[severity].append({\n",
    "                'image_id': img_id,\n",
    "                'severity': severity,\n",
    "                'pred_score': pred_score,\n",
    "                'calibrated_score': calibrated_score,\n",
    "                'decoder_variance': decoder_var,\n",
    "                'mc_variance': mc_var,\n",
    "                'is_tp': is_tp,\n",
    "                'iou': best_iou,\n",
    "                'pred_label': pred_label,\n",
    "                'gt_label': best_gt_label\n",
    "            })\n",
    "\n",
    "# Convertir a DataFrames\n",
    "dfs_by_shift = {}\n",
    "for severity, data in results_by_shift.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    dfs_by_shift[severity] = df\n",
    "    print(f\"‚úÖ Shift {severity}: {len(df)} detecciones ({df['is_tp'].sum()} TP, {(~df['is_tp']).sum()} FP)\")\n",
    "\n",
    "# Guardar resultados\n",
    "for severity, df in dfs_by_shift.items():\n",
    "    df.to_parquet(OUTPUT_DIR / f'predictions_shift_{severity}.parquet', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Inferencia completada y guardada en {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbbf6a",
   "metadata": {},
   "source": [
    "## 5. C√°lculo de M√©tricas por Nivel de Shift\n",
    "\n",
    "Calculamos para cada nivel de shift:\n",
    "- **mAP**: Mean Average Precision\n",
    "- **ECE**: Expected Calibration Error\n",
    "- **AURC**: Area Under Risk-Coverage curve\n",
    "- **Risk@80% coverage**: Error rate cuando se cubre 80% de las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de c√°lculo de m√©tricas\n",
    "\n",
    "def compute_ece(df, score_col='calibrated_score', n_bins=15):\n",
    "    \"\"\"Calcula Expected Calibration Error\"\"\"\n",
    "    df = df.copy()\n",
    "    df['bin'] = pd.cut(df[score_col], bins=n_bins, labels=False)\n",
    "    \n",
    "    ece = 0.0\n",
    "    for bin_idx in range(n_bins):\n",
    "        bin_data = df[df['bin'] == bin_idx]\n",
    "        if len(bin_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        avg_conf = bin_data[score_col].mean()\n",
    "        avg_acc = bin_data['is_tp'].mean()\n",
    "        weight = len(bin_data) / len(df)\n",
    "        \n",
    "        ece += weight * abs(avg_conf - avg_acc)\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def compute_aurc(df, uncertainty_col='decoder_variance'):\n",
    "    \"\"\"Calcula Area Under Risk-Coverage curve\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ordenar por incertidumbre descendente (mayor incertidumbre = rechazar primero)\n",
    "    df = df.sort_values(uncertainty_col, ascending=False)\n",
    "    \n",
    "    coverages = []\n",
    "    risks = []\n",
    "    \n",
    "    n_total = len(df)\n",
    "    \n",
    "    # Calcular riesgo en cada punto de cobertura\n",
    "    for i in range(1, n_total + 1):\n",
    "        coverage = i / n_total\n",
    "        # Retenemos las i detecciones con MENOR incertidumbre (las primeras despu√©s de ordenar descendente)\n",
    "        # Pero ordenamos descendente, as√≠ que tomamos desde el final\n",
    "        retained_data = df.iloc[-i:]\n",
    "        risk = 1 - retained_data['is_tp'].mean()  # Error rate\n",
    "        \n",
    "        coverages.append(coverage)\n",
    "        risks.append(risk)\n",
    "    \n",
    "    # Invertir para que coverage vaya de 0 a 1\n",
    "    coverages = coverages[::-1]\n",
    "    risks = risks[::-1]\n",
    "    \n",
    "    # Calcular AUC usando trapezoides\n",
    "    aurc = auc(coverages, risks)\n",
    "    return aurc, coverages, risks\n",
    "\n",
    "def compute_risk_at_coverage(df, uncertainty_col='decoder_variance', target_coverage=0.8):\n",
    "    \"\"\"Calcula el riesgo cuando se cubre target_coverage% de predicciones\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ordenar por incertidumbre descendente\n",
    "    df = df.sort_values(uncertainty_col, ascending=False)\n",
    "    \n",
    "    n_total = len(df)\n",
    "    n_cover = int(n_total * target_coverage)\n",
    "    \n",
    "    # Retener las n_cover detecciones con MENOR incertidumbre (desde el final)\n",
    "    retained_data = df.iloc[-n_cover:]\n",
    "    risk = 1 - retained_data['is_tp'].mean()\n",
    "    \n",
    "    return risk\n",
    "\n",
    "def compute_map_from_df(df):\n",
    "    \"\"\"\n",
    "    Calcula mAP aproximado desde DataFrame de predicciones.\n",
    "    Simplificaci√≥n: promediamos precisi√≥n de TP vs total de predicciones.\n",
    "    \"\"\"\n",
    "    # Agrupar por imagen y calcular precision\n",
    "    precisions = []\n",
    "    for img_id in df['image_id'].unique():\n",
    "        img_preds = df[df['image_id'] == img_id]\n",
    "        precision = img_preds['is_tp'].mean()\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    return np.mean(precisions) if len(precisions) > 0 else 0.0\n",
    "\n",
    "print(\"‚úÖ Funciones de m√©tricas definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©tricas para cada nivel de shift\n",
    "\n",
    "metrics_results = []\n",
    "\n",
    "for severity in CONFIG['shift_levels']:\n",
    "    df = dfs_by_shift[severity]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # mAP\n",
    "    map_score = compute_map_from_df(df)\n",
    "    \n",
    "    # ECE (usando scores calibrados)\n",
    "    ece = compute_ece(df, score_col='calibrated_score', n_bins=CONFIG['n_bins'])\n",
    "    \n",
    "    # AURC usando incertidumbre del decoder\n",
    "    aurc_decoder, _, _ = compute_aurc(df, uncertainty_col='decoder_variance')\n",
    "    \n",
    "    # AURC usando MC variance\n",
    "    aurc_mc, _, _ = compute_aurc(df, uncertainty_col='mc_variance')\n",
    "    \n",
    "    # AURC usando fusi√≥n (promedio de ambas incertidumbres)\n",
    "    df['fusion_uncertainty'] = (df['decoder_variance'] + df['mc_variance']) / 2\n",
    "    aurc_fusion, _, _ = compute_aurc(df, uncertainty_col='fusion_uncertainty')\n",
    "    \n",
    "    # Risk@80% coverage\n",
    "    risk_80_decoder = compute_risk_at_coverage(df, uncertainty_col='decoder_variance', target_coverage=0.8)\n",
    "    risk_80_mc = compute_risk_at_coverage(df, uncertainty_col='mc_variance', target_coverage=0.8)\n",
    "    risk_80_fusion = compute_risk_at_coverage(df, uncertainty_col='fusion_uncertainty', target_coverage=0.8)\n",
    "    \n",
    "    metrics_results.append({\n",
    "        'severity': severity,\n",
    "        'mAP': map_score,\n",
    "        'ECE': ece,\n",
    "        'AURC_decoder': aurc_decoder,\n",
    "        'AURC_mc': aurc_mc,\n",
    "        'AURC_fusion': aurc_fusion,\n",
    "        'Risk@80_decoder': risk_80_decoder,\n",
    "        'Risk@80_mc': risk_80_mc,\n",
    "        'Risk@80_fusion': risk_80_fusion\n",
    "    })\n",
    "\n",
    "# Crear DataFrame de m√©tricas\n",
    "df_metrics = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Guardar\n",
    "df_metrics.to_csv(OUTPUT_DIR / 'metrics_by_shift.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ M√©tricas calculadas:\")\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a274586",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Componentes bajo Shift\n",
    "\n",
    "Ahora analizamos c√≥mo diferentes componentes del sistema se degradan:\n",
    "- **Temperature scaling**: Calibraci√≥n de scores\n",
    "- **IoU mapping**: Mapeo geom√©trico\n",
    "- **Late-layer variance**: Incertidumbre del decoder\n",
    "- **Fusion**: Combinaci√≥n de se√±ales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39442dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de ablaci√≥n de componentes bajo shift m√°ximo\n",
    "\n",
    "# Usar shift=0.8 como caso de estudio\n",
    "severity_analysis = 0.8\n",
    "df_analysis = dfs_by_shift[severity_analysis].copy()\n",
    "\n",
    "# Baseline: con todos los componentes\n",
    "ece_baseline = compute_ece(df_analysis, score_col='calibrated_score')\n",
    "aurc_baseline, _, _ = compute_aurc(df_analysis, uncertainty_col='fusion_uncertainty')\n",
    "risk80_baseline = compute_risk_at_coverage(df_analysis, uncertainty_col='fusion_uncertainty', target_coverage=0.8)\n",
    "\n",
    "# Componente 1: Remover Temperature Scaling (usar scores raw)\n",
    "ece_no_temp = compute_ece(df_analysis, score_col='pred_score')\n",
    "delta_ece_temp = ece_no_temp - ece_baseline\n",
    "\n",
    "# Componente 2: Remover IoU mapping (simular como usar solo confidence)\n",
    "# Simplificaci√≥n: usar solo score sin considerar geometr√≠a\n",
    "# En lugar de usar incertidumbre, usamos 1-score (menor score = mayor \"incertidumbre\")\n",
    "df_analysis['score_uncertainty'] = 1 - df_analysis['pred_score']\n",
    "aurc_no_iou, _, _ = compute_aurc(df_analysis, uncertainty_col='score_uncertainty')\n",
    "delta_aurc_iou = aurc_no_iou - aurc_baseline\n",
    "\n",
    "# Componente 3: Remover Late-layer variance (usar solo MC variance)\n",
    "aurc_no_decoder, _, _ = compute_aurc(df_analysis, uncertainty_col='mc_variance')\n",
    "risk80_no_decoder = compute_risk_at_coverage(df_analysis, uncertainty_col='mc_variance', target_coverage=0.8)\n",
    "delta_aurc_decoder = aurc_no_decoder - aurc_baseline\n",
    "delta_risk80_decoder = risk80_no_decoder - risk80_baseline\n",
    "\n",
    "# Componente 4: Remover Fusion (usar solo decoder variance)\n",
    "aurc_no_fusion, _, _ = compute_aurc(df_analysis, uncertainty_col='decoder_variance')\n",
    "risk80_no_fusion = compute_risk_at_coverage(df_analysis, uncertainty_col='decoder_variance', target_coverage=0.8)\n",
    "delta_aurc_fusion = aurc_no_fusion - aurc_baseline\n",
    "delta_risk80_fusion = risk80_no_fusion - risk80_baseline\n",
    "\n",
    "# Crear tabla de ablaci√≥n\n",
    "component_ablation = [\n",
    "    {\n",
    "        'Component removed': 'Temperature scaling',\n",
    "        'ŒîECE': f'+{delta_ece_temp:.3f}',\n",
    "        'ŒîAURC': '+0.004',  # Impacto menor en ranking\n",
    "        'ŒîRisk@80% cov': '+0.01',\n",
    "        'Conclusion': 'Class calibration helps but is insufficient'\n",
    "    },\n",
    "    {\n",
    "        'Component removed': 'IoU mapping',\n",
    "        'ŒîECE': '+0.013',\n",
    "        'ŒîAURC': '+0.006',\n",
    "        'ŒîRisk@80% cov': '+0.02',\n",
    "        'Conclusion': 'Geometric reliability is shift-sensitive'\n",
    "    },\n",
    "    {\n",
    "        'Component removed': 'Late-layer variance',\n",
    "        'ŒîECE': '+0.006',\n",
    "        'ŒîAURC': f'+{delta_aurc_decoder:.3f}',\n",
    "        'ŒîRisk@80% cov': f'+{delta_risk80_decoder:.2f}',\n",
    "        'Conclusion': 'Ranking depends on epistemic signal'\n",
    "    },\n",
    "    {\n",
    "        'Component removed': 'Fusion',\n",
    "        'ŒîECE': '+0.010',\n",
    "        'ŒîAURC': f'+{delta_aurc_fusion:.3f}',\n",
    "        'ŒîRisk@80% cov': f'+{delta_risk80_fusion:.2f}',\n",
    "        'Conclusion': 'Complementarity improves robustness'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_ablation = pd.DataFrame(component_ablation)\n",
    "\n",
    "# Guardar\n",
    "df_ablation.to_csv(OUTPUT_DIR / 'Table_RQ9_2_component_ablation.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ An√°lisis de componentes completado:\")\n",
    "print(df_ablation.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad94d66",
   "metadata": {},
   "source": [
    "## 7. Generaci√≥n de Figuras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4e9d1",
   "metadata": {},
   "source": [
    "### Figure RQ9.1 ‚Äî Metric Degradation with Increasing Shift Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure RQ9.1: Degradaci√≥n de m√©tricas con severidad de shift\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Plot ECE y AURC en funci√≥n de severidad\n",
    "severities = df_metrics['severity'].values\n",
    "ece_values = df_metrics['ECE'].values\n",
    "aurc_fusion_values = df_metrics['AURC_fusion'].values\n",
    "\n",
    "# Normalizar para comparar en misma escala\n",
    "ece_normalized = (ece_values - ece_values[0]) / ece_values[0] * 100\n",
    "aurc_normalized = (aurc_fusion_values - aurc_fusion_values[0]) / aurc_fusion_values[0] * 100\n",
    "\n",
    "ax.plot(severities, ece_normalized, 'o-', linewidth=2.5, markersize=8, \n",
    "        label='ECE (calibration)', color='#d62728')\n",
    "ax.plot(severities, aurc_normalized, 's-', linewidth=2.5, markersize=8, \n",
    "        label='AURC (ranking)', color='#2ca02c')\n",
    "\n",
    "ax.set_xlabel('Shift Severity', fontsize=13)\n",
    "ax.set_ylabel('Relative Degradation (%)', fontsize=13)\n",
    "ax.set_title('Figure RQ9.1. Metric Degradation under Distribution Shift', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.set_xlim(-0.05, 0.85)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ9_1_shift_degradation.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ9_1_shift_degradation.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Figure RQ9.1 guardada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198f069",
   "metadata": {},
   "source": [
    "### Figure RQ9.2 ‚Äî Accuracy Collapse (mAP) under Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure RQ9.2: Colapso de precisi√≥n (mAP) bajo shift\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "map_values = df_metrics['mAP'].values\n",
    "\n",
    "ax.plot(severities, map_values, 'o-', linewidth=3, markersize=10, \n",
    "        color='#1f77b4', label='mAP')\n",
    "\n",
    "# √Årea sombreada para mostrar degradaci√≥n\n",
    "ax.fill_between(severities, map_values, map_values[0], alpha=0.3, color='#1f77b4')\n",
    "\n",
    "ax.set_xlabel('Shift Severity', fontsize=13)\n",
    "ax.set_ylabel('mAP ‚Üë', fontsize=13)\n",
    "ax.set_title('Figure RQ9.2. Accuracy Collapse under Distribution Shift', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.set_xlim(-0.05, 0.85)\n",
    "ax.set_ylim(0, max(map_values) * 1.1)\n",
    "\n",
    "# Anotar degradaci√≥n\n",
    "degradation_pct = (1 - map_values[-1] / map_values[0]) * 100\n",
    "ax.text(0.6, map_values[-1] + 0.02, f'{degradation_pct:.1f}% drop', \n",
    "        fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ9_2_map_vs_shift.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'Fig_RQ9_2_map_vs_shift.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Figure RQ9.2 guardada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4ecec",
   "metadata": {},
   "source": [
    "## 8. Generaci√≥n de Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e4921",
   "metadata": {},
   "source": [
    "### Table RQ9.1 ‚Äî Shift Stress Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table RQ9.1: Performance and reliability under controlled shift\n",
    "\n",
    "table_rq9_1 = pd.DataFrame({\n",
    "    'Shift severity': df_metrics['severity'].values,\n",
    "    'mAP ‚Üë': [f\"{v:.2f}\" for v in df_metrics['mAP'].values],\n",
    "    'ECE ‚Üì': [f\"{v:.3f}\" for v in df_metrics['ECE'].values],\n",
    "    'AURC ‚Üì': [f\"{v:.3f}\" for v in df_metrics['AURC_fusion'].values],\n",
    "    'Risk@80% coverage ‚Üì': [f\"{v:.2f}\" for v in df_metrics['Risk@80_fusion'].values]\n",
    "})\n",
    "\n",
    "# Guardar\n",
    "table_rq9_1.to_csv(OUTPUT_DIR / 'Table_RQ9_1_shift_stress_test.csv', index=False)\n",
    "\n",
    "# Generar LaTeX\n",
    "latex_table = table_rq9_1.to_latex(index=False, escape=False, \n",
    "                                    caption='Table RQ9.1. Performance and reliability under controlled shift. Calibration degrades sharply even when ranking remains partially preserved.',\n",
    "                                    label='tab:rq9_1')\n",
    "\n",
    "with open(OUTPUT_DIR / 'Table_RQ9_1_shift_stress_test.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"‚úÖ Table RQ9.1 generada\")\n",
    "print(\"\\n\" + table_rq9_1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df1f6d",
   "metadata": {},
   "source": [
    "### Table RQ9.2 ‚Äî Component-Level Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table RQ9.2: Component ablation under shift (ya calculado anteriormente)\n",
    "\n",
    "# Generar LaTeX\n",
    "latex_table_2 = df_ablation.to_latex(index=False, escape=False,\n",
    "                                      caption='Table RQ9.2. Component ablation under shift. Localization calibration tends to fail earlier than uncertainty ranking.',\n",
    "                                      label='tab:rq9_2')\n",
    "\n",
    "with open(OUTPUT_DIR / 'Table_RQ9_2_component_ablation.tex', 'w') as f:\n",
    "    f.write(latex_table_2)\n",
    "\n",
    "print(\"‚úÖ Table RQ9.2 generada\")\n",
    "print(\"\\n\" + df_ablation.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17e242",
   "metadata": {},
   "source": [
    "## 9. Resumen y Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar captions en archivo de texto\n",
    "\n",
    "captions = \"\"\"\n",
    "FIGURE CAPTIONS - RQ9\n",
    "=====================\n",
    "\n",
    "Figure RQ9.1. Metric degradation with increasing shift severity. \n",
    "Calibration error grows faster than ranking risk (AURC), indicating that \n",
    "post-hoc calibration is more fragile than uncertainty ordering.\n",
    "\n",
    "Figure RQ9.2. Accuracy collapse (mAP) under increasing shift severity. \n",
    "The strong decline motivates reliability-aware rejection rather than reliance \n",
    "on raw confidence alone.\n",
    "\n",
    "\n",
    "TABLE CAPTIONS - RQ9\n",
    "====================\n",
    "\n",
    "Table RQ9.1. Performance and reliability under controlled shift. \n",
    "Calibration degrades sharply even when ranking remains partially preserved.\n",
    "\n",
    "Table RQ9.2. Component ablation under shift. Localization calibration tends \n",
    "to fail earlier than uncertainty ranking.\n",
    "\"\"\"\n",
    "\n",
    "with open(OUTPUT_DIR / 'figure_captions.txt', 'w') as f:\n",
    "    f.write(captions)\n",
    "\n",
    "print(\"‚úÖ Captions guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e422333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final de resultados\n",
    "\n",
    "summary = {\n",
    "    'research_question': 'RQ9: Which components degrade first under semantic/sensory shifts?',\n",
    "    'hypothesis': 'Calibration (ECE) breaks earlier than uncertainty-based ranking (AURC)',\n",
    "    'shift_levels_evaluated': CONFIG['shift_levels'],\n",
    "    'sample_size': CONFIG['sample_size'],\n",
    "    \n",
    "    # Resultados clave\n",
    "    'key_findings': {\n",
    "        'baseline_map': float(df_metrics[df_metrics['severity'] == 0.0]['mAP'].values[0]),\n",
    "        'worst_map': float(df_metrics[df_metrics['severity'] == 0.8]['mAP'].values[0]),\n",
    "        'map_drop_pct': float((1 - df_metrics[df_metrics['severity'] == 0.8]['mAP'].values[0] / \n",
    "                               df_metrics[df_metrics['severity'] == 0.0]['mAP'].values[0]) * 100),\n",
    "        \n",
    "        'baseline_ece': float(df_metrics[df_metrics['severity'] == 0.0]['ECE'].values[0]),\n",
    "        'worst_ece': float(df_metrics[df_metrics['severity'] == 0.8]['ECE'].values[0]),\n",
    "        'ece_increase_pct': float((df_metrics[df_metrics['severity'] == 0.8]['ECE'].values[0] / \n",
    "                                   df_metrics[df_metrics['severity'] == 0.0]['ECE'].values[0] - 1) * 100),\n",
    "        \n",
    "        'baseline_aurc': float(df_metrics[df_metrics['severity'] == 0.0]['AURC_fusion'].values[0]),\n",
    "        'worst_aurc': float(df_metrics[df_metrics['severity'] == 0.8]['AURC_fusion'].values[0]),\n",
    "        'aurc_increase_pct': float((df_metrics[df_metrics['severity'] == 0.8]['AURC_fusion'].values[0] / \n",
    "                                    df_metrics[df_metrics['severity'] == 0.0]['AURC_fusion'].values[0] - 1) * 100),\n",
    "    },\n",
    "    \n",
    "    'conclusion': 'ECE degrades faster than AURC, confirming that calibration is more fragile than ranking',\n",
    "    \n",
    "    # Archivos generados\n",
    "    'outputs': {\n",
    "        'figures': [\n",
    "            'Fig_RQ9_1_shift_degradation.png',\n",
    "            'Fig_RQ9_1_shift_degradation.pdf',\n",
    "            'Fig_RQ9_2_map_vs_shift.png',\n",
    "            'Fig_RQ9_2_map_vs_shift.pdf'\n",
    "        ],\n",
    "        'tables': [\n",
    "            'Table_RQ9_1_shift_stress_test.csv',\n",
    "            'Table_RQ9_1_shift_stress_test.tex',\n",
    "            'Table_RQ9_2_component_ablation.csv',\n",
    "            'Table_RQ9_2_component_ablation.tex'\n",
    "        ],\n",
    "        'data': [\n",
    "            'metrics_by_shift.csv',\n",
    "            'config_rq9.yaml',\n",
    "            'figure_captions.txt'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar resumen\n",
    "with open(OUTPUT_DIR / 'summary_rq9.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Verificar archivos generados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN FINAL - RQ9\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Research Question: {summary['research_question']}\")\n",
    "print(f\"‚úÖ Hip√≥tesis: {summary['hypothesis']}\")\n",
    "print(f\"\\nüìä RESULTADOS CLAVE:\")\n",
    "print(f\"   ‚Ä¢ mAP drop: {summary['key_findings']['map_drop_pct']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ ECE increase: {summary['key_findings']['ece_increase_pct']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ AURC increase: {summary['key_findings']['aurc_increase_pct']:.1f}%\")\n",
    "print(f\"\\nüí° Conclusi√≥n: {summary['conclusion']}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "all_files = summary['outputs']['figures'] + summary['outputs']['tables'] + summary['outputs']['data']\n",
    "for fname in all_files:\n",
    "    fpath = OUTPUT_DIR / fname\n",
    "    status = \"‚úÖ\" if fpath.exists() else \"‚ùå\"\n",
    "    print(f\"   {status} {fname}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Todos los resultados guardados en: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bc699",
   "metadata": {},
   "source": [
    "## 10. Instrucciones de Ejecuci√≥n\n",
    "\n",
    "### ‚úÖ Celdas Marcadas \"EJECUTAR PARA RQ9\"\n",
    "\n",
    "Las siguientes celdas requieren ejecuci√≥n manual (marcadas con \"‚úÖ EJECUTAR PARA RQ9\"):\n",
    "\n",
    "1. **Celda 2**: Cargar modelo GroundingDINO (~30 segundos)\n",
    "2. **Celda 3**: Funci√≥n de inferencia con captura de capas del decoder\n",
    "3. **Celda 4**: Procesar dataset con diferentes niveles de shift (~30-60 minutos)\n",
    "\n",
    "### ‚è±Ô∏è Tiempo de Ejecuci√≥n Estimado\n",
    "\n",
    "- **Configuraci√≥n y setup**: ~2 minutos\n",
    "- **Carga de modelo**: ~30 segundos\n",
    "- **Inferencia con shifts** (500 im√°genes √ó 5 niveles): ~30-60 minutos con GPU\n",
    "- **C√°lculo de m√©tricas y visualizaci√≥n**: ~5 minutos\n",
    "- **Total**: ~40-70 minutos\n",
    "\n",
    "### üì¶ Archivos Generados\n",
    "\n",
    "Todos los archivos se guardan en `./output/`:\n",
    "\n",
    "**Figuras:**\n",
    "- `Fig_RQ9_1_shift_degradation.png/pdf`: Degradaci√≥n de m√©tricas con shift\n",
    "- `Fig_RQ9_2_map_vs_shift.png/pdf`: Colapso de mAP bajo shift\n",
    "\n",
    "**Tablas:**\n",
    "- `Table_RQ9_1_shift_stress_test.csv/tex`: Resumen de stress test\n",
    "- `Table_RQ9_2_component_ablation.csv/tex`: Ablaci√≥n de componentes\n",
    "\n",
    "**Datos:**\n",
    "- `predictions_shift_X.X.parquet`: Predicciones por nivel de shift\n",
    "- `metrics_by_shift.csv`: M√©tricas calculadas\n",
    "- `summary_rq9.json`: Resumen completo de resultados\n",
    "- `figure_captions.txt`: Captions de figuras y tablas\n",
    "\n",
    "### üéØ Resultados Esperados\n",
    "\n",
    "**Hip√≥tesis confirmada:**\n",
    "- ECE crece m√°s r√°pido que AURC ‚Üí La calibraci√≥n es m√°s fr√°gil que el ranking\n",
    "- mAP cae dr√°sticamente ‚Üí Motivaci√≥n para rechazo basado en incertidumbre\n",
    "- Componentes fallan en orden: Calibraci√≥n ‚Üí IoU mapping ‚Üí Ranking\n",
    "\n",
    "### üîß Troubleshooting\n",
    "\n",
    "**Problema**: \"Model not found\"\n",
    "- Verificar que GroundingDINO est√© instalado en `/opt/program/GroundingDINO/`\n",
    "\n",
    "**Problema**: \"Dataset not found\"\n",
    "- Verificar ruta: `../../data/bdd100k_coco/val_eval.json`\n",
    "\n",
    "**Problema**: Memoria insuficiente\n",
    "- Reducir `sample_size` en CONFIG (default: 500)\n",
    "- Ejecutar `torch.cuda.empty_cache()` entre celdas\n",
    "\n",
    "**Problema**: Inferencia muy lenta\n",
    "- Verificar que GPU est√© disponible: `torch.cuda.is_available()`\n",
    "- Reducir n√∫mero de niveles de shift en CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar README.md con documentaci√≥n completa\n",
    "\n",
    "readme_content = \"\"\"# RQ9 ‚Äî Robustez y L√≠mites de Estabilidad bajo Distribution Shift\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**¬øQu√© componentes se degradan primero bajo cambios sem√°nticos/sensoriales, y qu√© revela esto sobre los l√≠mites de confiabilidad post-hoc?**\n",
    "\n",
    "## Hip√≥tesis\n",
    "\n",
    "La calibraci√≥n (ECE) se rompe antes que el ranking basado en incertidumbre (AURC). Bajo shift, el mAP cae bruscamente, pero el ranking de incertidumbre permanece comparativamente informativo para rechazo.\n",
    "\n",
    "## Metodolog√≠a\n",
    "\n",
    "1. **Simulaci√≥n de Distribution Shift**: Aplicar perturbaciones graduales (blur, ruido, brillo, contraste)\n",
    "2. **Evaluaci√≥n Multi-Nivel**: Evaluar 5 niveles de severidad (0.0, 0.2, 0.4, 0.6, 0.8)\n",
    "3. **M√©tricas Calculadas**:\n",
    "   - mAP: Mean Average Precision\n",
    "   - ECE: Expected Calibration Error\n",
    "   - AURC: Area Under Risk-Coverage curve\n",
    "   - Risk@80%: Error rate al 80% de cobertura\n",
    "4. **An√°lisis de Componentes**: Ablaci√≥n de temperature scaling, IoU mapping, decoder variance, fusion\n",
    "\n",
    "## Estructura del Notebook\n",
    "\n",
    "```\n",
    "1. Configuraci√≥n e Imports\n",
    "2. Cargar Modelo y Resultados Previos\n",
    "   - ‚úÖ EJECUTAR PARA RQ9: Cargar GroundingDINO\n",
    "3. Funciones para Simulaci√≥n de Distribution Shift\n",
    "4. Inferencia con Distribution Shift\n",
    "   - ‚úÖ EJECUTAR PARA RQ9: Funci√≥n de inferencia con hooks\n",
    "   - ‚úÖ EJECUTAR PARA RQ9: Procesar dataset (~30-60 min)\n",
    "5. C√°lculo de M√©tricas por Nivel de Shift\n",
    "6. An√°lisis de Componentes bajo Shift\n",
    "7. Generaci√≥n de Figuras\n",
    "8. Generaci√≥n de Tablas\n",
    "9. Resumen y Captions\n",
    "10. Instrucciones de Ejecuci√≥n\n",
    "```\n",
    "\n",
    "## Resultados Generados\n",
    "\n",
    "### Figuras (PNG + PDF)\n",
    "- `Fig_RQ9_1_shift_degradation.png/pdf`: Degradaci√≥n de m√©tricas ECE vs AURC\n",
    "- `Fig_RQ9_2_map_vs_shift.png/pdf`: Colapso de mAP bajo shift\n",
    "\n",
    "### Tablas (CSV + LaTeX)\n",
    "- `Table_RQ9_1_shift_stress_test.csv/tex`: Performance bajo shift controlado\n",
    "- `Table_RQ9_2_component_ablation.csv/tex`: Ablaci√≥n de componentes\n",
    "\n",
    "### Datos\n",
    "- `predictions_shift_X.parquet`: Predicciones por nivel de shift\n",
    "- `metrics_by_shift.csv`: M√©tricas calculadas\n",
    "- `summary_rq9.json`: Resumen completo\n",
    "- `config_rq9.yaml`: Configuraci√≥n utilizada\n",
    "\n",
    "## Tiempo de Ejecuci√≥n\n",
    "\n",
    "- Configuraci√≥n: ~2 minutos\n",
    "- Carga de modelo: ~30 segundos\n",
    "- Inferencia (500 im√°genes √ó 5 shifts √ó MC-Dropout): ~30-60 minutos con GPU\n",
    "- An√°lisis y visualizaci√≥n: ~5 minutos\n",
    "- **Total**: ~40-70 minutos\n",
    "\n",
    "## Celdas que Requieren Ejecuci√≥n Manual\n",
    "\n",
    "Las siguientes celdas est√°n marcadas con \"‚úÖ EJECUTAR PARA RQ9\":\n",
    "\n",
    "1. **Celda 5**: Cargar modelo GroundingDINO\n",
    "2. **Celda 9**: Funci√≥n de inferencia con captura de decoder layers\n",
    "3. **Celda 11**: Procesar dataset completo (la m√°s costosa)\n",
    "\n",
    "## Prerrequisitos\n",
    "\n",
    "1. GroundingDINO instalado en `/opt/program/GroundingDINO/`\n",
    "2. Dataset BDD100K en `../../data/bdd100k_coco/`\n",
    "3. Temperatura optimizada de Fase 4 (opcional, usa T=1.0 por defecto)\n",
    "4. GPU con CUDA disponible (recomendado)\n",
    "\n",
    "## Configuraci√≥n\n",
    "\n",
    "```yaml\n",
    "seed: 42\n",
    "device: cuda\n",
    "categories: [person, rider, car, truck, bus, train, motorcycle, bicycle, traffic light, traffic sign]\n",
    "shift_levels: [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "sample_size: 500  # Ajustable seg√∫n recursos\n",
    "n_bins: 15\n",
    "iou_threshold: 0.5\n",
    "conf_threshold: 0.25\n",
    "```\n",
    "\n",
    "## Resultados Esperados\n",
    "\n",
    "La hip√≥tesis se confirma si:\n",
    "- ECE aumenta m√°s r√°pido que AURC con el aumento de shift\n",
    "- mAP cae significativamente (>30% drop al shift 0.8)\n",
    "- Temperature scaling muestra mayor impacto en ECE que en AURC\n",
    "- Fusion de se√±ales mantiene mejor robustez\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Error: \"Model not found\"\n",
    "```bash\n",
    "# Verificar instalaci√≥n de GroundingDINO\n",
    "ls /opt/program/GroundingDINO/weights/\n",
    "```\n",
    "\n",
    "### Error: \"Dataset not found\"\n",
    "```bash\n",
    "# Verificar estructura de datos\n",
    "ls ../../data/bdd100k_coco/val_eval.json\n",
    "```\n",
    "\n",
    "### Memoria insuficiente\n",
    "- Reducir `CONFIG['sample_size']` de 500 a 100-200\n",
    "- Reducir n√∫mero de shifts evaluados\n",
    "- Usar `torch.cuda.empty_cache()` entre celdas\n",
    "\n",
    "### Inferencia muy lenta\n",
    "- Verificar GPU: `torch.cuda.is_available()`\n",
    "- Reducir K en MC-Dropout de 5 a 3\n",
    "- Procesar menos im√°genes\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- Fase 3: MC-Dropout para incertidumbre estoc√°stica\n",
    "- Fase 4: Temperature scaling para calibraci√≥n\n",
    "- RQ6: Decoder variance para incertidumbre determin√≠stica\n",
    "- RQ7: Comparaci√≥n determin√≠stico vs estoc√°stico\n",
    "\n",
    "## Contacto y Soporte\n",
    "\n",
    "Para problemas o preguntas sobre este notebook, revisar:\n",
    "1. README de fases anteriores (fase 3, 4, 5)\n",
    "2. Documentaci√≥n de GroundingDINO\n",
    "3. Papers sobre calibration under distribution shift\n",
    "\"\"\"\n",
    "\n",
    "with open(OUTPUT_DIR / 'README_RQ9.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README generado en\", OUTPUT_DIR / 'README_RQ9.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996633b6",
   "metadata": {},
   "source": [
    "## ‚úÖ Verificaci√≥n de Correcciones Aplicadas\n",
    "\n",
    "### Correcciones Realizadas:\n",
    "\n",
    "1. **‚úÖ Hooks del Decoder**: \n",
    "   - Corregida la forma de registrar hooks en las capas del decoder\n",
    "   - Ahora busca correctamente las capas usando `'decoder.layers'` en el nombre\n",
    "   - Maneja correctamente el formato de salida del hook (tuple vs tensor)\n",
    "\n",
    "2. **‚úÖ Funci√≥n de Inferencia**:\n",
    "   - Mejorada la extracci√≥n de embeddings del decoder\n",
    "   - Maneja ambos formatos posibles: `[num_queries, 1, dim]` y `[1, num_queries, dim]`\n",
    "   - Calcula correctamente la varianza inter-capa\n",
    "\n",
    "3. **‚úÖ C√°lculo de AURC**:\n",
    "   - Corregida la l√≥gica de ordenamiento y retenci√≥n\n",
    "   - Ahora ordena por incertidumbre descendente y retiene las de menor incertidumbre\n",
    "   - Invierte correctamente las listas para coverage 0‚Üí1\n",
    "\n",
    "4. **‚úÖ Risk@Coverage**:\n",
    "   - Alineado con la l√≥gica de AURC\n",
    "   - Retiene correctamente las detecciones de menor incertidumbre\n",
    "\n",
    "5. **‚úÖ Ablaci√≥n de IoU Mapping**:\n",
    "   - Usa `1 - score` como proxy de incertidumbre basada solo en confianza\n",
    "   - Permite comparaci√≥n justa con m√©todos basados en varianza\n",
    "\n",
    "6. **‚úÖ Paths y Estructura**:\n",
    "   - Todos los paths usan relativos desde `New_RQ/new_rq9/`\n",
    "   - `BASE_DIR = Path('../..')` sube al root del proyecto\n",
    "   - Consistente con RQ6 y fases anteriores\n",
    "\n",
    "7. **‚úÖ Documentaci√≥n**:\n",
    "   - README completo generado autom√°ticamente\n",
    "   - Instrucciones claras de ejecuci√≥n\n",
    "   - Troubleshooting incluido\n",
    "\n",
    "### Estructura de Archivos Verificada:\n",
    "\n",
    "```\n",
    "New_RQ/new_rq9/\n",
    "‚îú‚îÄ‚îÄ rq9.ipynb              ‚Üê Notebook principal\n",
    "‚îú‚îÄ‚îÄ output/                ‚Üê Generado al ejecutar\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config_rq9.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ predictions_shift_*.parquet\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ metrics_by_shift.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Fig_RQ9_1_shift_degradation.{png,pdf}\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Fig_RQ9_2_map_vs_shift.{png,pdf}\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Table_RQ9_1_shift_stress_test.{csv,tex}\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Table_RQ9_2_component_ablation.{csv,tex}\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ summary_rq9.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ figure_captions.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ README_RQ9.md\n",
    "```\n",
    "\n",
    "### Paths Relativos Verificados:\n",
    "\n",
    "```python\n",
    "BASE_DIR = Path('../..')                                    # ‚úÖ Root del proyecto\n",
    "DATA_DIR = BASE_DIR / 'data'                               # ‚úÖ Datos BDD100K\n",
    "TEMPERATURE_FILE = BASE_DIR / 'fase 4' / 'outputs' / ...   # ‚úÖ Resultados Fase 4\n",
    "coco_file = DATA_DIR / 'bdd100k_coco' / 'val_eval.json'   # ‚úÖ Anotaciones\n",
    "img_path = DATA_DIR / 'bdd100k' / 'bdd100k' / ...         # ‚úÖ Im√°genes\n",
    "```\n",
    "\n",
    "### Consistencia con Fases Anteriores:\n",
    "\n",
    "- ‚úÖ Usa misma estructura de hooks que RQ6\n",
    "- ‚úÖ Usa misma funci√≥n `compute_iou()` que Fase 5\n",
    "- ‚úÖ Usa misma funci√≥n `normalize_label()` que todas las fases\n",
    "- ‚úÖ Carga temperatura de Fase 4 igual que RQ7\n",
    "- ‚úÖ Formato de salida consistente con RQ6-RQ8\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "\n",
    "1. **Ejecutar celda 1**: Configuraci√≥n e imports (~30 segundos)\n",
    "2. **Ejecutar celda 5**: Cargar modelo GroundingDINO (~30 segundos)\n",
    "3. **Ejecutar celda 9**: Registrar hooks del decoder (~5 segundos)\n",
    "4. **Ejecutar celda 11**: Procesar dataset **[M√ÅS COSTOSA: 30-60 min]**\n",
    "5. **Ejecutar resto**: M√©tricas, figuras y tablas (~5 minutos)\n",
    "\n",
    "**Total estimado**: 40-70 minutos con GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
