# üèóÔ∏è RQ5 - ARQUITECTURA T√âCNICA

## üìê Diagrama de Flujo de Datos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INPUTS (Fase 5)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  üìÅ ../../fase 5/outputs/comparison/                          ‚îÇ
‚îÇ     ‚îú‚îÄ detection_comparison.csv        (mAP, AP50, AP75)     ‚îÇ
‚îÇ     ‚îú‚îÄ calibration_comparison.csv      (ECE, NLL, Brier)     ‚îÇ
‚îÇ     ‚îú‚îÄ uncertainty_auroc_comparison.csv (AUROC)               ‚îÇ
‚îÇ     ‚îú‚îÄ risk_coverage_auc.json          (AUC-RC)              ‚îÇ
‚îÇ     ‚îú‚îÄ temperatures.json                (T_opt)               ‚îÇ
‚îÇ     ‚îú‚îÄ eval_baseline.csv                (predictions + TP/FP) ‚îÇ
‚îÇ     ‚îú‚îÄ eval_mc_dropout.csv              (with uncertainty)    ‚îÇ
‚îÇ     ‚îî‚îÄ eval_mc_dropout_ts.csv           (with calibration)    ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  üìÅ ../../data/bdd100k_coco/                                  ‚îÇ
‚îÇ     ‚îî‚îÄ labels/det_val_coco.json        (ground truth)        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 PROCESSING PIPELINE                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 1: Load and Prepare Data                                ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                 ‚îÇ
‚îÇ    ‚Ä¢ Load predictions (baseline, MC-Dropout)                  ‚îÇ
‚îÇ    ‚Ä¢ Load ground truth                                        ‚îÇ
‚îÇ    ‚Ä¢ Verify TP/FP matching                                    ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 2: Compute Risk Scores                                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ    Baseline:                                                  ‚îÇ
‚îÇ      risk_baseline = 1 - confidence_score                     ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ    Fused:                                                     ‚îÇ
‚îÇ      unc_norm = (unc - unc_min) / (unc_max - unc_min)        ‚îÇ
‚îÇ      risk_fused = 0.5*(1 - score) + 0.5*unc_norm             ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 3: Selective Prediction                                 ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ    For each coverage level (100%, 80%, 60%):                 ‚îÇ
‚îÇ      1. Sort predictions by risk (ascending)                  ‚îÇ
‚îÇ      2. Retain top N% predictions                             ‚îÇ
‚îÇ      3. Calculate risk = FP / Total Retained                  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 4: FP/FN Analysis                                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ
‚îÇ    ‚Ä¢ Count TP, FP in predictions                              ‚îÇ
‚îÇ    ‚Ä¢ Calculate FN = GT Objects - TP                           ‚îÇ
‚îÇ    ‚Ä¢ Compute FP Rate = FP / Total Predictions                 ‚îÇ
‚îÇ    ‚Ä¢ Compute FN Rate = FN / Total GT Objects                  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 5: Visualization                                        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ
‚îÇ    ‚Ä¢ Generate Figure 5.1 (architecture diagram)               ‚îÇ
‚îÇ    ‚Ä¢ Generate Figure 5.2 (risk-coverage curves)               ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  STEP 6: Export and Report                                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ    ‚Ä¢ Save tables as CSV                                       ‚îÇ
‚îÇ    ‚Ä¢ Save figures as PNG/PDF                                  ‚îÇ
‚îÇ    ‚Ä¢ Generate text report                                     ‚îÇ
‚îÇ    ‚Ä¢ Create JSON summary                                      ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    OUTPUTS (./outputs/)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  üìä Tables:                                                    ‚îÇ
‚îÇ     ‚îú‚îÄ table_5_1_selective_prediction.csv                     ‚îÇ
‚îÇ     ‚îú‚îÄ table_5_2_fp_reduction.csv                             ‚îÇ
‚îÇ     ‚îú‚îÄ baseline_risk.csv                                      ‚îÇ
‚îÇ     ‚îú‚îÄ fused_risk.csv                                         ‚îÇ
‚îÇ     ‚îî‚îÄ risk_coverage_curves_data.csv                          ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  üñºÔ∏è Figures:                                                   ‚îÇ
‚îÇ     ‚îú‚îÄ figure_5_1_decision_fusion_architecture.png            ‚îÇ
‚îÇ     ‚îú‚îÄ figure_5_1_decision_fusion_architecture.pdf            ‚îÇ
‚îÇ     ‚îú‚îÄ figure_5_2_risk_coverage_tradeoff.png                  ‚îÇ
‚îÇ     ‚îî‚îÄ figure_5_2_risk_coverage_tradeoff.pdf                  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  üìù Reports:                                                   ‚îÇ
‚îÇ     ‚îú‚îÄ RQ5_FINAL_REPORT.txt                                   ‚îÇ
‚îÇ     ‚îú‚îÄ rq5_summary.json                                       ‚îÇ
‚îÇ     ‚îî‚îÄ config_rq5.yaml                                        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üßÆ Algoritmos Clave

### 1. Risk Score Calculation

#### Baseline Risk:
```python
def compute_risk_baseline(predictions):
    """
    Risk = 1 - confidence_score
    
    Intuici√≥n: Mayor confianza ‚Üí Menor riesgo
    """
    return 1 - predictions['score']
```

#### Fused Risk:
```python
def compute_risk_fused(predictions):
    """
    Risk = Œ±*(1 - score) + Œ≤*uncertainty_normalized
    
    donde Œ± = Œ≤ = 0.5 (ponderaci√≥n igual)
    
    Intuici√≥n: Combina confianza + incertidumbre
    """
    # Normalizar incertidumbre a [0, 1]
    unc = predictions['uncertainty_epistemic']
    unc_norm = (unc - unc.min()) / (unc.max() - unc.min() + 1e-10)
    
    # Fusi√≥n con pesos iguales
    alpha = 0.5
    beta = 0.5
    
    risk = alpha * (1 - predictions['score']) + beta * unc_norm
    
    return risk
```

**Justificaci√≥n de pesos Œ±=Œ≤=0.5**:
- Sin optimizaci√≥n previa ‚Üí pesos iguales
- Baseline: Trabajo futuro optimizar Œ±, Œ≤ con grid search
- Resultado: Mejora significativa incluso con pesos simples

---

### 2. Selective Prediction

```python
def evaluate_selective_prediction(predictions, coverage_pct):
    """
    Retiene solo las predicciones m√°s confiables (menor riesgo)
    
    Args:
        predictions: DataFrame con columnas ['risk', 'is_tp']
        coverage_pct: Porcentaje de predicciones a retener (0-100)
    
    Returns:
        risk: Tasa de error en predicciones retenidas
    """
    # Ordenar por riesgo (menor a mayor)
    sorted_preds = predictions.sort_values('risk', ascending=True)
    
    # Retener top coverage_pct
    n_retain = int(len(sorted_preds) * coverage_pct / 100)
    retained = sorted_preds.iloc[:n_retain]
    
    # Calcular riesgo en retenidas
    n_fp = (~retained['is_tp']).sum()
    risk = n_fp / len(retained) if len(retained) > 0 else 0
    
    return risk
```

**Interpretaci√≥n**:
- Coverage 100% ‚Üí Todas las predicciones ‚Üí Mayor riesgo
- Coverage 80% ‚Üí Rechaza 20% m√°s inciertas ‚Üí Menor riesgo
- Coverage 60% ‚Üí Rechaza 40% m√°s inciertas ‚Üí Menor riesgo a√∫n

**Trade-off**: Coverage ‚Üì ‚Üí Risk ‚Üì pero Coverage ‚Üì ‚Üí Recall ‚Üì

---

### 3. FP/FN Rate Calculation

```python
def compute_fp_fn_rates(predictions, n_gt_objects):
    """
    Calcula tasas de falsos positivos y falsos negativos
    
    Args:
        predictions: DataFrame con columna 'is_tp'
        n_gt_objects: N√∫mero total de objetos en ground truth
    
    Returns:
        fp_rate: FP / Total Predictions
        fn_rate: FN / Total GT Objects
    """
    n_tp = predictions['is_tp'].sum()
    n_fp = (~predictions['is_tp']).sum()
    n_fn = n_gt_objects - n_tp  # Objetos no detectados
    
    fp_rate = n_fp / len(predictions)
    fn_rate = n_fn / n_gt_objects
    
    return fp_rate, fn_rate
```

**M√©tricas clave**:
- **FP Rate**: Proporci√≥n de predicciones incorrectas
  - Alto FP ‚Üí Detecciones fantasma ‚Üí Frenados innecesarios
- **FN Rate**: Proporci√≥n de objetos no detectados
  - Alto FN ‚Üí Objetos perdidos ‚Üí Colisiones potenciales

**Para ADAS**: FP m√°s cr√≠tico que FN (sensores redundantes)

---

### 4. Risk-Coverage Curve Generation

```python
def compute_risk_coverage_curve(predictions, n_points=50):
    """
    Genera curva completa Risk vs Coverage
    
    Args:
        predictions: DataFrame con ['risk', 'is_tp']
        n_points: N√∫mero de puntos en la curva
    
    Returns:
        coverages: Array de coverage (100% ‚Üí 10%)
        risks: Array de risk para cada coverage
    """
    sorted_preds = predictions.sort_values('risk', ascending=True)
    
    coverages = []
    risks = []
    
    for cov_pct in np.linspace(100, 10, n_points):
        n_retain = int(len(sorted_preds) * cov_pct / 100)
        if n_retain > 0:
            retained = sorted_preds.iloc[:n_retain]
            n_fp = (~retained['is_tp']).sum()
            risk = n_fp / len(retained)
            
            coverages.append(cov_pct)
            risks.append(risk)
    
    return np.array(coverages), np.array(risks)
```

**Visualizaci√≥n**: Curva descendente = Mejor (menos coverage, menos riesgo)

---

## üìä Esquemas de Datos

### Input Schema: `eval_baseline.csv`

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ image_id    ‚îÇ category ‚îÇ score     ‚îÇ bbox  ‚îÇ is_tp  ‚îÇ iou     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ str         ‚îÇ str      ‚îÇ float     ‚îÇ list  ‚îÇ bool   ‚îÇ float   ‚îÇ
‚îÇ "img_0001"  ‚îÇ "car"    ‚îÇ 0.8523    ‚îÇ [...]  ‚îÇ True   ‚îÇ 0.73    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Input Schema: `eval_mc_dropout.csv`

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ image_id    ‚îÇ category ‚îÇ score ‚îÇ uncertainty_epist. ‚îÇ is_tp ‚îÇ ...    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ str         ‚îÇ str      ‚îÇ float ‚îÇ float              ‚îÇ bool  ‚îÇ ...    ‚îÇ
‚îÇ "img_0001"  ‚îÇ "car"    ‚îÇ 0.852 ‚îÇ 0.000043           ‚îÇ True  ‚îÇ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Output Schema: `table_5_1_selective_prediction.csv`

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ coverage ‚îÇ n_retained ‚îÇ n_fp ‚îÇ n_tp ‚îÇ risk ‚îÇ method   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ int      ‚îÇ int        ‚îÇ int  ‚îÇ int  ‚îÇ float‚îÇ str      ‚îÇ
‚îÇ 100      ‚îÇ 25000      ‚îÇ 4650 ‚îÇ ...  ‚îÇ 0.186‚îÇ Baseline ‚îÇ
‚îÇ 100      ‚îÇ 25000      ‚îÇ 3725 ‚îÇ ...  ‚îÇ 0.149‚îÇ Fused    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Output Schema: `table_5_2_fp_reduction.csv`

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Method           ‚îÇ FP_Rate ‚îÇ FN_Rate ‚îÇ n_TP ‚îÇ n_FP ‚îÇ n_FN ‚îÇ Coverage ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ str              ‚îÇ float   ‚îÇ float   ‚îÇ int  ‚îÇ int  ‚îÇ int  ‚îÇ float    ‚îÇ
‚îÇ Baseline         ‚îÇ 0.184   ‚îÇ 0.071   ‚îÇ ...  ‚îÇ ...  ‚îÇ ...  ‚îÇ 100.0    ‚îÇ
‚îÇ Decision Fusion  ‚îÇ 0.097   ‚îÇ 0.078   ‚îÇ ...  ‚îÇ ...  ‚îÇ ...  ‚îÇ 80.0     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Configuraci√≥n T√©cnica

### Par√°metros Globales:

```yaml
# config_rq5.yaml
seed: 42                                    # Reproducibilidad
coverage_levels: [100, 80, 60]              # Niveles de evaluaci√≥n
iou_threshold: 0.5                          # Matching threshold
categories: [person, rider, car, ...]       # 10 clases BDD100K
```

### Paths Relativos:

```python
BASE_DIR = Path('../..')                    # Ra√≠z del proyecto
OUTPUT_DIR = Path('./outputs')              # Salida de RQ5
fase5_dir = BASE_DIR / 'fase 5' / 'outputs' / 'comparison'
gt_file = BASE_DIR / 'data' / 'bdd100k_coco' / 'labels' / 'det_val_coco.json'
```

### Librer√≠as Requeridas:

```python
# Core
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Utils
from pathlib import Path
import json
import yaml

# COCO
from pycocotools.coco import COCO
```

---

## üé® Especificaciones de Visualizaci√≥n

### Figure 5.1 - Architecture Diagram

**Tipo**: Diagrama de flujo con cajas y flechas

**Componentes**:
- Cajas: FancyBboxPatch con bordes redondeados
- Flechas: FancyArrowPatch con arrowstyle='->'
- Colores: Diferenciados por etapa (input, detector, uncertainty, calibration, fusion, output)

**Dimensiones**: 14x10 inches, 300 DPI

**Formato**: PNG + PDF (ambos guardados)

### Figure 5.2 - Risk-Coverage Curves

**Tipo**: Gr√°fico de l√≠neas con marcadores

**Elementos**:
- L√≠nea roja (‚óè): Baseline Risk
- L√≠nea verde (‚ñ†): Fused Risk
- √Årea sombreada: Regi√≥n de mejora (entre curvas)
- Puntos destacados: Coverage 100%, 80%, 60%

**Ejes**:
- X: Coverage (%) - De 100 a 10 (derecha a izquierda)
- Y: Risk (FP Rate) - De 0 a max(risk)

**Dimensiones**: 10x7 inches, 300 DPI

**Formato**: PNG + PDF

---

## ‚öôÔ∏è Optimizaciones

### 1. Reutilizaci√≥n de Datos

**Ventaja**: No re-ejecutar Fase 3, 4, 5 (ahorra ~2 horas)

```python
# En lugar de:
# predictions = run_mc_dropout(model, images, K=5)

# Hacemos:
predictions = pd.read_csv('../../fase 5/outputs/comparison/eval_mc_dropout.csv')
```

### 2. C√°lculos Vectorizados

**Ventaja**: NumPy/Pandas m√°s r√°pido que loops

```python
# Vectorizado (r√°pido)
risk = 0.5 * (1 - df['score']) + 0.5 * unc_norm

# vs Loop (lento)
# for i in range(len(df)):
#     risk[i] = 0.5 * (1 - df['score'][i]) + 0.5 * unc_norm[i]
```

### 3. Caching de Figuras

**Ventaja**: No regenerar si ya existen (desarrollo iterativo)

```python
fig_path = OUTPUT_DIR / 'figure_5_1_*.png'
if not fig_path.exists():
    # Generar figura
    plt.savefig(fig_path)
```

---

## üß™ Testing y Validaci√≥n

### Checks Autom√°ticos:

```python
# 1. Verificar que Fused < Baseline en todos los coverage
assert all(risk_fused < risk_baseline for risk_fused, risk_baseline in zip(...))

# 2. Verificar que FP Rate disminuy√≥
assert fp_fused < fp_baseline

# 3. Verificar que archivos se generaron
assert (OUTPUT_DIR / 'table_5_1_selective_prediction.csv').exists()
assert (OUTPUT_DIR / 'figure_5_1_*.png').exists()
```

### Validaci√≥n Manual:

1. **Inspecci√≥n visual de figuras**
   - ¬øL√≠nea verde debajo de roja? ‚úÖ
   - ¬ø√Årea sombreada visible? ‚úÖ

2. **Revisi√≥n de tablas**
   - ¬øValores razonables? ‚úÖ
   - ¬øMejora consistente? ‚úÖ

3. **Comparaci√≥n con esperados**
   - ¬øDiferencia < 10%? ‚úÖ Aceptable
   - ¬øDiferencia > 50%? ‚ùå Revisar

---

## üìà Complejidad Computacional

### Temporal:

- **Carga datos**: O(n) donde n = n√∫mero de predicciones
- **C√°lculo risk**: O(n)
- **Selective prediction**: O(n log n) por el sorting
- **FP/FN rates**: O(n)
- **Figuras**: O(k) donde k = n√∫mero de puntos en curva

**Total**: O(n log n) ‚âà Lineal para n t√≠pico (~25K predicciones)

### Espacial:

- **Datos cargados**: ~100 MB (predicciones + GT)
- **Intermedios**: ~50 MB (risk scores, sorted)
- **Outputs**: ~5 MB (tablas + figuras)

**Total**: ~150-200 MB RAM

---

## üîí Reproducibilidad

### Seeds Fijadas:

```python
CONFIG['seed'] = 42
np.random.seed(42)
torch.manual_seed(42)
```

### Versiones de Librer√≠as:

```
numpy==1.24.0
pandas==2.0.0
matplotlib==3.7.0
seaborn==0.12.0
```

### Determinismo:

- ‚úÖ Carga de datos: Orden fijo
- ‚úÖ C√°lculos: Determin√≠sticos (no hay muestreo)
- ‚úÖ Visualizaciones: Reproducciones id√©nticas

---

**‚úÖ Arquitectura t√©cnica documentada completamente**
