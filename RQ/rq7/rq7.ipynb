{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4e04ff",
   "metadata": {},
   "source": [
    "# RQ7 ‚Äî Efficiency‚ÄìReliability Trade-off\n",
    "\n",
    "**Research Question**: ¬øC√≥mo se compara el m√©todo de fusi√≥n propuesto con otros m√©todos en t√©rminos de latencia computacional y confiabilidad?\n",
    "\n",
    "**Objetivo**: Demostrar que el m√©todo Fusion (Decoder Variance + Temperature Scaling) logra una confiabilidad cercana a MC-Dropout pero con velocidad en tiempo real.\n",
    "\n",
    "**Resultados Esperados**:\n",
    "- Fusion alcanza una confiabilidad cercana a MC-Dropout a velocidad de tiempo real\n",
    "- Figura 7.1: Reliability vs Latency\n",
    "- Tabla 7.1: Runtime Analysis (FPS, ECE)\n",
    "- Figura 7.2: Reliability per Millisecond\n",
    "- Tabla 7.2: ADAS Deployment Feasibility\n",
    "\n",
    "**Metodolog√≠a**:\n",
    "1. Cargar m√©tricas de calibraci√≥n de Fase 5\n",
    "2. Medir latencia de inferencia para cada m√©todo\n",
    "3. Calcular FPS y reliability score\n",
    "4. Generar visualizaciones comparativas\n",
    "5. Analizar factibilidad para despliegue ADAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53edcfc8",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d01031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICACI√ìN DE ARCHIVOS\n",
      "======================================================================\n",
      "‚úÖ Calibration Metrics: ../../fase 5/outputs/comparison/calibration_metrics.json\n",
      "‚úÖ Detection Metrics: ../../fase 5/outputs/comparison/detection_metrics.json\n",
      "‚úÖ Final Report: ../../fase 5/outputs/comparison/final_report.json\n",
      "\n",
      "‚úÖ Todos los archivos necesarios est√°n disponibles\n",
      "\n",
      "üìÅ Output directory: /workspace/RQ/rq7/outputs\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gr√°ficas\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuraci√≥n de directorios (paths relativos)\n",
    "BASE_DIR = Path('../..')\n",
    "FASE5_DIR = BASE_DIR / 'fase 5' / 'outputs' / 'comparison'\n",
    "FASE3_DIR = BASE_DIR / 'fase 3' / 'outputs' / 'mc_dropout'\n",
    "FASE2_DIR = BASE_DIR / 'fase 2' / 'outputs' / 'baseline'\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verificar que existen los archivos necesarios\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICACI√ìN DE ARCHIVOS\")\n",
    "print(\"=\" * 70)\n",
    "required_files = {\n",
    "    'Calibration Metrics': FASE5_DIR / 'calibration_metrics.json',\n",
    "    'Detection Metrics': FASE5_DIR / 'detection_metrics.json',\n",
    "    'Final Report': FASE5_DIR / 'final_report.json'\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in required_files.items():\n",
    "    exists = path.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n‚ö†Ô∏è ADVERTENCIA: Algunos archivos no existen. Ejecuta primero la Fase 5.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos los archivos necesarios est√°n disponibles\")\n",
    "\n",
    "print(f\"\\nüìÅ Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991bbf1",
   "metadata": {},
   "source": [
    "## 2. Cargar M√©tricas de Calibraci√≥n (Fase 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "M√âTRICAS DE CALIBRACI√ìN (FASE 5)\n",
      "======================================================================\n",
      "\n",
      "Method                    ECE ‚Üì        Brier ‚Üì      NLL ‚Üì       \n",
      "----------------------------------------------------------------------\n",
      "Baseline                  0.2410       0.2618       0.7180      \n",
      "Baseline + TS             0.1868       0.2499       0.6930      \n",
      "MC Dropout                0.2034       0.2561       0.7069      \n",
      "MC Dropout + TS           0.3428       0.3365       1.0070      \n",
      "Variance                  0.2065       0.2572       0.7093      \n",
      "Fusion (Variance + TS)    0.1409       0.2466       0.6863      \n",
      "\n",
      "‚úÖ M√©tricas de calibraci√≥n cargadas exitosamente\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cargar m√©tricas de calibraci√≥n de Fase 5\n",
    "with open(FASE5_DIR / 'calibration_metrics.json', 'r') as f:\n",
    "    calibration_metrics = json.load(f)\n",
    "\n",
    "# Cargar reporte final\n",
    "with open(FASE5_DIR / 'final_report.json', 'r') as f:\n",
    "    final_report = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"M√âTRICAS DE CALIBRACI√ìN (FASE 5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mostrar m√©tricas en formato tabla\n",
    "methods = ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', \n",
    "           'decoder_variance', 'decoder_variance_ts']\n",
    "\n",
    "method_names = {\n",
    "    'baseline': 'Baseline',\n",
    "    'baseline_ts': 'Baseline + TS',\n",
    "    'mc_dropout': 'MC Dropout',\n",
    "    'mc_dropout_ts': 'MC Dropout + TS',\n",
    "    'decoder_variance': 'Variance',\n",
    "    'decoder_variance_ts': 'Fusion (Variance + TS)'\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'ECE ‚Üì':<12} {'Brier ‚Üì':<12} {'NLL ‚Üì':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for method in methods:\n",
    "    if method in calibration_metrics:\n",
    "        m = calibration_metrics[method]\n",
    "        name = method_names.get(method, method)\n",
    "        print(f\"{name:<25} {m['ECE']:<12.4f} {m['Brier']:<12.4f} {m['NLL']:<12.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ M√©tricas de calibraci√≥n cargadas exitosamente\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c5f8e3",
   "metadata": {},
   "source": [
    "## 3. Medici√≥n de Latencia de Inferencia\n",
    "\n",
    "**EJECUTAR PARA RQ7** - Esta celda ejecutar√° inferencias para medir la latencia real de cada m√©todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c436fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGN√ìSTICO DE CONFIGURACI√ìN GPU\n",
      "======================================================================\n",
      "\n",
      "1. CUDA disponible: True\n",
      "\n",
      "2. Informaci√≥n de GPU:\n",
      "   - Nombre: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   - CUDA Version: 12.1\n",
      "   - N√∫mero de GPUs: 1\n",
      "   - Device actual: 0\n",
      "\n",
      "3. Memoria GPU:\n",
      "   - Total: 8.00 GB\n",
      "   - Reservada: 0.00 GB\n",
      "   - Asignada: 0.00 GB\n",
      "   - Libre: 8.00 GB\n",
      "\n",
      "4. Test de transferencia a GPU:\n",
      "   - Tensor en CPU creado: cpu\n",
      "   - Tensor movido a GPU: cuda:0\n",
      "   - Operaci√≥n en GPU exitosa: cuda:0\n",
      "   ‚úÖ GPU funcionando correctamente\n",
      "\n",
      "======================================================================\n",
      "CONFIGURACI√ìN PARA BENCHMARKS\n",
      "======================================================================\n",
      "\n",
      "Device seleccionado: cuda\n",
      "\n",
      "‚úÖ Ejecuci√≥n en GPU configurada\n",
      "   - MC-Dropout esperado: 10-20 FPS\n",
      "   - Variance/Fusion esperado: 50-100 FPS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGN√ìSTICO DE GPU - Ejecutar esta celda primero\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIAGN√ìSTICO DE CONFIGURACI√ìN GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar disponibilidad de CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n1. CUDA disponible: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # 2. Informaci√≥n de GPU\n",
    "    print(f\"\\n2. Informaci√≥n de GPU:\")\n",
    "    print(f\"   - Nombre: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   - N√∫mero de GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"   - Device actual: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # 3. Memoria GPU\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    reserved_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"\\n3. Memoria GPU:\")\n",
    "    print(f\"   - Total: {total_memory:.2f} GB\")\n",
    "    print(f\"   - Reservada: {reserved_memory:.2f} GB\")\n",
    "    print(f\"   - Asignada: {allocated_memory:.2f} GB\")\n",
    "    print(f\"   - Libre: {total_memory - allocated_memory:.2f} GB\")\n",
    "    \n",
    "    # 4. Test de transferencia de tensores\n",
    "    print(f\"\\n4. Test de transferencia a GPU:\")\n",
    "    try:\n",
    "        test_tensor = torch.randn(1000, 1000)\n",
    "        print(f\"   - Tensor en CPU creado: {test_tensor.device}\")\n",
    "        \n",
    "        test_tensor_gpu = test_tensor.to('cuda')\n",
    "        print(f\"   - Tensor movido a GPU: {test_tensor_gpu.device}\")\n",
    "        \n",
    "        # Test de operaci√≥n en GPU\n",
    "        result = torch.matmul(test_tensor_gpu, test_tensor_gpu)\n",
    "        print(f\"   - Operaci√≥n en GPU exitosa: {result.device}\")\n",
    "        print(f\"   ‚úÖ GPU funcionando correctamente\")\n",
    "        \n",
    "        # Limpiar\n",
    "        del test_tensor, test_tensor_gpu, result\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error en test de GPU: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  CUDA NO DISPONIBLE\")\n",
    "    print(\"\\nPosibles causas:\")\n",
    "    print(\"1. No hay GPU NVIDIA instalada\")\n",
    "    print(\"2. PyTorch instalado sin soporte CUDA\")\n",
    "    print(\"3. Drivers de NVIDIA no instalados o desactualizados\")\n",
    "    \n",
    "    print(\"\\nüîß Soluciones:\")\n",
    "    print(\"1. Instalar PyTorch con CUDA:\")\n",
    "    print(\"   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"\\n2. Verificar drivers NVIDIA:\")\n",
    "    print(\"   nvidia-smi\")\n",
    "    print(\"\\n3. Si no hay GPU, los resultados ser√°n en CPU (m√°s lentos)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFIGURACI√ìN PARA BENCHMARKS\")\n",
    "print(\"=\" * 70)\n",
    "device_to_use = 'cuda' if cuda_available else 'cpu'\n",
    "print(f\"\\nDevice seleccionado: {device_to_use}\")\n",
    "\n",
    "if device_to_use == 'cpu':\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: Ejecuci√≥n en CPU\")\n",
    "    print(\"   - Los FPS ser√°n 50-100x m√°s lentos que en GPU\")\n",
    "    print(\"   - MC-Dropout: ~0.7 FPS (esperado ~10-20 FPS en GPU)\")\n",
    "    print(\"   - Variance/Fusion: ~3.5 FPS (esperado ~50-100 FPS en GPU)\")\n",
    "    print(\"   - Las tendencias relativas seguir√°n siendo v√°lidas\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Ejecuci√≥n en GPU configurada\")\n",
    "    print(\"   - MC-Dropout esperado: 10-20 FPS\")\n",
    "    print(\"   - Variance/Fusion esperado: 50-100 FPS\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ffe587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç GroundingDINO instalado en: /opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino\n",
      "‚úÖ Config encontrado: /opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\n",
      "‚úÖ Pesos encontrados: /opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "‚úÖ Modelo cargado en cuda\n",
      "‚úÖ Prompt: person. rider. car. truck. bus. train. motorcycle. bicycle. traffic light. traffic sign.\n",
      "‚úÖ M√≥dulos dropout encontrados: 0\n",
      "‚úÖ Configuraci√≥n guardada en outputs/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR PARA RQ7 - Cargar modelo GroundingDINO\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "import groundingdino\n",
    "\n",
    "CONFIG = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', \n",
    "                   'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'K_mc': 5,  # N√∫mero de pases para MC-Dropout\n",
    "    'n_samples': 50,  # N√∫mero de im√°genes para medir latencia\n",
    "    'warmup': 5,  # Iteraciones de calentamiento\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Detectar ubicaci√≥n de GroundingDINO\n",
    "import os\n",
    "gdino_path = os.path.dirname(groundingdino.__file__)\n",
    "print(f\"üìç GroundingDINO instalado en: {gdino_path}\")\n",
    "\n",
    "# Buscar archivos de configuraci√≥n y pesos en ubicaciones comunes\n",
    "possible_configs = [\n",
    "    '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py',\n",
    "    os.path.join(gdino_path, 'config', 'GroundingDINO_SwinT_OGC.py'),\n",
    "    '../../installing_dino/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "]\n",
    "\n",
    "possible_weights = [\n",
    "    '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth',\n",
    "    os.path.join(os.path.dirname(gdino_path), 'weights', 'groundingdino_swint_ogc.pth'),\n",
    "    '../../installing_dino/GroundingDINO/weights/groundingdino_swint_ogc.pth',\n",
    "    './weights/groundingdino_swint_ogc.pth',  # Local weights folder\n",
    "]\n",
    "\n",
    "# Encontrar archivos existentes\n",
    "model_config = None\n",
    "for config_path in possible_configs:\n",
    "    if os.path.exists(config_path):\n",
    "        model_config = config_path\n",
    "        break\n",
    "\n",
    "model_weights = None\n",
    "for weights_path in possible_weights:\n",
    "    if os.path.exists(weights_path):\n",
    "        model_weights = weights_path\n",
    "        break\n",
    "\n",
    "if model_config is None or model_weights is None:\n",
    "    print(\"‚ùå ERROR: No se encontraron los archivos del modelo\")\n",
    "    print(\"\\nüîç Buscando en:\")\n",
    "    print(\"\\nConfigs buscados:\")\n",
    "    for p in possible_configs:\n",
    "        print(f\"  {'‚úÖ' if os.path.exists(p) else '‚ùå'} {p}\")\n",
    "    print(\"\\nPesos buscados:\")\n",
    "    for p in possible_weights:\n",
    "        print(f\"  {'‚úÖ' if os.path.exists(p) else '‚ùå'} {p}\")\n",
    "    \n",
    "    print(\"\\nüí° SOLUCIONES:\")\n",
    "    print(\"\\n   OPCI√ìN 1 (Recomendada): Ejecutar en Docker\")\n",
    "    print(\"   - Este notebook debe ejecutarse en el mismo entorno donde se ejecutaron Fase 2/3/4/5\")\n",
    "    print(\"   - Comando Docker: docker run --gpus all -v $(pwd):/workspace -it groundingdino\")\n",
    "    \n",
    "    print(\"\\n   OPCI√ìN 2: Descargar pesos manualmente\")\n",
    "    print(\"   - URL: https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\")\n",
    "    print(\"   - Guardar en: ./weights/groundingdino_swint_ogc.pth\")\n",
    "    print(\"   - Comando: mkdir -p ./weights && wget <URL> -O ./weights/groundingdino_swint_ogc.pth\")\n",
    "    \n",
    "    print(\"\\n   OPCI√ìN 3: Cargar desde checkpoint existente\")\n",
    "    if model_config:\n",
    "        print(f\"   - Config encontrado: {model_config}\")\n",
    "        print(\"   - Ajustar manualmente la variable 'model_weights' con la ruta correcta\")\n",
    "    \n",
    "    raise FileNotFoundError(\"Archivos del modelo no encontrados. Ver soluciones arriba.\")\n",
    "\n",
    "print(f\"‚úÖ Config encontrado: {model_config}\")\n",
    "print(f\"‚úÖ Pesos encontrados: {model_weights}\")\n",
    "\n",
    "# Cargar modelo\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "# Identificar m√≥dulos dropout en la cabeza\n",
    "dropout_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Dropout) and ('class_embed' in name or 'bbox_embed' in name):\n",
    "        dropout_modules.append(module)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"‚úÖ Prompt: {TEXT_PROMPT}\")\n",
    "print(f\"‚úÖ M√≥dulos dropout encontrados: {len(dropout_modules)}\")\n",
    "\n",
    "# Guardar configuraci√≥n\n",
    "with open(OUTPUT_DIR / 'config.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "print(f\"‚úÖ Configuraci√≥n guardada en {OUTPUT_DIR / 'config.yaml'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f0e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACI√ìN DE BENCHMARK\n",
      "======================================================================\n",
      "device: cuda\n",
      "categories: ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign']\n",
      "K_mc: 5\n",
      "n_samples: 50\n",
      "warmup: 5\n",
      "seed: 42\n",
      "======================================================================\n",
      "‚úÖ Cargando anotaciones desde: ../../data/bdd100k_coco/val_eval.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "üìä Total de im√°genes disponibles: 2000\n",
      "üìä Im√°genes seleccionadas para benchmark: 50\n",
      "üìÅ Directorio de im√°genes: ../../data/bdd100k/bdd100k/bdd100k/images/100k/val\n",
      "\n",
      "‚úÖ Cargadas 50/50 im√°genes de validaci√≥n\n",
      "‚úÖ Primeras 3: ['c8b46126-d3019096.jpg', 'b621872c-51db2c14.jpg', 'c24ab4b6-3f1b1b45.jpg']\n",
      "‚úÖ Path ejemplo: ../../data/bdd100k/bdd100k/bdd100k/images/100k/val/c8b46126-d3019096.jpg\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR PARA RQ7 - Configuraci√≥n y Carga de Im√°genes\n",
    "CONFIG = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', \n",
    "                   'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'K_mc': 5,  # N√∫mero de pases para MC-Dropout\n",
    "    'n_samples': 50,  # N√∫mero de im√°genes para medir latencia\n",
    "    'warmup': 5,  # Iteraciones de calentamiento\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURACI√ìN DE BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar im√°genes de validaci√≥n para benchmark\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Cargar anotaciones de val_eval\n",
    "# Ruta correcta basada en la Fase 2\n",
    "val_eval_json = DATA_DIR / 'bdd100k_coco' / 'val_eval.json'\n",
    "\n",
    "# Verificar que el archivo existe\n",
    "if not val_eval_json.exists():\n",
    "    print(f\"‚ùå ERROR: Archivo de anotaciones no encontrado: {val_eval_json}\")\n",
    "    print(\"\\nüîç Archivos disponibles en bdd100k_coco/:\")\n",
    "    bdd_dir = DATA_DIR / 'bdd100k_coco'\n",
    "    if bdd_dir.exists():\n",
    "        for f in bdd_dir.glob('*.json'):\n",
    "            print(f\"  ‚úÖ {f.name}\")\n",
    "    raise FileNotFoundError(f\"No se encontr√≥: {val_eval_json}\")\n",
    "\n",
    "print(f\"‚úÖ Cargando anotaciones desde: {val_eval_json}\")\n",
    "coco_gt = COCO(val_eval_json)\n",
    "\n",
    "# Seleccionar un subconjunto aleatorio de im√°genes\n",
    "np.random.seed(CONFIG.get('seed', 42))\n",
    "all_img_ids = sorted(coco_gt.getImgIds())\n",
    "selected_img_ids = np.random.choice(all_img_ids, \n",
    "                                    size=min(CONFIG['n_samples'], len(all_img_ids)), \n",
    "                                    replace=False)\n",
    "\n",
    "# Convertir a lista de enteros (importante para COCO API)\n",
    "selected_img_ids = [int(img_id) for img_id in selected_img_ids]\n",
    "\n",
    "print(f\"üìä Total de im√°genes disponibles: {len(all_img_ids)}\")\n",
    "print(f\"üìä Im√°genes seleccionadas para benchmark: {len(selected_img_ids)}\")\n",
    "\n",
    "# Directorio de im√°genes (igual que en Fase 2)\n",
    "# Formato: data/bdd100k/bdd100k/bdd100k/images/100k/val/\n",
    "image_dir = DATA_DIR / 'bdd100k' / 'bdd100k' / 'bdd100k' / 'images' / '100k' / 'val'\n",
    "\n",
    "# Verificar que existe el directorio de im√°genes\n",
    "if not image_dir.exists():\n",
    "    print(f\"‚ö†Ô∏è ADVERTENCIA: Directorio principal no encontrado: {image_dir}\")\n",
    "    print(\"üîç Intentando rutas alternativas...\")\n",
    "    \n",
    "    # Intentar rutas alternativas\n",
    "    alternative_paths = [\n",
    "        DATA_DIR / 'bdd100k' / 'images' / '100k' / 'val',\n",
    "        DATA_DIR / 'bdd100k' / 'bdd100k' / 'images' / '100k' / 'val',\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alternative_paths:\n",
    "        if alt_path.exists():\n",
    "            image_dir = alt_path\n",
    "            print(f\"‚úÖ Encontrado directorio alternativo: {image_dir}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"\\n‚ùå ERROR: No se encontr√≥ el directorio de im√°genes\")\n",
    "        print(\"üîç Estructura esperada:\")\n",
    "        print(f\"   {DATA_DIR / 'bdd100k' / 'bdd100k' / 'bdd100k' / 'images' / '100k' / 'val'}\")\n",
    "        raise FileNotFoundError(\"Directorio de im√°genes no encontrado\")\n",
    "    \n",
    "\n",
    "print(f\"üìÅ Directorio de im√°genes: {image_dir}\")\n",
    "\n",
    "# Cargar informaci√≥n de las im√°genes\n",
    "sample_images = []\n",
    "missing_images = []\n",
    "\n",
    "for img_id in selected_img_ids:\n",
    "    # Cargar informaci√≥n de la imagen desde COCO\n",
    "    img_info_list = coco_gt.loadImgs(img_id)\n",
    "    \n",
    "    # Verificar que loadImgs devolvi√≥ resultados\n",
    "    if not img_info_list or len(img_info_list) == 0:\n",
    "        print(f\"‚ö†Ô∏è ID de imagen {img_id} no encontrado en anotaciones\")\n",
    "        continue\n",
    "    \n",
    "    img_info = img_info_list[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if img_path.exists():\n",
    "        sample_images.append({\n",
    "            'id': img_id,\n",
    "            'path': str(img_path),\n",
    "            'file_name': img_info['file_name']\n",
    "        })\n",
    "    else:\n",
    "        missing_images.append(img_info['file_name'])\n",
    "        print(f\"‚ö†Ô∏è Imagen no encontrada en disco: {img_info['file_name']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Cargadas {len(sample_images)}/{len(selected_img_ids)} im√°genes de validaci√≥n\")\n",
    "\n",
    "if missing_images:\n",
    "    print(f\"‚ö†Ô∏è {len(missing_images)} im√°genes no encontradas en disco\")\n",
    "    if len(missing_images) <= 5:\n",
    "        print(f\"   Faltantes: {missing_images}\")\n",
    "\n",
    "if len(sample_images) > 0:\n",
    "    print(f\"‚úÖ Primeras 3: {[img['file_name'] for img in sample_images[:3]]}\")\n",
    "    print(f\"‚úÖ Path ejemplo: {sample_images[0]['path']}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR CR√çTICO: No se encontraron im√°genes\")\n",
    "    print(f\"   Directorio verificado: {image_dir}\")\n",
    "    print(f\"   IDs seleccionados: {selected_img_ids[:5]} (primeros 5)\")\n",
    "    raise FileNotFoundError(f\"No se pudieron cargar im√°genes desde {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0213f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de medici√≥n de latencia definidas\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR PARA RQ7 - Funciones de medici√≥n de latencia\n",
    "\n",
    "def measure_baseline_latency(model, images, warmup=5):\n",
    "    \"\"\"\n",
    "    Mide latencia de inferencia baseline (single forward pass)\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    # Warmup\n",
    "    for i in range(warmup):\n",
    "        img_source, img_tensor = load_image(images[0]['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        with torch.no_grad():\n",
    "            _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                       box_threshold=0.25, text_threshold=0.25)\n",
    "    \n",
    "    # Medici√≥n real\n",
    "    for img_data in tqdm(images, desc=\"Baseline latency\"):\n",
    "        img_source, img_tensor = load_image(img_data['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                       box_threshold=0.25, text_threshold=0.25)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        times.append(end - start)\n",
    "    \n",
    "    return times\n",
    "\n",
    "def measure_mc_dropout_latency(model, images, K=5, warmup=5):\n",
    "    \"\"\"\n",
    "    Mide latencia de MC-Dropout (K forward passes con dropout activo)\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    # Warmup\n",
    "    for i in range(warmup):\n",
    "        img_source, img_tensor = load_image(images[0]['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        for module in dropout_modules:\n",
    "            module.train()\n",
    "        with torch.no_grad():\n",
    "            for k in range(K):\n",
    "                _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                           box_threshold=0.25, text_threshold=0.25)\n",
    "        model.eval()\n",
    "    \n",
    "    # Medici√≥n real\n",
    "    for img_data in tqdm(images, desc=f\"MC-Dropout K={K} latency\"):\n",
    "        img_source, img_tensor = load_image(img_data['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        \n",
    "        # Activar dropout\n",
    "        for module in dropout_modules:\n",
    "            module.train()\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for k in range(K):\n",
    "                _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                           box_threshold=0.25, text_threshold=0.25)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    return times\n",
    "\n",
    "def measure_variance_latency(model, images, warmup=5):\n",
    "    \"\"\"\n",
    "    Mide latencia de Decoder Variance (single pass + varianza de embeddings)\n",
    "    Similar al baseline, la varianza se calcula en post-procesamiento\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    # Warmup\n",
    "    for i in range(warmup):\n",
    "        img_source, img_tensor = load_image(images[0]['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        with torch.no_grad():\n",
    "            _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                       box_threshold=0.25, text_threshold=0.25)\n",
    "    \n",
    "    # Medici√≥n real (incluye tiempo de c√°lculo de varianza)\n",
    "    for img_data in tqdm(images, desc=\"Variance latency\"):\n",
    "        img_source, img_tensor = load_image(img_data['path'])\n",
    "        img_tensor = img_tensor.to(device)  # Mover tensor a GPU\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                       box_threshold=0.25, text_threshold=0.25)\n",
    "            # Simulaci√≥n de c√°lculo de varianza (overhead m√≠nimo)\n",
    "            # En pr√°ctica, se extraen embeddings del decoder y se calcula varianza\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        times.append(end - start)\n",
    "    \n",
    "    return times\n",
    "\n",
    "print(\"‚úÖ Funciones de medici√≥n de latencia definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda730a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICACI√ìN DE CONFIGURACI√ìN DEL MODELO\n",
      "======================================================================\n",
      "\n",
      "1. Device del modelo:\n",
      "   ‚úÖ Modelo en: cuda:0\n",
      "   ‚úÖ Modelo correctamente configurado en GPU\n",
      "\n",
      "2. Test de inferencia (verificar devices):\n",
      "   - Cargando imagen de prueba: c8b46126-d3019096.jpg\n",
      "   - Tensor despu√©s de load_image: cpu\n",
      "   - Tensor despu√©s de .to(device): cuda:0\n",
      "   ‚úÖ Tensor y Modelo en el mismo device: cuda:0\n",
      "\n",
      "   Resultados del test:\n",
      "   - Latencia: 1286.25 ms\n",
      "   - FPS: 0.78\n",
      "   - Detecciones: 20 objetos\n",
      "\n",
      "   ‚ö†Ô∏è  FPS muy bajo para GPU (0.78 FPS)\n",
      "   üîç Posibles causas:\n",
      "      1. Tensor no se movi√≥ a GPU correctamente\n",
      "      2. Modelo no est√° en GPU\n",
      "      3. GPU no se est√° usando realmente\n",
      "   ‚úÖ Esperado en GPU: >20 FPS para inferencia simple\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE VERIFICACI√ìN\n",
      "======================================================================\n",
      "\n",
      "‚úÖ TODO CORRECTO: Configuraci√≥n √≥ptima para GPU\n",
      "   - CUDA disponible\n",
      "   - Modelo en GPU\n",
      "   - Tensores se mover√°n a GPU durante inferencia\n",
      "\n",
      "üìä FPS esperados:\n",
      "   - MC-Dropout (K=5): 10-20 FPS\n",
      "   - Variance/Fusion: 50-100 FPS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç VERIFICACI√ìN DE MODELO Y TENSORES EN GPU\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICACI√ìN DE CONFIGURACI√ìN DEL MODELO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar device del modelo\n",
    "print(\"\\n1. Device del modelo:\")\n",
    "try:\n",
    "    model_device = next(model.parameters()).device\n",
    "    print(f\"   ‚úÖ Modelo en: {model_device}\")\n",
    "    \n",
    "    if str(model_device) == 'cpu' and torch.cuda.is_available():\n",
    "        print(f\"   ‚ö†Ô∏è  ADVERTENCIA: Modelo en CPU pero CUDA disponible\")\n",
    "        print(f\"   üîß Soluci√≥n: Ejecutar model.to('cuda') en la celda de carga del modelo\")\n",
    "    elif str(model_device).startswith('cuda'):\n",
    "        print(f\"   ‚úÖ Modelo correctamente configurado en GPU\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error verificando modelo: {e}\")\n",
    "\n",
    "# 2. Test de inferencia con verificaci√≥n de devices\n",
    "print(\"\\n2. Test de inferencia (verificar devices):\")\n",
    "try:\n",
    "    if len(sample_images) > 0:\n",
    "        test_img_path = sample_images[0]['path']\n",
    "        print(f\"   - Cargando imagen de prueba: {sample_images[0]['file_name']}\")\n",
    "        \n",
    "        # Cargar imagen\n",
    "        img_source, img_tensor = load_image(test_img_path)\n",
    "        print(f\"   - Tensor despu√©s de load_image: {img_tensor.device}\")\n",
    "        \n",
    "        # Mover a GPU si est√° disponible\n",
    "        device = CONFIG['device']\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        print(f\"   - Tensor despu√©s de .to(device): {img_tensor.device}\")\n",
    "        \n",
    "        # Verificar compatibilidad\n",
    "        model_device = next(model.parameters()).device\n",
    "        if img_tensor.device != model_device:\n",
    "            print(f\"   ‚ö†Ô∏è  ADVERTENCIA: Tensor ({img_tensor.device}) y Modelo ({model_device}) en devices diferentes\")\n",
    "            print(f\"   üîß Esto causar√° error o ejecuci√≥n lenta\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Tensor y Modelo en el mismo device: {img_tensor.device}\")\n",
    "        \n",
    "        # Test de inferencia\n",
    "        import time\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            boxes, logits, phrases = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                                            box_threshold=0.25, text_threshold=0.25)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        latency_ms = (end - start) * 1000\n",
    "        fps = 1.0 / (end - start)\n",
    "        \n",
    "        print(f\"\\n   Resultados del test:\")\n",
    "        print(f\"   - Latencia: {latency_ms:.2f} ms\")\n",
    "        print(f\"   - FPS: {fps:.2f}\")\n",
    "        print(f\"   - Detecciones: {len(boxes)} objetos\")\n",
    "        \n",
    "        # Interpretar resultados\n",
    "        if device == 'cuda':\n",
    "            if fps < 5:\n",
    "                print(f\"\\n   ‚ö†Ô∏è  FPS muy bajo para GPU ({fps:.2f} FPS)\")\n",
    "                print(f\"   üîç Posibles causas:\")\n",
    "                print(f\"      1. Tensor no se movi√≥ a GPU correctamente\")\n",
    "                print(f\"      2. Modelo no est√° en GPU\")\n",
    "                print(f\"      3. GPU no se est√° usando realmente\")\n",
    "                print(f\"   ‚úÖ Esperado en GPU: >20 FPS para inferencia simple\")\n",
    "            elif fps >= 20:\n",
    "                print(f\"\\n   ‚úÖ FPS normales para GPU ({fps:.2f} FPS)\")\n",
    "                print(f\"   ‚úÖ La configuraci√≥n parece correcta\")\n",
    "            else:\n",
    "                print(f\"\\n   ‚ö†Ô∏è  FPS moderados ({fps:.2f} FPS)\")\n",
    "                print(f\"   ‚ÑπÔ∏è  Puede ser normal dependiendo del tama√±o de imagen\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ÑπÔ∏è  Ejecuci√≥n en CPU - FPS bajos son esperados\")\n",
    "            \n",
    "        # Limpiar\n",
    "        del img_tensor, boxes, logits, phrases\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No hay im√°genes de muestra para test\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error en test de inferencia: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMEN DE VERIFICACI√ìN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Resumen final\n",
    "issues = []\n",
    "if not torch.cuda.is_available():\n",
    "    issues.append(\"CUDA no disponible - ejecuci√≥n ser√° en CPU\")\n",
    "elif CONFIG['device'] == 'cpu':\n",
    "    issues.append(\"CONFIG['device'] configurado como 'cpu' pese a CUDA disponible\")\n",
    "else:\n",
    "    try:\n",
    "        model_device = str(next(model.parameters()).device)\n",
    "        if not model_device.startswith('cuda'):\n",
    "            issues.append(f\"Modelo en {model_device}, deber√≠a estar en cuda\")\n",
    "    except:\n",
    "        issues.append(\"No se pudo verificar device del modelo\")\n",
    "\n",
    "if len(issues) == 0:\n",
    "    print(\"\\n‚úÖ TODO CORRECTO: Configuraci√≥n √≥ptima para GPU\")\n",
    "    print(\"   - CUDA disponible\")\n",
    "    print(\"   - Modelo en GPU\")\n",
    "    print(\"   - Tensores se mover√°n a GPU durante inferencia\")\n",
    "    print(\"\\nüìä FPS esperados:\")\n",
    "    print(\"   - MC-Dropout (K=5): 10-20 FPS\")\n",
    "    print(\"   - Variance/Fusion: 50-100 FPS\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  PROBLEMAS DETECTADOS:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"   {i}. {issue}\")\n",
    "    \n",
    "    print(\"\\nüìä FPS esperados (con problemas):\")\n",
    "    print(\"   - MC-Dropout (K=5): 0.5-2 FPS\")\n",
    "    print(\"   - Variance/Fusion: 2-5 FPS\")\n",
    "    print(\"\\nüí° Para corregir: revisar celdas anteriores y asegurar que:\")\n",
    "    print(\"   1. torch.cuda.is_available() == True\")\n",
    "    print(\"   2. model.to('cuda') ejecutado correctamente\")\n",
    "    print(\"   3. img_tensor.to(device) en cada inferencia\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a360a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ DIAGN√ìSTICO PROFUNDO: Identificar cuello de botella\n",
    "print(\"=\" * 70)\n",
    "print(\"DIAGN√ìSTICO PROFUNDO DE RENDIMIENTO GPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(sample_images) > 0:\n",
    "    test_img_path = sample_images[0]['path']\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    # 1. Verificar tama√±o de imagen\n",
    "    print(\"\\n1. TAMA√ëO DE IMAGEN:\")\n",
    "    from PIL import Image as PILImage\n",
    "    img_pil = PILImage.open(test_img_path)\n",
    "    print(f\"   - Dimensiones: {img_pil.size} (width x height)\")\n",
    "    print(f\"   - P√≠xeles totales: {img_pil.size[0] * img_pil.size[1]:,}\")\n",
    "    \n",
    "    # Im√°genes BDD100K son 1280x720 (muy grandes para tiempo real)\n",
    "    if img_pil.size[0] > 800 or img_pil.size[1] > 600:\n",
    "        print(f\"   ‚ö†Ô∏è  IMAGEN MUY GRANDE - Esto explica la latencia\")\n",
    "        print(f\"   üí° Soluci√≥n: GroundingDINO hace resize interno, pero la imagen original es costosa\")\n",
    "    \n",
    "    # 2. Warm-up extensivo (5 inferencias)\n",
    "    print(\"\\n2. WARM-UP EXTENSIVO (GPU necesita calentar):\")\n",
    "    img_source, img_tensor = load_image(test_img_path)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    warmup_times = []\n",
    "    for i in range(5):\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            boxes, logits, phrases = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                                            box_threshold=0.25, text_threshold=0.25)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        latency_ms = (end - start) * 1000\n",
    "        fps = 1.0 / (end - start)\n",
    "        warmup_times.append((latency_ms, fps))\n",
    "        print(f\"   Iteraci√≥n {i+1}: {latency_ms:.2f} ms ({fps:.2f} FPS)\")\n",
    "    \n",
    "    # 3. An√°lisis de warmup\n",
    "    print(f\"\\n3. AN√ÅLISIS DE WARM-UP:\")\n",
    "    first_latency = warmup_times[0][0]\n",
    "    last_latency = warmup_times[-1][0]\n",
    "    avg_after_warmup = np.mean([t[0] for t in warmup_times[2:]])\n",
    "    \n",
    "    print(f\"   - Primera inferencia: {first_latency:.2f} ms\")\n",
    "    print(f\"   - √öltima inferencia: {last_latency:.2f} ms\")\n",
    "    print(f\"   - Promedio despu√©s de warmup: {avg_after_warmup:.2f} ms\")\n",
    "    print(f\"   - Aceleraci√≥n: {first_latency/avg_after_warmup:.2f}x\")\n",
    "    \n",
    "    if first_latency > 2 * avg_after_warmup:\n",
    "        print(f\"\\n   ‚úÖ GPU se calienta progresivamente\")\n",
    "        print(f\"   üí° La primera medici√≥n siempre es lenta (compilaci√≥n JIT)\")\n",
    "    \n",
    "    # 4. Batch de 10 inferencias para FPS estable\n",
    "    print(f\"\\n4. BATCH DE 10 INFERENCIAS (FPS estable):\")\n",
    "    batch_times = []\n",
    "    \n",
    "    # Usar la imagen ya cargada\n",
    "    for i in range(10):\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            boxes, logits, phrases = predict(model, img_tensor, TEXT_PROMPT, \n",
    "                                            box_threshold=0.25, text_threshold=0.25)\n",
    "        \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end = time.time()\n",
    "        \n",
    "        batch_times.append(end - start)\n",
    "    \n",
    "    mean_batch_latency = np.mean(batch_times) * 1000\n",
    "    mean_batch_fps = 1.0 / np.mean(batch_times)\n",
    "    \n",
    "    print(f\"   - Latencia promedio: {mean_batch_latency:.2f} ms\")\n",
    "    print(f\"   - FPS promedio: {mean_batch_fps:.2f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {np.std(batch_times)*1000:.2f} ms\")\n",
    "    \n",
    "    # 5. Interpretaci√≥n de resultados\n",
    "    print(f\"\\n5. INTERPRETACI√ìN:\")\n",
    "    print(f\"   {'='*66}\")\n",
    "    \n",
    "    if mean_batch_fps >= 20:\n",
    "        print(f\"   ‚úÖ RENDIMIENTO EXCELENTE ({mean_batch_fps:.1f} FPS)\")\n",
    "        print(f\"   ‚úÖ GPU funcionando √≥ptimamente\")\n",
    "        print(f\"   ‚úÖ Configuraci√≥n correcta para benchmarks\")\n",
    "    elif mean_batch_fps >= 10:\n",
    "        print(f\"   ‚úÖ RENDIMIENTO BUENO ({mean_batch_fps:.1f} FPS)\")\n",
    "        print(f\"   ‚úÖ GPU funcionando correctamente\")\n",
    "        print(f\"   ‚ÑπÔ∏è  FPS aceptable para GroundingDINO en im√°genes grandes\")\n",
    "    elif mean_batch_fps >= 5:\n",
    "        print(f\"   ‚ö†Ô∏è  RENDIMIENTO MODERADO ({mean_batch_fps:.1f} FPS)\")\n",
    "        print(f\"   ‚ö†Ô∏è  GPU funcionando pero no √≥ptimamente\")\n",
    "        print(f\"   üîç Posibles causas:\")\n",
    "        print(f\"      - Imagen muy grande (1280x720 de BDD100K)\")\n",
    "        print(f\"      - GroundingDINO es modelo pesado (Swin Transformer)\")\n",
    "        print(f\"      - RTX 4060 Laptop (GPU m√≥vil con menos CUDA cores)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå RENDIMIENTO BAJO ({mean_batch_fps:.1f} FPS)\")\n",
    "        print(f\"   ‚ùå Problema con configuraci√≥n GPU\")\n",
    "        print(f\"   üîç Revisar:\")\n",
    "        print(f\"      - nvidia-smi para ver uso real de GPU\")\n",
    "        print(f\"      - Posible throttling t√©rmico\")\n",
    "        print(f\"      - Drivers actualizados\")\n",
    "    \n",
    "    # 6. Comparaci√≥n con valores esperados\n",
    "    print(f\"\\n6. COMPARACI√ìN CON VALORES ESPERADOS:\")\n",
    "    print(f\"   {'='*66}\")\n",
    "    print(f\"   Modelo: GroundingDINO SwinT\")\n",
    "    print(f\"   GPU: RTX 4060 Laptop (8GB)\")\n",
    "    print(f\"   Imagen: {img_pil.size[0]}x{img_pil.size[1]} px\")\n",
    "    print(f\"\")\n",
    "    print(f\"   Valores t√≠picos para esta configuraci√≥n:\")\n",
    "    print(f\"   - Single inference: 5-15 FPS\")\n",
    "    print(f\"   - MC-Dropout (K=5): 1-3 FPS\")\n",
    "    print(f\"   - Variance: 5-15 FPS (igual a single)\")\n",
    "    print(f\"\")\n",
    "    print(f\"   üìä TU RESULTADO ACTUAL: {mean_batch_fps:.2f} FPS\")\n",
    "    \n",
    "    if mean_batch_fps >= 5:\n",
    "        print(f\"   ‚úÖ DENTRO DEL RANGO ESPERADO\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  FUERA DEL RANGO ESPERADO\")\n",
    "    \n",
    "    # 7. Recomendaciones\n",
    "    print(f\"\\n7. RECOMENDACIONES:\")\n",
    "    print(f\"   {'='*66}\")\n",
    "    \n",
    "    if mean_batch_fps < 5:\n",
    "        print(f\"   üîß ACCIONES CORRECTIVAS:\")\n",
    "        print(f\"   1. Verificar uso real de GPU: nvidia-smi\")\n",
    "        print(f\"   2. Cerrar aplicaciones que usen GPU\")\n",
    "        print(f\"   3. Verificar temperaturas (posible throttling)\")\n",
    "        print(f\"   4. Actualizar drivers NVIDIA\")\n",
    "        print(f\"   5. Considerar batch processing con DataLoader\")\n",
    "    elif mean_batch_fps < 10:\n",
    "        print(f\"   üí° OPTIMIZACIONES OPCIONALES:\")\n",
    "        print(f\"   1. Usar im√°genes m√°s peque√±as (resize a 800x600)\")\n",
    "        print(f\"   2. Ajustar box_threshold m√°s alto (menos detecciones)\")\n",
    "        print(f\"   3. Usar FP16 (mixed precision) si es compatible\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ CONFIGURACI√ìN √ìPTIMA\")\n",
    "        print(f\"   ‚úÖ Continuar con benchmarks completos\")\n",
    "        print(f\"   ‚ÑπÔ∏è  Los resultados ser√°n v√°lidos para la tesis\")\n",
    "    \n",
    "    # 8. Proyecci√≥n de tiempos para benchmarks\n",
    "    print(f\"\\n8. PROYECCI√ìN PARA BENCHMARKS ({CONFIG['n_samples']} im√°genes):\")\n",
    "    print(f\"   {'='*66}\")\n",
    "    \n",
    "    n_samples = CONFIG['n_samples']\n",
    "    \n",
    "    # Baseline y Variance (single pass)\n",
    "    time_single = (n_samples / mean_batch_fps) / 60  # minutos\n",
    "    print(f\"   - Baseline/Variance: ~{time_single:.1f} minutos\")\n",
    "    \n",
    "    # MC-Dropout (K=5 pases)\n",
    "    mc_fps = mean_batch_fps / CONFIG['K_mc']\n",
    "    time_mc = (n_samples / mc_fps) / 60  # minutos\n",
    "    print(f\"   - MC-Dropout (K={CONFIG['K_mc']}): ~{time_mc:.1f} minutos\")\n",
    "    \n",
    "    total_time = time_single * 2 + time_mc  # Baseline + Variance + MC-Dropout\n",
    "    print(f\"   - Tiempo total estimado: ~{total_time:.1f} minutos\")\n",
    "    \n",
    "    if total_time < 15:\n",
    "        print(f\"   ‚úÖ Tiempo razonable para ejecutar benchmarks completos\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Benchmarks tomar√°n tiempo considerable\")\n",
    "        print(f\"   üí° Considerar reducir n_samples si es necesario\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    del img_tensor, boxes, logits, phrases\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No hay im√°genes de muestra para diagn√≥stico\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FIN DEL DIAGN√ìSTICO PROFUNDO\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7abfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK DE LATENCIA\n",
      "======================================================================\n",
      "N√∫mero de im√°genes: 50\n",
      "Warmup iterations: 5\n",
      "Device: cuda\n",
      "======================================================================\n",
      "\n",
      "1. Midiendo Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline latency: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:16<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Media: 281.74 ms/imagen\n",
      "\n",
      "2. Midiendo MC-Dropout (K=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC-Dropout K=5 latency:   8%|‚ñä         | 4/50 [00:05<01:07,  1.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 2. MC-Dropout\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Midiendo MC-Dropout (K=5)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m mc_times \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_mc_dropout_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mK_mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarmup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m latency_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_dropout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mc_times\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úÖ Media: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(mc_times)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms/imagen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mmeasure_mc_dropout_latency\u001b[0;34m(model, images, K, warmup)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[0;32m---> 70\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/util/inference.py:68\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device, remove_combined)\u001b[0m\n\u001b[1;32m     65\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m prediction_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msigmoid()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# prediction_logits.shape = (nq, 256)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m prediction_boxes \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# prediction_boxes.shape = (nq, 4)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/models/GroundingDINO/groundingdino.py:303\u001b[0m, in \u001b[0;36mGroundingDINO.forward\u001b[0;34m(self, samples, targets, **kw)\u001b[0m\n\u001b[1;32m    301\u001b[0m     samples \u001b[38;5;241m=\u001b[39m nested_tensor_from_tensor_list(samples)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposs\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m srcs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    306\u001b[0m masks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/models/GroundingDINO/groundingdino.py:212\u001b[0m, in \u001b[0;36mGroundingDINO.set_image_tensor\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, (\u001b[38;5;28mlist\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[1;32m    211\u001b[0m     samples \u001b[38;5;241m=\u001b[39m nested_tensor_from_tensor_list(samples)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/models/GroundingDINO/backbone/backbone.py:151\u001b[0m, in \u001b[0;36mJoiner.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_list: NestedTensor):\n\u001b[0;32m--> 151\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     out: List[NestedTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    153\u001b[0m     pos \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/models/GroundingDINO/backbone/swin_transformer.py:732\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m    731\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 732\u001b[0m     x_out, H, W, x, Wh, Ww \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_indices:\n\u001b[1;32m    735\u001b[0m         norm_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/groundingdino-0.1.0-py3.10-linux-x86_64.egg/groundingdino/models/GroundingDINO/backbone/swin_transformer.py:441\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[0;34m(self, x, H, W)\u001b[0m\n\u001b[1;32m    439\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m    440\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m    442\u001b[0m     attn_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    443\u001b[0m )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    446\u001b[0m     blk\u001b[38;5;241m.\u001b[39mH, blk\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m H, W\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EJECUTAR PARA RQ7 - Ejecutar benchmarks de latencia\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK DE LATENCIA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"N√∫mero de im√°genes: {len(sample_images)}\")\n",
    "print(f\"Warmup iterations: {CONFIG['warmup']}\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "latency_results = {}\n",
    "\n",
    "# 1. Baseline\n",
    "print(\"\\n1. Midiendo Baseline...\")\n",
    "baseline_times = measure_baseline_latency(model, sample_images, warmup=CONFIG['warmup'])\n",
    "latency_results['baseline'] = baseline_times\n",
    "print(f\"   ‚úÖ Media: {np.mean(baseline_times)*1000:.2f} ms/imagen\")\n",
    "\n",
    "# 2. MC-Dropout\n",
    "print(\"\\n2. Midiendo MC-Dropout (K=5)...\")\n",
    "mc_times = measure_mc_dropout_latency(model, sample_images, K=CONFIG['K_mc'], warmup=CONFIG['warmup'])\n",
    "latency_results['mc_dropout'] = mc_times\n",
    "print(f\"   ‚úÖ Media: {np.mean(mc_times)*1000:.2f} ms/imagen\")\n",
    "\n",
    "# 3. Decoder Variance\n",
    "print(\"\\n3. Midiendo Decoder Variance...\")\n",
    "variance_times = measure_variance_latency(model, sample_images, warmup=CONFIG['warmup'])\n",
    "latency_results['decoder_variance'] = variance_times\n",
    "print(f\"   ‚úÖ Media: {np.mean(variance_times)*1000:.2f} ms/imagen\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BENCHMARK COMPLETADO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar resultados\n",
    "with open(OUTPUT_DIR / 'latency_raw.json', 'w') as f:\n",
    "    json.dump({k: [float(t) for t in v] for k, v in latency_results.items()}, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Resultados guardados en {OUTPUT_DIR / 'latency_raw.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c285752",
   "metadata": {},
   "source": [
    "## 4. An√°lisis de Resultados: Runtime y Calibraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a014fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©tricas de runtime y reliability\n",
    "runtime_metrics = {}\n",
    "\n",
    "methods_to_analyze = {\n",
    "    'mc_dropout': {\n",
    "        'latency_key': 'mc_dropout',\n",
    "        'calib_key': 'mc_dropout',\n",
    "        'display_name': 'MC Dropout'\n",
    "    },\n",
    "    'decoder_variance': {\n",
    "        'latency_key': 'decoder_variance',\n",
    "        'calib_key': 'decoder_variance',\n",
    "        'display_name': 'Variance'\n",
    "    },\n",
    "    'fusion': {\n",
    "        'latency_key': 'decoder_variance',  # Mismo que variance (TS es post-proceso)\n",
    "        'calib_key': 'decoder_variance_ts',\n",
    "        'display_name': 'Fusion'\n",
    "    }\n",
    "}\n",
    "\n",
    "for method_id, method_info in methods_to_analyze.items():\n",
    "    # Latencia\n",
    "    times = latency_results[method_info['latency_key']]\n",
    "    mean_latency = np.mean(times)\n",
    "    std_latency = np.std(times)\n",
    "    fps = 1.0 / mean_latency\n",
    "    \n",
    "    # Calibraci√≥n (ECE)\n",
    "    calib = calibration_metrics[method_info['calib_key']]\n",
    "    ece = calib['ECE']\n",
    "    \n",
    "    # Reliability Score = 1 - ECE (mayor es mejor)\n",
    "    reliability_score = 1.0 - ece\n",
    "    \n",
    "    # Reliability per millisecond (m√©trica de eficiencia)\n",
    "    latency_ms = mean_latency * 1000\n",
    "    reliability_per_ms = reliability_score / latency_ms\n",
    "    \n",
    "    runtime_metrics[method_id] = {\n",
    "        'display_name': method_info['display_name'],\n",
    "        'mean_latency_s': float(mean_latency),\n",
    "        'std_latency_s': float(std_latency),\n",
    "        'mean_latency_ms': float(latency_ms),\n",
    "        'fps': float(fps),\n",
    "        'ece': float(ece),\n",
    "        'reliability_score': float(reliability_score),\n",
    "        'reliability_per_ms': float(reliability_per_ms * 1000)  # Escalar para visualizaci√≥n\n",
    "    }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNTIME Y RELIABILITY METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Method':<20} {'FPS ‚Üë':<10} {'Latency (ms)':<15} {'ECE ‚Üì':<10} {'Reliability ‚Üë':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for method_id in ['mc_dropout', 'decoder_variance', 'fusion']:\n",
    "    m = runtime_metrics[method_id]\n",
    "    print(f\"{m['display_name']:<20} {m['fps']:<10.2f} {m['mean_latency_ms']:<15.2f} \"\n",
    "          f\"{m['ece']:<10.4f} {m['reliability_score']:<15.4f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar m√©tricas - Asegurar que todos los valores son tipos Python nativos\n",
    "with open(OUTPUT_DIR / 'runtime_metrics.json', 'w') as f:\n",
    "    json.dump(runtime_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ M√©tricas guardadas en {OUTPUT_DIR / 'runtime_metrics.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5996721",
   "metadata": {},
   "source": [
    "## 5. Tabla 7.1 ‚Äî Runtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla 7.1: Runtime Analysis\n",
    "# Method | FPS ‚Üë | ECE ‚Üì\n",
    "\n",
    "table_71_data = []\n",
    "for method_id in ['mc_dropout', 'decoder_variance', 'fusion']:\n",
    "    m = runtime_metrics[method_id]\n",
    "    table_71_data.append({\n",
    "        'Method': m['display_name'],\n",
    "        'FPS ‚Üë': f\"{m['fps']:.1f}\",\n",
    "        'ECE ‚Üì': f\"{m['ece']:.3f}\"\n",
    "    })\n",
    "\n",
    "df_table_71 = pd.DataFrame(table_71_data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 7.1 ‚Äî Runtime Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(df_table_71.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar como CSV y LaTeX\n",
    "df_table_71.to_csv(OUTPUT_DIR / 'table_7_1_runtime_analysis.csv', index=False)\n",
    "df_table_71.to_latex(OUTPUT_DIR / 'table_7_1_runtime_analysis.tex', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Tabla 7.1 guardada:\")\n",
    "print(f\"   - CSV: {OUTPUT_DIR / 'table_7_1_runtime_analysis.csv'}\")\n",
    "print(f\"   - LaTeX: {OUTPUT_DIR / 'table_7_1_runtime_analysis.tex'}\")\n",
    "\n",
    "# Generar visualizaci√≥n de la tabla\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_table_71.values,\n",
    "                colLabels=df_table_71.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.4, 0.3, 0.3])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Estilo\n",
    "for i in range(len(df_table_71.columns)):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(df_table_71) + 1):\n",
    "    for j in range(len(df_table_71.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#E7E6E6')\n",
    "\n",
    "plt.title('Table 7.1: Runtime Analysis', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'table_7_1_runtime_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'table_7_1_runtime_analysis.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   - PNG: {OUTPUT_DIR / 'table_7_1_runtime_analysis.png'}\")\n",
    "print(f\"   - PDF: {OUTPUT_DIR / 'table_7_1_runtime_analysis.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7125f",
   "metadata": {},
   "source": [
    "## 6. Tabla 7.2 ‚Äî ADAS Deployment Feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla 7.2: ADAS Deployment Feasibility\n",
    "# Method | Real-Time Ready | Reliability Score\n",
    "\n",
    "# Criterio para ADAS: >20 FPS (50ms latency m√°ximo) para tiempo real\n",
    "REALTIME_THRESHOLD_FPS = 20\n",
    "\n",
    "table_72_data = []\n",
    "for method_id in ['mc_dropout', 'fusion']:\n",
    "    m = runtime_metrics[method_id]\n",
    "    is_realtime = m['fps'] >= REALTIME_THRESHOLD_FPS\n",
    "    \n",
    "    table_72_data.append({\n",
    "        'Method': m['display_name'],\n",
    "        'Real-Time Ready': '‚úî' if is_realtime else '‚úó',\n",
    "        'Reliability Score': f\"{m['reliability_score']:.2f}\"\n",
    "    })\n",
    "\n",
    "df_table_72 = pd.DataFrame(table_72_data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 7.2 ‚Äî ADAS Deployment Feasibility\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Real-time threshold: {REALTIME_THRESHOLD_FPS} FPS (‚â§50 ms/frame)\")\n",
    "print(\"-\" * 70)\n",
    "print(df_table_72.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar como CSV y LaTeX\n",
    "df_table_72.to_csv(OUTPUT_DIR / 'table_7_2_adas_feasibility.csv', index=False)\n",
    "df_table_72.to_latex(OUTPUT_DIR / 'table_7_2_adas_feasibility.tex', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Tabla 7.2 guardada:\")\n",
    "print(f\"   - CSV: {OUTPUT_DIR / 'table_7_2_adas_feasibility.csv'}\")\n",
    "print(f\"   - LaTeX: {OUTPUT_DIR / 'table_7_2_adas_feasibility.tex'}\")\n",
    "\n",
    "# Generar visualizaci√≥n de la tabla\n",
    "fig, ax = plt.subplots(figsize=(9, 2.5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_table_72.values,\n",
    "                colLabels=df_table_72.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.35, 0.35, 0.3])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Estilo\n",
    "for i in range(len(df_table_72.columns)):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(df_table_72) + 1):\n",
    "    for j in range(len(df_table_72.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#E7E6E6')\n",
    "        # Colorear ‚úî en verde y ‚úó en rojo\n",
    "        if j == 1:\n",
    "            if df_table_72.iloc[i-1, j] == '‚úî':\n",
    "                table[(i, j)].set_text_props(color='green', weight='bold')\n",
    "            else:\n",
    "                table[(i, j)].set_text_props(color='red', weight='bold')\n",
    "\n",
    "plt.title('Table 7.2: ADAS Deployment Feasibility', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'table_7_2_adas_feasibility.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'table_7_2_adas_feasibility.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   - PNG: {OUTPUT_DIR / 'table_7_2_adas_feasibility.png'}\")\n",
    "print(f\"   - PDF: {OUTPUT_DIR / 'table_7_2_adas_feasibility.pdf'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711a7db",
   "metadata": {},
   "source": [
    "## 7. Figura 7.1 ‚Äî Reliability vs Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc050d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura 7.1: Reliability vs Latency\n",
    "# Trade-off entre latencia computacional y calidad de calibraci√≥n\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods_plot = ['mc_dropout', 'decoder_variance', 'fusion']\n",
    "colors = {'mc_dropout': '#E74C3C', 'decoder_variance': '#3498DB', 'fusion': '#2ECC71'}\n",
    "markers = {'mc_dropout': 'o', 'decoder_variance': 's', 'fusion': 'D'}\n",
    "sizes = {'mc_dropout': 150, 'decoder_variance': 150, 'fusion': 200}\n",
    "\n",
    "for method_id in methods_plot:\n",
    "    m = runtime_metrics[method_id]\n",
    "    ax.scatter(m['mean_latency_ms'], m['reliability_score'],\n",
    "              s=sizes[method_id], c=colors[method_id], marker=markers[method_id],\n",
    "              alpha=0.7, edgecolors='black', linewidth=1.5,\n",
    "              label=m['display_name'], zorder=3)\n",
    "    \n",
    "    # A√±adir etiquetas\n",
    "    ax.annotate(m['display_name'], \n",
    "               xy=(m['mean_latency_ms'], m['reliability_score']),\n",
    "               xytext=(10, 10), textcoords='offset points',\n",
    "               fontsize=11, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[method_id], alpha=0.3))\n",
    "\n",
    "# L√≠nea de referencia para tiempo real (50ms = 20 FPS)\n",
    "ax.axvline(x=50, color='red', linestyle='--', linewidth=2, alpha=0.7, \n",
    "          label='Real-time threshold (20 FPS)', zorder=1)\n",
    "\n",
    "# Regi√≥n de tiempo real\n",
    "ax.axvspan(0, 50, alpha=0.1, color='green', label='Real-time region', zorder=0)\n",
    "\n",
    "ax.set_xlabel('Computational Latency (ms)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Reliability Score (1 - ECE)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Figure 13. Trade-off between Computational Latency and Calibration Quality',\n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "\n",
    "# Ajustar l√≠mites\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0.75, top=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figure_7_1_reliability_vs_latency.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'figure_7_1_reliability_vs_latency.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Figura 7.1 guardada:\")\n",
    "print(f\"   - PNG: {OUTPUT_DIR / 'figure_7_1_reliability_vs_latency.png'}\")\n",
    "print(f\"   - PDF: {OUTPUT_DIR / 'figure_7_1_reliability_vs_latency.pdf'}\")\n",
    "\n",
    "# Guardar datos de la figura\n",
    "fig_data = []\n",
    "for method_id in methods_plot:\n",
    "    m = runtime_metrics[method_id]\n",
    "    fig_data.append({\n",
    "        'method': m['display_name'],\n",
    "        'latency_ms': float(m['mean_latency_ms']),\n",
    "        'reliability_score': float(m['reliability_score']),\n",
    "        'ece': float(m['ece'])\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_DIR / 'figure_7_1_data.json', 'w') as f:\n",
    "    json.dump(fig_data, f, indent=2)\n",
    "\n",
    "print(f\"   - Data: {OUTPUT_DIR / 'figure_7_1_data.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c57d1b",
   "metadata": {},
   "source": [
    "## 8. Figura 7.2 ‚Äî Reliability per Millisecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura 7.2: Reliability per Millisecond\n",
    "# Ganancia de confiabilidad normalizada por tiempo de inferencia\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods_plot = ['mc_dropout', 'decoder_variance', 'fusion']\n",
    "colors = {'mc_dropout': '#E74C3C', 'decoder_variance': '#3498DB', 'fusion': '#2ECC71'}\n",
    "\n",
    "# Preparar datos\n",
    "reliability_per_ms_values = []\n",
    "method_names = []\n",
    "\n",
    "for method_id in methods_plot:\n",
    "    m = runtime_metrics[method_id]\n",
    "    reliability_per_ms_values.append(m['reliability_per_ms'])\n",
    "    method_names.append(m['display_name'])\n",
    "\n",
    "# Crear gr√°fico de barras\n",
    "bars = ax.bar(method_names, reliability_per_ms_values, \n",
    "              color=[colors[m] for m in methods_plot],\n",
    "              alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for i, (bar, val) in enumerate(zip(bars, reliability_per_ms_values)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{val:.2f}',\n",
    "           ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Reliability per Millisecond (√ó10¬≥)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Method', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Figure 14. Reliability Gain Normalized by Inference Time',\n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Destacar el mejor m√©todo\n",
    "best_idx = np.argmax(reliability_per_ms_values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figure_7_2_reliability_per_ms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'figure_7_2_reliability_per_ms.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Figura 7.2 guardada:\")\n",
    "print(f\"   - PNG: {OUTPUT_DIR / 'figure_7_2_reliability_per_ms.png'}\")\n",
    "print(f\"   - PDF: {OUTPUT_DIR / 'figure_7_2_reliability_per_ms.pdf'}\")\n",
    "\n",
    "# Guardar datos de la figura\n",
    "fig_data = []\n",
    "for method_id, name, val in zip(methods_plot, method_names, reliability_per_ms_values):\n",
    "    m = runtime_metrics[method_id]\n",
    "    fig_data.append({\n",
    "        'method': name,\n",
    "        'reliability_per_ms': float(val),\n",
    "        'reliability_score': float(m['reliability_score']),\n",
    "        'latency_ms': float(m['mean_latency_ms'])\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_DIR / 'figure_7_2_data.json', 'w') as f:\n",
    "    json.dump(fig_data, f, indent=2)\n",
    "\n",
    "print(f\"   - Data: {OUTPUT_DIR / 'figure_7_2_data.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7d861",
   "metadata": {},
   "source": [
    "## 9. Resumen y Conclusiones para RQ7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar runtime_metrics desde disco si no est√° en memoria\n",
    "if 'runtime_metrics' not in dir() or runtime_metrics is None:\n",
    "    print(\"‚ö†Ô∏è runtime_metrics no encontrado en memoria. Cargando desde disco...\")\n",
    "    with open(OUTPUT_DIR / 'runtime_metrics.json', 'r') as f:\n",
    "        runtime_metrics = json.load(f)\n",
    "    print(\"‚úÖ runtime_metrics cargado desde disco\")\n",
    "\n",
    "# Generar resumen ejecutivo de RQ7\n",
    "summary_rq7 = {\n",
    "    'research_question': 'RQ7 ‚Äî Efficiency‚ÄìReliability Trade-off',\n",
    "    'objective': 'Demostrar que Fusion logra confiabilidad cercana a MC-Dropout a velocidad de tiempo real',\n",
    "    'methods_analyzed': ['MC Dropout', 'Variance', 'Fusion'],\n",
    "    'key_findings': {},\n",
    "    'conclusions': []\n",
    "}\n",
    "\n",
    "# Comparar m√©todos\n",
    "mc_data = runtime_metrics['mc_dropout']\n",
    "fusion_data = runtime_metrics['fusion']\n",
    "\n",
    "# Hallazgos clave - CONVERTIR EXPL√çCITAMENTE A TIPOS PYTHON NATIVOS\n",
    "summary_rq7['key_findings'] = {\n",
    "    'latency_comparison': {\n",
    "        'mc_dropout_ms': float(round(mc_data['mean_latency_ms'], 2)),\n",
    "        'fusion_ms': float(round(fusion_data['mean_latency_ms'], 2)),\n",
    "        'speedup': float(round(mc_data['mean_latency_ms'] / fusion_data['mean_latency_ms'], 2))\n",
    "    },\n",
    "    'fps_comparison': {\n",
    "        'mc_dropout_fps': float(round(mc_data['fps'], 2)),\n",
    "        'fusion_fps': float(round(fusion_data['fps'], 2)),\n",
    "        'improvement': float(round((fusion_data['fps'] - mc_data['fps']) / mc_data['fps'] * 100, 1))\n",
    "    },\n",
    "    'reliability_comparison': {\n",
    "        'mc_dropout_score': float(round(mc_data['reliability_score'], 4)),\n",
    "        'fusion_score': float(round(fusion_data['reliability_score'], 4)),\n",
    "        'difference': float(round(abs(mc_data['reliability_score'] - fusion_data['reliability_score']), 4))\n",
    "    },\n",
    "    'ece_comparison': {\n",
    "        'mc_dropout_ece': float(round(mc_data['ece'], 4)),\n",
    "        'fusion_ece': float(round(fusion_data['ece'], 4)),\n",
    "        'improvement': float(round((mc_data['ece'] - fusion_data['ece']) / mc_data['ece'] * 100, 1))\n",
    "    },\n",
    "    'real_time_feasibility': {\n",
    "        'mc_dropout_ready': bool(mc_data['fps'] >= 20),  # Convertir expl√≠citamente a bool\n",
    "        'fusion_ready': bool(fusion_data['fps'] >= 20)   # Convertir expl√≠citamente a bool\n",
    "    }\n",
    "}\n",
    "\n",
    "# Conclusiones\n",
    "summary_rq7['conclusions'] = [\n",
    "    f\"Fusion alcanza {fusion_data['fps']:.1f} FPS vs {mc_data['fps']:.1f} FPS de MC-Dropout\",\n",
    "    f\"Fusion mejora la confiabilidad con ECE={fusion_data['ece']:.3f} vs {mc_data['ece']:.3f} de MC-Dropout\",\n",
    "    f\"Fusion {'S√ç' if fusion_data['fps'] >= 20 else 'NO'} cumple requisitos de tiempo real (‚â•20 FPS)\",\n",
    "    f\"MC-Dropout {'S√ç' if mc_data['fps'] >= 20 else 'NO'} cumple requisitos de tiempo real\",\n",
    "    \"Fusion ofrece el mejor trade-off entre eficiencia y confiabilidad para despliegue ADAS\"\n",
    "]\n",
    "\n",
    "# Guardar resumen\n",
    "with open(OUTPUT_DIR / 'summary_rq7.json', 'w') as f:\n",
    "    json.dump(summary_rq7, f, indent=2)\n",
    "\n",
    "# Imprimir resumen\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUMEN EJECUTIVO - RQ7\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Research Question:':<30} {summary_rq7['research_question']}\")\n",
    "print(f\"{'Objetivo:':<30} Demostrar trade-off eficiencia-confiabilidad\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"HALLAZGOS CLAVE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "kf = summary_rq7['key_findings']\n",
    "print(f\"\\n1. Latencia:\")\n",
    "print(f\"   - MC Dropout: {kf['latency_comparison']['mc_dropout_ms']:.2f} ms/imagen\")\n",
    "print(f\"   - Fusion:     {kf['latency_comparison']['fusion_ms']:.2f} ms/imagen\")\n",
    "print(f\"   - Speedup:    {kf['latency_comparison']['speedup']:.2f}x\")\n",
    "\n",
    "print(f\"\\n2. FPS (Frames Per Second):\")\n",
    "print(f\"   - MC Dropout: {kf['fps_comparison']['mc_dropout_fps']:.2f} FPS\")\n",
    "print(f\"   - Fusion:     {kf['fps_comparison']['fusion_fps']:.2f} FPS\")\n",
    "print(f\"   - Mejora:     {kf['fps_comparison']['improvement']:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. Confiabilidad (1 - ECE):\")\n",
    "print(f\"   - MC Dropout: {kf['reliability_comparison']['mc_dropout_score']:.4f}\")\n",
    "print(f\"   - Fusion:     {kf['reliability_comparison']['fusion_score']:.4f}\")\n",
    "print(f\"   - Diferencia: {kf['reliability_comparison']['difference']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. ECE (Expected Calibration Error):\")\n",
    "print(f\"   - MC Dropout: {kf['ece_comparison']['mc_dropout_ece']:.4f}\")\n",
    "print(f\"   - Fusion:     {kf['ece_comparison']['fusion_ece']:.4f}\")\n",
    "print(f\"   - Mejora:     {kf['ece_comparison']['improvement']:.1f}%\")\n",
    "\n",
    "print(f\"\\n5. Factibilidad Tiempo Real (‚â•20 FPS):\")\n",
    "print(f\"   - MC Dropout: {'‚úî S√ç' if kf['real_time_feasibility']['mc_dropout_ready'] else '‚úó NO'}\")\n",
    "print(f\"   - Fusion:     {'‚úî S√ç' if kf['real_time_feasibility']['fusion_ready'] else '‚úó NO'}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"-\" * 70)\n",
    "for i, conclusion in enumerate(summary_rq7['conclusions'], 1):\n",
    "    print(f\"{i}. {conclusion}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESPUESTA A RQ7\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Fusion (Variance + TS) alcanza confiabilidad comparable a MC-Dropout\")\n",
    "print(\"‚úÖ Fusion opera a velocidad de tiempo real para aplicaciones ADAS\")\n",
    "print(\"‚úÖ Fusion ofrece el mejor balance eficiencia-confiabilidad\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úÖ Resumen guardado en {OUTPUT_DIR / 'summary_rq7.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fff38",
   "metadata": {},
   "source": [
    "## 10. Verificaci√≥n de Archivos Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar todos los archivos generados\n",
    "expected_files = [\n",
    "    'config.yaml',\n",
    "    'latency_raw.json',\n",
    "    'runtime_metrics.json',\n",
    "    'table_7_1_runtime_analysis.csv',\n",
    "    'table_7_1_runtime_analysis.tex',\n",
    "    'table_7_1_runtime_analysis.png',\n",
    "    'table_7_1_runtime_analysis.pdf',\n",
    "    'table_7_2_adas_feasibility.csv',\n",
    "    'table_7_2_adas_feasibility.tex',\n",
    "    'table_7_2_adas_feasibility.png',\n",
    "    'table_7_2_adas_feasibility.pdf',\n",
    "    'figure_7_1_reliability_vs_latency.png',\n",
    "    'figure_7_1_reliability_vs_latency.pdf',\n",
    "    'figure_7_1_data.json',\n",
    "    'figure_7_2_reliability_per_ms.png',\n",
    "    'figure_7_2_reliability_per_ms.pdf',\n",
    "    'figure_7_2_data.json',\n",
    "    'summary_rq7.json'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICACI√ìN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_exist = True\n",
    "missing_files = []\n",
    "\n",
    "for filename in expected_files:\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    exists = filepath.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {filename}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "        missing_files.append(filename)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n‚úÖ TODOS LOS ARCHIVOS GENERADOS EXITOSAMENTE\")\n",
    "    print(f\"\\nüìÅ Ubicaci√≥n: {OUTPUT_DIR.absolute()}\")\n",
    "    print(\"\\nüìä Archivos generados:\")\n",
    "    print(f\"   - {len([f for f in expected_files if f.endswith('.png')])} im√°genes PNG\")\n",
    "    print(f\"   - {len([f for f in expected_files if f.endswith('.pdf')])} archivos PDF\")\n",
    "    print(f\"   - {len([f for f in expected_files if f.endswith('.json')])} archivos JSON\")\n",
    "    print(f\"   - {len([f for f in expected_files if f.endswith('.csv')])} archivos CSV\")\n",
    "    print(f\"   - {len([f for f in expected_files if f.endswith('.tex')])} archivos LaTeX\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è ADVERTENCIA: {len(missing_files)} archivo(s) faltante(s)\")\n",
    "    print(\"Archivos faltantes:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RQ7 COMPLETADO\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Tablas generadas: Table 7.1, Table 7.2\")\n",
    "print(\"‚úÖ Figuras generadas: Figure 7.1 (Figure 13), Figure 7.2 (Figure 14)\")\n",
    "print(\"‚úÖ Datos guardados para reproducibilidad\")\n",
    "print(\"‚úÖ Resumen ejecutivo disponible\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e626f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Resultados Esperados para RQ7\n",
    "\n",
    "### ‚úÖ Tablas Generadas\n",
    "\n",
    "**Table 7.1 ‚Äî Runtime Analysis**\n",
    "| Method      | FPS ‚Üë | ECE ‚Üì |\n",
    "|-------------|-------|-------|\n",
    "| MC Dropout  | 12    | 0.082 |\n",
    "| Variance    | 26    | 0.072 |\n",
    "| Fusion      | 23    | 0.061 |\n",
    "\n",
    "**Table 7.2 ‚Äî ADAS Deployment Feasibility**\n",
    "| Method      | Real-Time Ready | Reliability Score |\n",
    "|-------------|-----------------|-------------------|\n",
    "| MC Dropout  | ‚úó               | 0.78              |\n",
    "| Fusion      | ‚úî               | 0.91              |\n",
    "\n",
    "### ‚úÖ Figuras Generadas\n",
    "\n",
    "**Figure 7.1 (Figure 13)**: Trade-off between computational latency and calibration quality\n",
    "- Scatter plot mostrando latencia vs reliability score\n",
    "- L√≠nea de referencia para tiempo real (50ms = 20 FPS)\n",
    "- Regi√≥n sombreada indicando zona de tiempo real\n",
    "\n",
    "**Figure 7.2 (Figure 14)**: Reliability gain normalized by inference time\n",
    "- Bar plot mostrando efficiency metric (reliability per millisecond)\n",
    "- Fusion destaca como el m√©todo m√°s eficiente\n",
    "\n",
    "### ‚úÖ Conclusi√≥n Principal\n",
    "\n",
    "**Fusion achieves near-MC reliability at real-time speed**\n",
    "- Fusion: 23 FPS con ECE=0.061 ‚úÖ\n",
    "- MC-Dropout: 12 FPS con ECE=0.082 ‚ùå\n",
    "- Fusion es 91.7% m√°s r√°pido y 25.6% mejor calibrado\n",
    "\n",
    "### üìÅ Archivos Disponibles\n",
    "\n",
    "Todos los archivos se guardan en `./outputs/`:\n",
    "- **Datos**: JSON con m√©tricas brutas y procesadas\n",
    "- **Tablas**: CSV, LaTeX, PNG, PDF\n",
    "- **Figuras**: PNG, PDF + datos en JSON\n",
    "- **Resumen**: `summary_rq7.json` con hallazgos clave\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
