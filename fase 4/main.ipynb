{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667d76eb",
   "metadata": {},
   "source": [
    "# Fase 4: Temperature Scaling para Calibración de Probabilidades\n",
    "\n",
    "**Objetivo**: Calibrar las probabilidades del modelo para que reflejen la confianza real (p=0.8 → 80% accuracy).\n",
    "\n",
    "**Pipeline**:\n",
    "1. Extraer predicciones del baseline en val_calib\n",
    "2. Ajustar temperatura T minimizando NLL\n",
    "3. Aplicar T en val_eval\n",
    "4. Evaluar mejora en calibración (ECE, Brier, NLL, Reliability Diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384a60c",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "OUTPUT_DIR = Path('./outputs/temperature_scaling')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'iou_matching': 0.5,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign']\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb7463",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d14b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "\n",
    "model_config = '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_weights = '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "print(f\"Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"Prompt: {TEXT_PROMPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5f03b",
   "metadata": {},
   "source": [
    "## 3. Inferencia en val_calib y Matching con GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd07ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Calcula IoU entre dos boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normaliza label del modelo a categoría canónica\"\"\"\n",
    "    synonyms = {'bike': 'bicycle', 'motorbike': 'motorcycle', 'pedestrian': 'person', \n",
    "                'stop sign': 'traffic sign', 'red light': 'traffic light'}\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "def run_inference_on_image(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"Ejecuta inferencia en una imagen\"\"\"\n",
    "    image_source, image = load_image(str(image_path))\n",
    "    boxes, logits, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    h, w = image_source.shape[:2]\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for box, logit, phrase in zip(boxes_xyxy.cpu().numpy(), logits.cpu().numpy(), phrases):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat in CONFIG['categories']:\n",
    "            detections.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': float(logit),\n",
    "                'logit': float(logit),\n",
    "                'category': cat\n",
    "            })\n",
    "    \n",
    "    boxes_t = torch.tensor([d['bbox'] for d in detections])\n",
    "    scores_t = torch.tensor([d['score'] for d in detections])\n",
    "    keep = torchvision.ops.nms(boxes_t, scores_t, 0.65)\n",
    "    \n",
    "    return [detections[i] for i in keep.numpy()]\n",
    "\n",
    "def match_predictions_to_gt(predictions, gt_anns, iou_thresh):\n",
    "    \"\"\"Asigna TP/FP a predicciones según matching con GT\"\"\"\n",
    "    gt_matched = set()\n",
    "    results = []\n",
    "    \n",
    "    cat_to_id = {cat: i+1 for i, cat in enumerate(CONFIG['categories'])}\n",
    "    id_to_cat = {v: k for k, v in cat_to_id.items()}\n",
    "    \n",
    "    for pred in predictions:\n",
    "        pred_cat = pred['category']\n",
    "        pred_box = pred['bbox']\n",
    "        \n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for idx, gt in enumerate(gt_anns):\n",
    "            if idx in gt_matched:\n",
    "                continue\n",
    "            \n",
    "            gt_cat_id = gt['category_id']\n",
    "            gt_cat = id_to_cat.get(gt_cat_id, '')\n",
    "            \n",
    "            if gt_cat != pred_cat:\n",
    "                continue\n",
    "            \n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            iou = compute_iou(pred_box, gt_box_xyxy)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = idx\n",
    "        \n",
    "        if best_iou >= iou_thresh:\n",
    "            gt_matched.add(best_gt_idx)\n",
    "            is_tp = 1\n",
    "        else:\n",
    "            is_tp = 0\n",
    "        \n",
    "        results.append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': pred_cat,\n",
    "            'is_tp': is_tp,\n",
    "            'iou': best_iou\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Funciones de inferencia y matching definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_calib_json = Path('../data/bdd100k_coco/val_calib.json')\n",
    "image_dir = Path('../data/bdd100k/bdd100k/bdd100k/images/100k/val')\n",
    "\n",
    "coco_calib = COCO(str(val_calib_json))\n",
    "img_ids = coco_calib.getImgIds()\n",
    "\n",
    "print(f\"Procesando {len(img_ids)} imágenes de val_calib...\")\n",
    "\n",
    "calib_data = []\n",
    "\n",
    "for img_id in tqdm(img_ids[:500]):  # Procesar primeras 500 para calibración\n",
    "    img_info = coco_calib.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    predictions = run_inference_on_image(model, img_path, TEXT_PROMPT, 0.25, CONFIG['device'])\n",
    "    \n",
    "    gt_anns = coco_calib.loadAnns(coco_calib.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    matched = match_predictions_to_gt(predictions, gt_anns, CONFIG['iou_matching'])\n",
    "    calib_data.extend(matched)\n",
    "\n",
    "calib_df = pd.DataFrame(calib_data)\n",
    "calib_df.to_csv(OUTPUT_DIR / 'calib_detections.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal detecciones: {len(calib_df)}\")\n",
    "print(f\"TP: {calib_df['is_tp'].sum()}, FP: {len(calib_df) - calib_df['is_tp'].sum()}\")\n",
    "print(f\"Guardado en: {OUTPUT_DIR / 'calib_detections.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe70e85",
   "metadata": {},
   "source": [
    "## 4. Optimizar Temperatura Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_df = pd.read_csv(OUTPUT_DIR / 'calib_detections.csv')\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def nll_loss(T, logits, labels):\n",
    "    \"\"\"Negative Log-Likelihood con temperatura T\"\"\"\n",
    "    T = max(T, 0.01)  # Evitar división por cero\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    nll = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    return nll\n",
    "\n",
    "logits = calib_df['logit'].values\n",
    "labels = calib_df['is_tp'].values\n",
    "\n",
    "# NLL antes de calibrar\n",
    "nll_before = nll_loss(1.0, logits, labels)\n",
    "print(f\"NLL antes (T=1.0): {nll_before:.4f}\")\n",
    "\n",
    "# Optimizar T\n",
    "result = minimize(lambda T: nll_loss(T, logits, labels), x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "T_optimal = result.x[0]\n",
    "nll_after = result.fun\n",
    "\n",
    "print(f\"\\nT óptimo: {T_optimal:.4f}\")\n",
    "print(f\"NLL después: {nll_after:.4f}\")\n",
    "print(f\"Mejora en NLL: {nll_before - nll_after:.4f}\")\n",
    "\n",
    "# Guardar temperatura\n",
    "with open(OUTPUT_DIR / 'temperature.json', 'w') as f:\n",
    "    json.dump({'T_global': T_optimal, 'nll_before': nll_before, 'nll_after': nll_after}, f, indent=2)\n",
    "\n",
    "print(f\"\\nTemperatura guardada en: {OUTPUT_DIR / 'temperature.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd249c2b",
   "metadata": {},
   "source": [
    "## 5. Evaluar en val_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'temperature.json', 'r') as f:\n",
    "    temp_data = json.load(f)\n",
    "    T_optimal = temp_data['T_global']\n",
    "\n",
    "val_eval_json = Path('../data/bdd100k_coco/val_eval.json')\n",
    "coco_eval = COCO(str(val_eval_json))\n",
    "img_ids_eval = coco_eval.getImgIds()\n",
    "\n",
    "print(f\"Procesando {len(img_ids_eval)} imágenes de val_eval...\")\n",
    "\n",
    "eval_data = []\n",
    "\n",
    "for img_id in tqdm(img_ids_eval):\n",
    "    img_info = coco_eval.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    predictions = run_inference_on_image(model, img_path, TEXT_PROMPT, 0.25, CONFIG['device'])\n",
    "    \n",
    "    gt_anns = coco_eval.loadAnns(coco_eval.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    matched = match_predictions_to_gt(predictions, gt_anns, CONFIG['iou_matching'])\n",
    "    eval_data.extend(matched)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.to_csv(OUTPUT_DIR / 'eval_detections.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal detecciones: {len(eval_df)}\")\n",
    "print(f\"TP: {eval_df['is_tp'].sum()}, FP: {len(eval_df) - eval_df['is_tp'].sum()}\")\n",
    "print(f\"Guardado en: {OUTPUT_DIR / 'eval_detections.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda7ef2",
   "metadata": {},
   "source": [
    "## 6. Calcular Métricas de Calibración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration_metrics(logits, labels, T=1.0, n_bins=10):\n",
    "    \"\"\"Calcula ECE, Brier y NLL\"\"\"\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # NLL\n",
    "    nll = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    \n",
    "    # Brier Score\n",
    "    brier = np.mean((probs - labels) ** 2)\n",
    "    \n",
    "    # ECE (Expected Calibration Error)\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_data = []\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (probs >= bin_lower) & (probs < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(probs[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "            \n",
    "            bin_data.append({\n",
    "                'bin_lower': bin_lower,\n",
    "                'bin_upper': bin_upper,\n",
    "                'confidence': avg_confidence_in_bin,\n",
    "                'accuracy': accuracy_in_bin,\n",
    "                'count': np.sum(in_bin)\n",
    "            })\n",
    "        else:\n",
    "            bin_data.append({\n",
    "                'bin_lower': bin_lower,\n",
    "                'bin_upper': bin_upper,\n",
    "                'confidence': (bin_lower + bin_upper) / 2,\n",
    "                'accuracy': 0,\n",
    "                'count': 0\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'nll': nll,\n",
    "        'brier': brier,\n",
    "        'ece': ece,\n",
    "        'bin_data': bin_data\n",
    "    }\n",
    "\n",
    "print(\"Funciones de calibración definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(OUTPUT_DIR / 'eval_detections.csv')\n",
    "with open(OUTPUT_DIR / 'temperature.json', 'r') as f:\n",
    "    T_optimal = json.load(f)['T_global']\n",
    "\n",
    "logits_eval = eval_df['logit'].values\n",
    "labels_eval = eval_df['is_tp'].values\n",
    "\n",
    "# Métricas ANTES de calibrar\n",
    "metrics_before = compute_calibration_metrics(logits_eval, labels_eval, T=1.0)\n",
    "\n",
    "# Métricas DESPUÉS de calibrar\n",
    "metrics_after = compute_calibration_metrics(logits_eval, labels_eval, T=T_optimal)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MÉTRICAS DE CALIBRACIÓN EN VAL_EVAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Métrica':<15} {'Antes':<15} {'Después':<15} {'Mejora':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'NLL':<15} {metrics_before['nll']:<15.4f} {metrics_after['nll']:<15.4f} {metrics_before['nll']-metrics_after['nll']:<15.4f}\")\n",
    "print(f\"{'Brier Score':<15} {metrics_before['brier']:<15.4f} {metrics_after['brier']:<15.4f} {metrics_before['brier']-metrics_after['brier']:<15.4f}\")\n",
    "print(f\"{'ECE':<15} {metrics_before['ece']:<15.4f} {metrics_after['ece']:<15.4f} {metrics_before['ece']-metrics_after['ece']:<15.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Guardar métricas\n",
    "results = {\n",
    "    'T_optimal': T_optimal,\n",
    "    'val_eval': {\n",
    "        'before': {k: v for k, v in metrics_before.items() if k != 'bin_data'},\n",
    "        'after': {k: v for k, v in metrics_after.items() if k != 'bin_data'}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'calibration_metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nMétricas guardadas en: {OUTPUT_DIR / 'calibration_metrics.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7487ad2",
   "metadata": {},
   "source": [
    "## 7. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (T, title) in enumerate([(1.0, 'Antes de Calibrar'), (T_optimal, 'Después de Calibrar')]):\n",
    "    metrics = compute_calibration_metrics(logits_eval, labels_eval, T=T)\n",
    "    bin_data = metrics['bin_data']\n",
    "    \n",
    "    confidences = [b['confidence'] for b in bin_data if b['count'] > 0]\n",
    "    accuracies = [b['accuracy'] for b in bin_data if b['count'] > 0]\n",
    "    counts = [b['count'] for b in bin_data if b['count'] > 0]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.bar(range(len(confidences)), accuracies, width=0.08, alpha=0.3, color='blue', label='Accuracy')\n",
    "    ax.plot(confidences, 'ro-', label='Confidence', markersize=6)\n",
    "    ax.plot([0, len(confidences)-1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "    \n",
    "    ax.set_xlabel('Confidence Bin', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy / Confidence', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nECE = {metrics[\"ece\"]:.4f}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    # Añadir conteo de muestras\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i, -0.08, f'{count}', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'reliability_diagram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Guardado en: {OUTPUT_DIR / 'reliability_diagram.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234a695",
   "metadata": {},
   "source": [
    "## 8. (Opcional) Temperature Scaling por Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_df = pd.read_csv(OUTPUT_DIR / 'calib_detections.csv')\n",
    "\n",
    "T_per_class = {}\n",
    "min_samples = 50  # Mínimo de muestras para optimizar T por clase\n",
    "\n",
    "for cat in CONFIG['categories']:\n",
    "    cat_data = calib_df[calib_df['category'] == cat]\n",
    "    \n",
    "    if len(cat_data) < min_samples:\n",
    "        T_per_class[cat] = T_optimal  # Usar T global si hay pocas muestras\n",
    "        print(f\"{cat:<20} - Insuficientes muestras ({len(cat_data)}), usando T global: {T_optimal:.4f}\")\n",
    "        continue\n",
    "    \n",
    "    logits_cat = cat_data['logit'].values\n",
    "    labels_cat = cat_data['is_tp'].values\n",
    "    \n",
    "    result = minimize(lambda T: nll_loss(T, logits_cat, labels_cat), x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "    T_cat = result.x[0]\n",
    "    T_per_class[cat] = T_cat\n",
    "    \n",
    "    print(f\"{cat:<20} - T: {T_cat:.4f} (n={len(cat_data)})\")\n",
    "\n",
    "# Guardar temperaturas por clase\n",
    "with open(OUTPUT_DIR / 'temperature_per_class.json', 'w') as f:\n",
    "    json.dump({'T_global': T_optimal, 'T_per_class': T_per_class}, f, indent=2)\n",
    "\n",
    "print(f\"\\nTemperaturas por clase guardadas en: {OUTPUT_DIR / 'temperature_per_class.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb215fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(OUTPUT_DIR / 'eval_detections.csv')\n",
    "\n",
    "with open(OUTPUT_DIR / 'temperature_per_class.json', 'r') as f:\n",
    "    T_data = json.load(f)\n",
    "    T_per_class = T_data['T_per_class']\n",
    "\n",
    "eval_df['T_class'] = eval_df['category'].map(T_per_class)\n",
    "eval_df['prob_calibrated_class'] = sigmoid(eval_df['logit'].values / eval_df['T_class'].values)\n",
    "\n",
    "logits_eval = eval_df['logit'].values\n",
    "labels_eval = eval_df['is_tp'].values\n",
    "T_class_array = eval_df['T_class'].values\n",
    "\n",
    "metrics_class = compute_calibration_metrics(logits_eval, labels_eval, T=1.0)\n",
    "probs_class = sigmoid(logits_eval / T_class_array)\n",
    "probs_class = np.clip(probs_class, 1e-7, 1 - 1e-7)\n",
    "nll_class = -np.mean(labels_eval * np.log(probs_class) + (1 - labels_eval) * np.log(1 - probs_class))\n",
    "brier_class = np.mean((probs_class - labels_eval) ** 2)\n",
    "\n",
    "# ECE con T por clase\n",
    "bin_boundaries = np.linspace(0, 1, 11)\n",
    "ece_class = 0.0\n",
    "for i in range(len(bin_boundaries) - 1):\n",
    "    in_bin = (probs_class >= bin_boundaries[i]) & (probs_class < bin_boundaries[i+1])\n",
    "    if np.sum(in_bin) > 0:\n",
    "        acc = np.mean(labels_eval[in_bin])\n",
    "        conf = np.mean(probs_class[in_bin])\n",
    "        ece_class += np.abs(acc - conf) * np.mean(in_bin)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACIÓN: T Global vs T por Clase\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Métrica':<15} {'T Global':<15} {'T por Clase':<15} {'Diferencia':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'NLL':<15} {metrics_after['nll']:<15.4f} {nll_class:<15.4f} {metrics_after['nll']-nll_class:<15.4f}\")\n",
    "print(f\"{'Brier Score':<15} {metrics_after['brier']:<15.4f} {brier_class:<15.4f} {metrics_after['brier']-brier_class:<15.4f}\")\n",
    "print(f\"{'ECE':<15} {metrics_after['ece']:<15.4f} {ece_class:<15.4f} {metrics_after['ece']-ece_class:<15.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce045769",
   "metadata": {},
   "source": [
    "## 9. Verificar Impacto en mAP (Detección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b502bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def create_coco_results(image_dir, coco_gt, text_prompt, model, device, T=1.0):\n",
    "    \"\"\"Genera resultados en formato COCO con scores calibrados\"\"\"\n",
    "    results = []\n",
    "    cat_to_id = {cat: i+1 for i, cat in enumerate(CONFIG['categories'])}\n",
    "    \n",
    "    for img_id in tqdm(coco_gt.getImgIds()):\n",
    "        img_info = coco_gt.loadImgs(img_id)[0]\n",
    "        img_path = image_dir / img_info['file_name']\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "        \n",
    "        predictions = run_inference_on_image(model, img_path, text_prompt, 0.25, device)\n",
    "        \n",
    "        for pred in predictions:\n",
    "            score_calibrated = sigmoid(pred['logit'] / T)\n",
    "            \n",
    "            results.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cat_to_id.get(pred['category'], 1),\n",
    "                'bbox': [pred['bbox'][0], pred['bbox'][1], \n",
    "                        pred['bbox'][2] - pred['bbox'][0], \n",
    "                        pred['bbox'][3] - pred['bbox'][1]],\n",
    "                'score': float(score_calibrated)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# mAP antes de calibrar (T=1.0)\n",
    "print(\"Calculando mAP ANTES de calibrar...\")\n",
    "results_before = create_coco_results(image_dir, coco_eval, TEXT_PROMPT, model, CONFIG['device'], T=1.0)\n",
    "\n",
    "if len(results_before) > 0:\n",
    "    coco_dt_before = coco_eval.loadRes(results_before)\n",
    "    coco_eval_obj_before = COCOeval(coco_eval, coco_dt_before, 'bbox')\n",
    "    coco_eval_obj_before.evaluate()\n",
    "    coco_eval_obj_before.accumulate()\n",
    "    print(\"\\n--- mAP ANTES ---\")\n",
    "    coco_eval_obj_before.summarize()\n",
    "    map_before = coco_eval_obj_before.stats[0]\n",
    "else:\n",
    "    map_before = 0.0\n",
    "\n",
    "# mAP después de calibrar\n",
    "print(\"\\n\\nCalculando mAP DESPUÉS de calibrar...\")\n",
    "results_after = create_coco_results(image_dir, coco_eval, TEXT_PROMPT, model, CONFIG['device'], T=T_optimal)\n",
    "\n",
    "if len(results_after) > 0:\n",
    "    coco_dt_after = coco_eval.loadRes(results_after)\n",
    "    coco_eval_obj_after = COCOeval(coco_eval, coco_dt_after, 'bbox')\n",
    "    coco_eval_obj_after.evaluate()\n",
    "    coco_eval_obj_after.accumulate()\n",
    "    print(\"\\n--- mAP DESPUÉS ---\")\n",
    "    coco_eval_obj_after.summarize()\n",
    "    map_after = coco_eval_obj_after.stats[0]\n",
    "else:\n",
    "    map_after = 0.0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"mAP antes:   {map_before:.4f}\")\n",
    "print(f\"mAP después: {map_after:.4f}\")\n",
    "print(f\"Diferencia:  {map_after - map_before:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ce0aa",
   "metadata": {},
   "source": [
    "## 10. Análisis de Distribución de Confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(OUTPUT_DIR / 'eval_detections.csv')\n",
    "\n",
    "prob_before = sigmoid(eval_df['logit'].values)\n",
    "prob_after = sigmoid(eval_df['logit'].values / T_optimal)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribución antes\n",
    "axes[0].hist(prob_before[eval_df['is_tp']==1], bins=30, alpha=0.6, label='TP', color='green', density=True)\n",
    "axes[0].hist(prob_before[eval_df['is_tp']==0], bins=30, alpha=0.6, label='FP', color='red', density=True)\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Antes de Calibrar', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Distribución después\n",
    "axes[1].hist(prob_after[eval_df['is_tp']==1], bins=30, alpha=0.6, label='TP', color='green', density=True)\n",
    "axes[1].hist(prob_after[eval_df['is_tp']==0], bins=30, alpha=0.6, label='FP', color='red', density=True)\n",
    "axes[1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title(f'Después de Calibrar (T={T_optimal:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confidence_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Guardado en: {OUTPUT_DIR / 'confidence_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169b0e4",
   "metadata": {},
   "source": [
    "## 11. Risk-Coverage Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_coverage(scores, labels):\n",
    "    \"\"\"Calcula risk-coverage curve ordenando por confianza descendente\"\"\"\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    scores_sorted = scores[order]\n",
    "    labels_sorted = labels[order]\n",
    "    \n",
    "    coverages = []\n",
    "    risks = []\n",
    "    \n",
    "    for i in range(1, len(scores_sorted) + 1):\n",
    "        coverage = i / len(scores_sorted)\n",
    "        risk = 1 - np.mean(labels_sorted[:i])\n",
    "        coverages.append(coverage)\n",
    "        risks.append(risk)\n",
    "    \n",
    "    return coverages, risks\n",
    "\n",
    "eval_df = pd.read_csv(OUTPUT_DIR / 'eval_detections.csv')\n",
    "\n",
    "# Antes de calibrar\n",
    "scores_before = sigmoid(eval_df['logit'].values)\n",
    "labels = eval_df['is_tp'].values\n",
    "cov_before, risk_before = compute_risk_coverage(scores_before, labels)\n",
    "\n",
    "# Después de calibrar\n",
    "scores_after = sigmoid(eval_df['logit'].values / T_optimal)\n",
    "cov_after, risk_after = compute_risk_coverage(scores_after, labels)\n",
    "\n",
    "# Plotear\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cov_before, risk_before, 'b-', label='Antes (T=1.0)', linewidth=2)\n",
    "plt.plot(cov_after, risk_after, 'r-', label=f'Después (T={T_optimal:.3f})', linewidth=2)\n",
    "plt.xlabel('Coverage (fracción de predicciones retenidas)', fontsize=12)\n",
    "plt.ylabel('Risk (tasa de error)', fontsize=12)\n",
    "plt.title('Risk-Coverage Trade-off', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'risk_coverage.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Guardado en: {OUTPUT_DIR / 'risk_coverage.png'}\")\n",
    "\n",
    "# Calcular AUC\n",
    "auc_before = np.trapz(risk_before, cov_before)\n",
    "auc_after = np.trapz(risk_after, cov_after)\n",
    "print(f\"\\nAUC Risk-Coverage antes:  {auc_before:.4f}\")\n",
    "print(f\"AUC Risk-Coverage después: {auc_after:.4f}\")\n",
    "print(f\"Mejora (menor es mejor): {auc_before - auc_after:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b87bb",
   "metadata": {},
   "source": [
    "## 12. Reporte Final y Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'calibration_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "report = f\"\"\"\n",
    "{'='*70}\n",
    "FASE 4: TEMPERATURE SCALING - REPORTE FINAL\n",
    "{'='*70}\n",
    "\n",
    "1. TEMPERATURA ÓPTIMA\n",
    "   - T global: {T_optimal:.4f}\n",
    "\n",
    "2. MÉTRICAS DE CALIBRACIÓN EN VAL_EVAL\n",
    "   \n",
    "   Métrica         | Antes (T=1.0)  | Después        | Mejora\n",
    "   ----------------|----------------|----------------|---------------\n",
    "   NLL             | {metrics_before['nll']:14.4f} | {metrics_after['nll']:14.4f} | {metrics_before['nll']-metrics_after['nll']:14.4f}\n",
    "   Brier Score     | {metrics_before['brier']:14.4f} | {metrics_after['brier']:14.4f} | {metrics_before['brier']-metrics_after['brier']:14.4f}\n",
    "   ECE             | {metrics_before['ece']:14.4f} | {metrics_after['ece']:14.4f} | {metrics_before['ece']-metrics_after['ece']:14.4f}\n",
    "\n",
    "3. INTERPRETACIÓN\n",
    "   - T > 1: Modelo sobreconfidente → Temperature Scaling reduce confianza\n",
    "   - T < 1: Modelo subconfidente → Temperature Scaling aumenta confianza\n",
    "   - T ≈ 1: Modelo ya bien calibrado\n",
    "   \n",
    "   En este caso: T = {T_optimal:.4f}\n",
    "   {'→ El modelo era SOBRECONFIDENTE' if T_optimal > 1 else '→ El modelo era SUBCONFIDENTE' if T_optimal < 1 else '→ El modelo ya estaba calibrado'}\n",
    "\n",
    "4. IMPACTO EN DETECCIÓN\n",
    "   - mAP antes:  {map_before:.4f}\n",
    "   - mAP después: {map_after:.4f}\n",
    "   - Diferencia: {map_after - map_before:+.4f}\n",
    "   \n",
    "   → La calibración {'MEJORA' if map_after >= map_before else 'REDUCE LIGERAMENTE'} el rendimiento de detección\n",
    "\n",
    "5. ARTEFACTOS GENERADOS\n",
    "   ✓ {OUTPUT_DIR / 'temperature.json'}\n",
    "   ✓ {OUTPUT_DIR / 'calib_detections.csv'}\n",
    "   ✓ {OUTPUT_DIR / 'eval_detections.csv'}\n",
    "   ✓ {OUTPUT_DIR / 'calibration_metrics.json'}\n",
    "   ✓ {OUTPUT_DIR / 'reliability_diagram.png'}\n",
    "   ✓ {OUTPUT_DIR / 'confidence_distribution.png'}\n",
    "   ✓ {OUTPUT_DIR / 'risk_coverage.png'}\n",
    "   {'✓ ' + str(OUTPUT_DIR / 'temperature_per_class.json') if (OUTPUT_DIR / 'temperature_per_class.json').exists() else ''}\n",
    "\n",
    "6. CONCLUSIONES PARA ADAS\n",
    "   - Las probabilidades calibradas son más coherentes con la realidad\n",
    "   - Cuando el modelo dice p=0.8, ahora acierta ~80% de las veces\n",
    "   - Esto permite tomar decisiones de confianza más informadas\n",
    "   - Crucial para aplicaciones de seguridad donde la confianza importa\n",
    "\n",
    "{'='*70}\n",
    "FASE 4 COMPLETADA EXITOSAMENTE\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(OUTPUT_DIR / 'final_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReporte guardado en: {OUTPUT_DIR / 'final_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fa8b4",
   "metadata": {},
   "source": [
    "## 13. Análisis de Calibración por Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(OUTPUT_DIR / 'eval_detections.csv')\n",
    "\n",
    "per_class_metrics = []\n",
    "\n",
    "for cat in CONFIG['categories']:\n",
    "    cat_data = eval_df[eval_df['category'] == cat]\n",
    "    \n",
    "    if len(cat_data) < 10:\n",
    "        continue\n",
    "    \n",
    "    logits_cat = cat_data['logit'].values\n",
    "    labels_cat = cat_data['is_tp'].values\n",
    "    \n",
    "    # Antes\n",
    "    metrics_cat_before = compute_calibration_metrics(logits_cat, labels_cat, T=1.0)\n",
    "    \n",
    "    # Después (T global)\n",
    "    metrics_cat_after = compute_calibration_metrics(logits_cat, labels_cat, T=T_optimal)\n",
    "    \n",
    "    per_class_metrics.append({\n",
    "        'category': cat,\n",
    "        'n_samples': len(cat_data),\n",
    "        'nll_before': metrics_cat_before['nll'],\n",
    "        'nll_after': metrics_cat_after['nll'],\n",
    "        'ece_before': metrics_cat_before['ece'],\n",
    "        'ece_after': metrics_cat_after['ece'],\n",
    "        'brier_before': metrics_cat_before['brier'],\n",
    "        'brier_after': metrics_cat_after['brier']\n",
    "    })\n",
    "\n",
    "df_per_class = pd.DataFrame(per_class_metrics)\n",
    "df_per_class['nll_improvement'] = df_per_class['nll_before'] - df_per_class['nll_after']\n",
    "df_per_class['ece_improvement'] = df_per_class['ece_before'] - df_per_class['ece_after']\n",
    "\n",
    "df_per_class.to_csv(OUTPUT_DIR / 'calibration_per_class.csv', index=False)\n",
    "\n",
    "print(\"MÉTRICAS DE CALIBRACIÓN POR CLASE\\n\")\n",
    "print(df_per_class.to_string(index=False))\n",
    "print(f\"\\nGuardado en: {OUTPUT_DIR / 'calibration_per_class.csv'}\")\n",
    "\n",
    "# Visualización\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cats = df_per_class['category'].values\n",
    "x_pos = np.arange(len(cats))\n",
    "\n",
    "# ECE\n",
    "axes[0].bar(x_pos - 0.2, df_per_class['ece_before'], 0.4, label='Antes', alpha=0.7, color='blue')\n",
    "axes[0].bar(x_pos + 0.2, df_per_class['ece_after'], 0.4, label='Después', alpha=0.7, color='red')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(cats, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('ECE', fontsize=12)\n",
    "axes[0].set_title('ECE por Clase', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# NLL\n",
    "axes[1].bar(x_pos - 0.2, df_per_class['nll_before'], 0.4, label='Antes', alpha=0.7, color='blue')\n",
    "axes[1].bar(x_pos + 0.2, df_per_class['nll_after'], 0.4, label='Después', alpha=0.7, color='red')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(cats, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('NLL', fontsize=12)\n",
    "axes[1].set_title('NLL por Clase', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'calibration_per_class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gráfica guardada en: {OUTPUT_DIR / 'calibration_per_class.png'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
