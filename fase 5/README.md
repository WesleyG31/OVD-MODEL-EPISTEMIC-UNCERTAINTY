# Fase 5: ComparaciÃ³n Completa de MÃ©todos âš¡ (Optimizado)

## ğŸ¯ Objetivo

Comparar 6 mÃ©todos de incertidumbre y calibraciÃ³n lado a lado:
1. **Baseline** (sin incertidumbre, sin calibraciÃ³n)
2. **Baseline + TS** (Temperature Scaling)
3. **MC-Dropout K=5** (incertidumbre epistÃ©mica)
4. **MC-Dropout K=5 + TS** (incertidumbre + calibraciÃ³n)
5. **Varianza entre capas** (single-pass, decoder layers)
6. **Varianza entre capas + TS**

---

## âš¡ NUEVO: OptimizaciÃ³n de Rendimiento

**Este notebook ha sido optimizado para reutilizar resultados de fases anteriores.**

### âœ… Tiempo de EjecuciÃ³n

| Escenario | Tiempo Original | Tiempo Optimizado | Ahorro |
|-----------|----------------|-------------------|--------|
| **Con archivos previos** | ~2 horas | **~15-20 minutos** | âš¡ 85% |
| **Sin archivos previos** | ~2 horas | ~2 horas | - |

### ğŸ“¦ Archivos Requeridos

Para mÃ¡ximo beneficio, asegÃºrate de tener:

```
fase 2/outputs/baseline/preds_raw.json           â† Predicciones baseline
fase 3/outputs/mc_dropout/preds_mc_aggregated.json â† Predicciones MC-Dropout
fase 4/outputs/temperature_scaling/temperature.json â† Temperaturas optimizadas
```

### ğŸ” Verificar OptimizaciÃ³n

Ejecuta el script de verificaciÃ³n antes de correr el notebook:

```bash
python verify_optimization.py
```

Esto te dirÃ¡:
- âœ… QuÃ© archivos estÃ¡n disponibles
- â±ï¸ CuÃ¡nto tiempo ahorrarÃ¡s
- ğŸ“‹ QuÃ© necesitas ejecutar primero (si algo falta)

---

## ğŸ“Š MÃ©tricas Evaluadas

### 1. DetecciÃ³n
- **mAP@[0.5:0.95]**: MÃ©trica principal de COCO
- **AP50**: PrecisiÃ³n a IoU=0.5
- **AP75**: PrecisiÃ³n a IoU=0.75
- **Por clase**: MÃ©tricas individuales para cada categorÃ­a

### 2. CalibraciÃ³n
- **NLL** (Negative Log-Likelihood): PÃ©rdida probabilÃ­stica
- **Brier Score**: Error cuadrÃ¡tico de predicciones probabilÃ­sticas
- **ECE** (Expected Calibration Error): Diferencia entre confianza y precisiÃ³n
- **Reliability Diagrams**: VisualizaciÃ³n de calibraciÃ³n

### 3. Risk-Coverage
- **Curvas Risk-Coverage**: Trade-off entre riesgo y cobertura
- **AUC**: Ãrea bajo la curva (mayor es mejor)
- **Uncertainty AUROC**: Capacidad de discriminar TP vs FP usando incertidumbre

---

## ğŸ—‚ï¸ Estructura de Datos

### Splits de ValidaciÃ³n
```python
val_calib.json  # 500 imÃ¡genes â†’ Ajustar temperaturas
val_eval.json   # ~10,000 imÃ¡genes â†’ EvaluaciÃ³n final
```

### Outputs Generados
```
outputs/comparison/
â”œâ”€â”€ config.yaml                      # ConfiguraciÃ³n usada
â”‚
â”œâ”€â”€ calib_baseline.csv               # Datos de calibraciÃ³n
â”œâ”€â”€ calib_mc_dropout.csv
â”œâ”€â”€ calib_decoder_variance.csv
â”‚
â”œâ”€â”€ temperatures.json                # Temperaturas optimizadas
â”‚
â”œâ”€â”€ eval_baseline.csv                # Predicciones en val_eval
â”œâ”€â”€ eval_baseline_ts.csv
â”œâ”€â”€ eval_mc_dropout.csv
â”œâ”€â”€ eval_mc_dropout_ts.csv
â”œâ”€â”€ eval_decoder_variance.csv
â”œâ”€â”€ eval_decoder_variance_ts.csv
â”‚
â”œâ”€â”€ detection_metrics.json           # MÃ©tricas mAP por mÃ©todo
â”œâ”€â”€ calibration_metrics.json         # MÃ©tricas de calibraciÃ³n
â”œâ”€â”€ risk_coverage_auc.json          # AUCs de risk-coverage
â”œâ”€â”€ uncertainty_auroc.json          # AUROC por mÃ©todo
â”‚
â””â”€â”€ visualizations/
    â”œâ”€â”€ comparison_map.png           # ComparaciÃ³n de mAP
    â”œâ”€â”€ comparison_calibration.png   # ComparaciÃ³n de NLL/ECE
    â”œâ”€â”€ reliability_diagrams.png     # 6 reliability diagrams
    â”œâ”€â”€ risk_coverage_curves.png     # Curvas risk-coverage
    â””â”€â”€ uncertainty_auroc.png        # AUROC de incertidumbre
```

---

## ğŸš€ CÃ³mo Ejecutar

### OpciÃ³n A: Con OptimizaciÃ³n (RECOMENDADO)

```bash
# 1. Verificar que tienes los archivos previos
python verify_optimization.py

# 2. Si sale âœ…, ejecutar el notebook
jupyter notebook main.ipynb

# Tiempo: ~15-20 minutos âš¡
```

### OpciÃ³n B: Primera Vez (Sin archivos previos)

```bash
# Ejecuta directamente
jupyter notebook main.ipynb

# Tiempo: ~2 horas ğŸŒ
# Pero generarÃ¡ archivos para futuras ejecuciones
```

### OpciÃ³n C: Ejecutar Fases Previas Primero

```bash
# 1. Ejecutar Fase 2
cd "../fase 2"
jupyter notebook main.ipynb

# 2. Ejecutar Fase 3
cd "../fase 3"
jupyter notebook main.ipynb

# 3. Ejecutar Fase 4
cd "../fase 4"
jupyter notebook main.ipynb

# 4. Ahora Fase 5 serÃ¡ rÃ¡pida
cd "../fase 5"
jupyter notebook main.ipynb
```

---

## ğŸ“ Detalles de ImplementaciÃ³n

### MÃ©todos de Inferencia

#### 1. Baseline
```python
def inference_baseline(model, image_path, text_prompt, conf_thresh, device):
    # Single-pass, dropout desactivado
    # Incertidumbre = 0.0 (no tiene)
```

#### 2. MC-Dropout
```python
def inference_mc_dropout(model, image_path, text_prompt, conf_thresh, device, K=5):
    # K pases con dropout activo
    # Incertidumbre = varianza de scores entre pases
    # AlineaciÃ³n de detecciones con IoU >= 0.5
```

#### 3. Decoder Variance
```python
def inference_decoder_variance(model, image_path, text_prompt, conf_thresh, device):
    # Single-pass con hooks en capas del decoder
    # Incertidumbre = varianza de scores entre capas
    # RÃ¡pido (sin mÃºltiples pases)
```

### Temperature Scaling

```python
# OptimizaciÃ³n por NLL
T_opt = minimize(lambda T: nll_loss(T, logits, labels), x0=1.0)

# AplicaciÃ³n
score_calibrated = sigmoid(logit / T_opt)
```

---

## ğŸ“ˆ AnÃ¡lisis de Resultados

### MÃ©tricas Clave a Comparar

1. **DetecciÃ³n (mAP)**:
   - Â¿Mejora la calibraciÃ³n la detecciÃ³n?
   - Â¿Afecta el MC-Dropout el rendimiento?

2. **CalibraciÃ³n (NLL, ECE)**:
   - Â¿QuÃ© mÃ©todo tiene mejor calibraciÃ³n inicial?
   - Â¿Mejora significativamente el Temperature Scaling?

3. **Risk-Coverage (AUC)**:
   - Â¿QuÃ© mÃ©todo permite mejor trade-off riesgo/cobertura?
   - Â¿La incertidumbre epistÃ©mica ayuda?

4. **Uncertainty Quality (AUROC)**:
   - Â¿Puede la incertidumbre discriminar TP vs FP?
   - Â¿MC-Dropout vs Decoder Variance?

---

## ğŸ”§ ConfiguraciÃ³n

```yaml
seed: 42
device: cuda  # o cpu
categories: [person, rider, car, truck, bus, train, motorcycle, 
            bicycle, traffic light, traffic sign]
iou_matching: 0.5
conf_threshold: 0.25
nms_threshold: 0.65
K_mc: 5
n_bins: 10
```

---

## ğŸ› Troubleshooting

### Problema: "ModuleNotFoundError: groundingdino"
```bash
# AsegÃºrate de estar en el entorno correcto
source /opt/program/venv/bin/activate  # Linux
# O
/opt/program/venv/Scripts/activate  # Windows
```

### Problema: "CUDA out of memory"
```python
# Reducir batch size implÃ­cito o usar CPU
CONFIG['device'] = 'cpu'
```

### Problema: "Archivos no encontrados"
```bash
# Verificar paths
python verify_optimization.py

# Si faltan, ejecuta las fases anteriores
```

### Problema: "Resultados no coinciden"
```python
# Verifica que uses el mismo split
# Todas las fases deben usar:
#   - val_calib.json (mismo)
#   - val_eval.json (mismo)
```

---

## ğŸ“š Referencias

- **Baseline**: GroundingDINO estÃ¡ndar
- **MC-Dropout**: [Gal & Ghahramani, 2016](https://arxiv.org/abs/1506.02142)
- **Temperature Scaling**: [Guo et al., 2017](https://arxiv.org/abs/1706.04599)
- **Risk-Coverage**: [Geifman & El-Yaniv, 2017](https://arxiv.org/abs/1705.08500)

---

## ğŸ“– DocumentaciÃ³n Adicional

- **[OPTIMIZACIONES.md](OPTIMIZACIONES.md)**: Detalles tÃ©cnicos de las optimizaciones
- **[verify_optimization.py](verify_optimization.py)**: Script de verificaciÃ³n
- **main.ipynb**: Notebook principal (con comentarios extensos)

---

## âœ… Checklist de EjecuciÃ³n

- [ ] Â¿Tienes los archivos de fases anteriores? â†’ Ejecuta `verify_optimization.py`
- [ ] Â¿Configuraste correctamente los paths? â†’ Revisa `BASE_DIR`, `DATA_DIR`
- [ ] Â¿Tienes GPU disponible? â†’ Verifica `CONFIG['device']`
- [ ] Â¿Instalaste dependencias? â†’ `torch`, `groundingdino`, `pycocotools`
- [ ] Â¿Activaste el entorno virtual? â†’ `source /opt/program/venv/bin/activate`

---

## ğŸ“ Resultados Esperados

Al finalizar, tendrÃ¡s:

1. âœ… ComparaciÃ³n cuantitativa de 6 mÃ©todos
2. âœ… Visualizaciones de comparaciÃ³n
3. âœ… AnÃ¡lisis de calibraciÃ³n
4. âœ… Curvas risk-coverage
5. âœ… EvaluaciÃ³n de calidad de incertidumbre
6. âœ… Reporte final con recomendaciones

**Pregunta clave**: Â¿Vale la pena el costo computacional del MC-Dropout comparado con mÃ©todos single-pass?

---

## ğŸ“§ Soporte

Si encuentras problemas o tienes preguntas, revisa:
1. La documentaciÃ³n en `OPTIMIZACIONES.md`
2. Los comentarios en el notebook
3. Los mensajes de error del script de verificaciÃ³n

**Ãšltima actualizaciÃ³n**: 2024
**VersiÃ³n**: 2.0 (Optimizado)
**Estado**: âœ… Probado y funcional
