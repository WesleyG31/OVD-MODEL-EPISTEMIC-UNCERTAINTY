{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a95a23",
   "metadata": {},
   "source": [
    "# ‚ö° OPTIMIZACI√ìN: Reutilizar Resultados de Fases Anteriores\n",
    "\n",
    "**Este notebook ha sido optimizado para reutilizar resultados existentes:**\n",
    "\n",
    "- ‚úÖ **Fase 2 (Baseline)**: Carga predicciones desde `../fase 2/outputs/baseline/preds_raw.json`\n",
    "- ‚úÖ **Fase 3 (MC-Dropout)**: Carga predicciones desde `../fase 3/outputs/mc_dropout/preds_mc_aggregated.json`\n",
    "- ‚úÖ **Fase 4 (Temperature Scaling)**: Carga temperaturas desde `../fase 4/outputs/temperature_scaling/temperature.json`\n",
    "\n",
    "**Ventajas**:\n",
    "- üöÄ Reduce tiempo de ejecuci√≥n de ~2 horas a ~15 minutos\n",
    "- üíæ Evita recalcular predicciones costosas (especialmente MC-Dropout con K=5)\n",
    "- ‚ôªÔ∏è Garantiza consistencia con resultados de fases anteriores\n",
    "\n",
    "**Modo de operaci√≥n**:\n",
    "- Si los archivos existen ‚Üí Los carga y reutiliza\n",
    "- Si no existen ‚Üí Ejecuta inferencia completa (fallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6846700",
   "metadata": {},
   "source": [
    "# Fase 5: Comparaci√≥n Completa de M√©todos de Incertidumbre y Calibraci√≥n\n",
    "\n",
    "**Objetivo**: Comparar 6 m√©todos lado a lado en detecci√≥n, calibraci√≥n y risk-coverage.\n",
    "\n",
    "**M√©todos evaluados**:\n",
    "1. Baseline (sin incertidumbre, sin calibraci√≥n)\n",
    "2. Baseline + TS\n",
    "3. MC-Dropout K=5\n",
    "4. MC-Dropout K=5 + TS\n",
    "5. Varianza entre capas (single-pass)\n",
    "6. Varianza entre capas + TS\n",
    "\n",
    "**Splits**:\n",
    "- val_calib: ajustar temperaturas\n",
    "- val_eval: evaluaci√≥n final\n",
    "\n",
    "**M√©tricas**:\n",
    "- Detecci√≥n: mAP@[0.5:0.95], AP50, AP75, por clase\n",
    "- Calibraci√≥n: NLL, Brier, ECE, Reliability Diagrams\n",
    "- Risk-Coverage: curvas y AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5375caf",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04069526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Output: outputs/comparison\n",
      "Config guardado\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 1: Configuraci√≥n e Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./outputs/comparison')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'iou_matching': 0.5,\n",
    "    'conf_threshold': 0.25,\n",
    "    'nms_threshold': 0.65,\n",
    "    'K_mc': 5,\n",
    "    'n_bins': 10\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "with open(OUTPUT_DIR / 'config.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Config guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c7eab",
   "metadata": {},
   "source": [
    "## 1.1 Cargar Resultados de Fases Anteriores (Optimizaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538fe7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cargando predicciones Baseline desde Fase 2...\n",
      "   ‚Üí 22162 predicciones cargadas\n",
      "‚úÖ Cargando predicciones MC-Dropout desde Fase 3 (con incertidumbre)...\n",
      "   ‚Üí 29914 predicciones cargadas (con incertidumbre)\n",
      "‚úÖ Cargando temperaturas optimizadas desde Fase 4...\n",
      "   ‚Üí Temperatura baseline: N/A\n",
      "\n",
      "============================================================\n",
      "RESUMEN DE OPTIMIZACI√ìN:\n",
      "============================================================\n",
      "Baseline disponible:      ‚úÖ S√ç\n",
      "MC-Dropout disponible:    ‚úÖ S√ç\n",
      "Temperaturas disponibles: ‚úÖ S√ç\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 2: Cargar resultados de fases anteriores\n",
    "\n",
    "# Rutas a resultados de fases anteriores\n",
    "FASE2_BASELINE = BASE_DIR / 'fase 2' / 'outputs' / 'baseline' / 'preds_raw.json'\n",
    "FASE3_MC_DROPOUT_PARQUET = BASE_DIR / 'fase 3' / 'outputs' / 'mc_dropout' / 'mc_stats_labeled.parquet'\n",
    "FASE3_MC_DROPOUT_JSON = BASE_DIR / 'fase 3' / 'outputs' / 'mc_dropout' / 'preds_mc_aggregated.json'\n",
    "FASE4_TEMPERATURE = BASE_DIR / 'fase 4' / 'outputs' / 'temperature_scaling' / 'temperature.json'\n",
    "FASE4_CALIB_DATA = BASE_DIR / 'fase 4' / 'outputs' / 'temperature_scaling' / 'calib_detections.csv'\n",
    "\n",
    "# Diccionario para almacenar predicciones cargadas\n",
    "cached_predictions = {\n",
    "    'baseline': None,\n",
    "    'mc_dropout': None,\n",
    "    'temperatures': None\n",
    "}\n",
    "\n",
    "# Cargar Baseline (Fase 2)\n",
    "if FASE2_BASELINE.exists():\n",
    "    print(f\"‚úÖ Cargando predicciones Baseline desde Fase 2...\")\n",
    "    with open(FASE2_BASELINE, 'r') as f:\n",
    "        cached_predictions['baseline'] = json.load(f)\n",
    "    print(f\"   ‚Üí {len(cached_predictions['baseline'])} predicciones cargadas\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No se encontr√≥ {FASE2_BASELINE}, se ejecutar√° inferencia completa\")\n",
    "\n",
    "# Cargar MC-Dropout (Fase 3) - PRIORIZAR PARQUET CON INCERTIDUMBRE\n",
    "if FASE3_MC_DROPOUT_PARQUET.exists():\n",
    "    print(f\"‚úÖ Cargando predicciones MC-Dropout desde Fase 3 (con incertidumbre)...\")\n",
    "    mc_df = pd.read_parquet(FASE3_MC_DROPOUT_PARQUET)\n",
    "    # Convertir a formato JSON similar\n",
    "    cached_predictions['mc_dropout'] = []\n",
    "    for _, row in mc_df.iterrows():\n",
    "        # Convertir bbox de xyxy a xywh para consistencia\n",
    "        bbox = row['bbox']\n",
    "        if isinstance(bbox, (list, np.ndarray)) and len(bbox) == 4:\n",
    "            # Si est√° en formato xyxy, convertir a xywh\n",
    "            if bbox[2] > bbox[0] and bbox[3] > bbox[1]:\n",
    "                bbox_xywh = [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]]\n",
    "            else:\n",
    "                bbox_xywh = bbox\n",
    "        else:\n",
    "            bbox_xywh = bbox\n",
    "            \n",
    "        cached_predictions['mc_dropout'].append({\n",
    "            'image_id': int(row['image_id']),\n",
    "            'category_id': int(row['category_id']) + 1,  # Convertir de 0-indexed a 1-indexed\n",
    "            'bbox': bbox_xywh,\n",
    "            'score': float(row['score_mean']),\n",
    "            'uncertainty': float(row['uncertainty'])  # ¬°IMPORTANTE: con incertidumbre!\n",
    "        })\n",
    "    print(f\"   ‚Üí {len(cached_predictions['mc_dropout'])} predicciones cargadas (con incertidumbre)\")\n",
    "elif FASE3_MC_DROPOUT_JSON.exists():\n",
    "    print(f\"‚ö†Ô∏è  Cargando MC-Dropout desde JSON (SIN incertidumbre)...\")\n",
    "    with open(FASE3_MC_DROPOUT_JSON, 'r') as f:\n",
    "        cached_predictions['mc_dropout'] = json.load(f)\n",
    "    print(f\"   ‚Üí {len(cached_predictions['mc_dropout'])} predicciones cargadas\")\n",
    "    print(f\"   ‚ö†Ô∏è  ADVERTENCIA: Este archivo NO contiene incertidumbre, se calcular√° como 0.0\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No se encontr√≥ {FASE3_MC_DROPOUT_PARQUET}, se ejecutar√° inferencia completa\")\n",
    "\n",
    "# Cargar Temperaturas (Fase 4)\n",
    "if FASE4_TEMPERATURE.exists():\n",
    "    print(f\"‚úÖ Cargando temperaturas optimizadas desde Fase 4...\")\n",
    "    with open(FASE4_TEMPERATURE, 'r') as f:\n",
    "        cached_predictions['temperatures'] = json.load(f)\n",
    "    print(f\"   ‚Üí Temperatura baseline: {cached_predictions['temperatures'].get('optimal_temperature', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No se encontr√≥ {FASE4_TEMPERATURE}, se calcular√°n temperaturas\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESUMEN DE OPTIMIZACI√ìN:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Baseline disponible:      {'‚úÖ S√ç' if cached_predictions['baseline'] else '‚ùå NO (se calcular√°)'}\")\n",
    "print(f\"MC-Dropout disponible:    {'‚úÖ S√ç' if cached_predictions['mc_dropout'] else '‚ùå NO (se calcular√°)'}\")\n",
    "print(f\"Temperaturas disponibles: {'‚úÖ S√ç' if cached_predictions['temperatures'] else '‚ùå NO (se calcular√°n)'}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6cde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de conversi√≥n de formato definidas\n"
     ]
    }
   ],
   "source": [
    "# Funciones para convertir formatos de predicciones desde fases anteriores\n",
    "\n",
    "def convert_baseline_predictions(baseline_data, image_filename_to_id):\n",
    "    \"\"\"\n",
    "    Convierte predicciones de fase 2 (baseline) al formato esperado.\n",
    "    baseline_data: lista de dicts con keys: image_id, category_id, bbox, score\n",
    "    \"\"\"\n",
    "    converted = {}\n",
    "    for pred in baseline_data:\n",
    "        img_id = pred.get('image_id')\n",
    "        if img_id not in converted:\n",
    "            converted[img_id] = []\n",
    "        \n",
    "        bbox = pred['bbox']  # [x, y, w, h] en formato COCO\n",
    "        bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "        score = pred['score']\n",
    "        score_clipped = np.clip(score, 1e-7, 1 - 1e-7)\n",
    "        logit = np.log(score_clipped / (1 - score_clipped))\n",
    "        \n",
    "        converted[img_id].append({\n",
    "            'bbox': bbox_xyxy,\n",
    "            'score': score_clipped,\n",
    "            'logit': logit,\n",
    "            'category_id': pred['category_id'],\n",
    "            'uncertainty': pred.get('uncertainty', 0.0)  # Baseline no tiene incertidumbre\n",
    "        })\n",
    "    \n",
    "    return converted\n",
    "\n",
    "def convert_mc_predictions(mc_data, image_filename_to_id):\n",
    "    \"\"\"\n",
    "    Convierte predicciones de fase 3 (MC-Dropout) al formato esperado.\n",
    "    Maneja tanto bbox en formato [x,y,w,h] como [x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    converted = {}\n",
    "    for pred in mc_data:\n",
    "        img_id = pred.get('image_id')\n",
    "        if img_id not in converted:\n",
    "            converted[img_id] = []\n",
    "        \n",
    "        bbox = pred['bbox']  # Podr√≠a ser [x, y, w, h] o [x1, y1, x2, y2]\n",
    "        \n",
    "        # Detectar formato: si bbox[2] > bbox[0] y bbox[3] > bbox[1], probablemente es xywh\n",
    "        # Si bbox[2] < bbox[0] o bbox[3] < bbox[1], est√° mal\n",
    "        # Asumimos que si w,h > x,y entonces es xyxy\n",
    "        if len(bbox) == 4:\n",
    "            # Si parece formato xywh (w,h son razonables)\n",
    "            if bbox[2] < bbox[0] or bbox[3] < bbox[1]:\n",
    "                # Ya est√° en formato xyxy\n",
    "                bbox_xyxy = bbox\n",
    "            else:\n",
    "                # Formato xywh, convertir a xyxy\n",
    "                bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "        else:\n",
    "            bbox_xyxy = bbox\n",
    "        \n",
    "        score = pred['score']\n",
    "        score_clipped = np.clip(score, 1e-7, 1 - 1e-7)\n",
    "        logit = np.log(score_clipped / (1 - score_clipped))\n",
    "        \n",
    "        converted[img_id].append({\n",
    "            'bbox': bbox_xyxy,\n",
    "            'score': score_clipped,\n",
    "            'logit': logit,\n",
    "            'category_id': pred['category_id'],\n",
    "            'uncertainty': pred.get('uncertainty', 0.0)  # ¬°IMPORTANTE: preservar incertidumbre!\n",
    "        })\n",
    "    \n",
    "    return converted\n",
    "\n",
    "print(\"‚úÖ Funciones de conversi√≥n de formato definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ed4b",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo y Preparar Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483452c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Modelo cargado en cuda\n",
      "Prompt: person. rider. car. truck. bus. train. motorcycle. bicycle. traffic light. traffic sign.\n",
      "M√≥dulos dropout en cabeza: 0\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 3: Cargar modelo Grounding DINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "\n",
    "model_config = '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_weights = '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "print(f\"Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"Prompt: {TEXT_PROMPT}\")\n",
    "\n",
    "# Guardar referencias de m√≥dulos dropout\n",
    "dropout_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Dropout) and ('class_embed' in name or 'bbox_embed' in name):\n",
    "        dropout_modules.append(module)\n",
    "\n",
    "print(f\"M√≥dulos dropout en cabeza: {len(dropout_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91608a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones auxiliares definidas\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 4: Funciones auxiliares\n",
    "\n",
    "def normalize_label(label):\n",
    "    synonyms = {'bike': 'bicycle', 'motorbike': 'motorcycle', 'pedestrian': 'person', \n",
    "                'stop sign': 'traffic sign', 'red light': 'traffic light'}\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def apply_nms(detections, iou_thresh=0.65):\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "    boxes_t = torch.tensor([d['bbox'] for d in detections], dtype=torch.float32)\n",
    "    scores_t = torch.tensor([d['score'] for d in detections], dtype=torch.float32)\n",
    "    keep = torchvision.ops.nms(boxes_t, scores_t, iou_thresh)\n",
    "    return [detections[i] for i in keep.numpy()]\n",
    "\n",
    "print(\"Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58514ed6",
   "metadata": {},
   "source": [
    "## 3. M√©todos de Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89986b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©todo 1: Baseline definido\n"
     ]
    }
   ],
   "source": [
    "def inference_baseline(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"M√©todo 1: Baseline single-pass sin incertidumbre\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.eval()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    h, w = image_source.shape[:2]\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for box, score, phrase in zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat in CONFIG['categories']:\n",
    "            score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            detections.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'category': cat,\n",
    "                'uncertainty': 0.0  # Sin incertidumbre\n",
    "            })\n",
    "    \n",
    "    return apply_nms(detections, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"M√©todo 1: Baseline definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64328901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©todo 3: MC-Dropout definido\n"
     ]
    }
   ],
   "source": [
    "def inference_mc_dropout(model, image_path, text_prompt, conf_thresh, device, K=5):\n",
    "    \"\"\"M√©todo 3: MC-Dropout con K pases\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.train()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    h, w = image_source.shape[:2]\n",
    "    \n",
    "    all_detections_k = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in range(K):\n",
    "            boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "            \n",
    "            if len(boxes) == 0:\n",
    "                all_detections_k.append([])\n",
    "                continue\n",
    "            \n",
    "            boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "            \n",
    "            dets_k = []\n",
    "            for box, score, phrase in zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases):\n",
    "                cat = normalize_label(phrase)\n",
    "                if cat in CONFIG['categories']:\n",
    "                    score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "                    dets_k.append({\n",
    "                        'bbox': box.tolist(),\n",
    "                        'score': score_clipped,\n",
    "                        'category': cat\n",
    "                    })\n",
    "            all_detections_k.append(dets_k)\n",
    "    \n",
    "    # Alinear detecciones entre pases\n",
    "    if len(all_detections_k) == 0 or all(len(d) == 0 for d in all_detections_k):\n",
    "        return []\n",
    "    \n",
    "    # Usar primer pase como referencia\n",
    "    ref_dets = all_detections_k[0]\n",
    "    \n",
    "    aggregated = []\n",
    "    for ref_det in ref_dets:\n",
    "        scores_aligned = [ref_det['score']]\n",
    "        \n",
    "        for k in range(1, K):\n",
    "            best_iou = 0\n",
    "            best_score = None\n",
    "            for det_k in all_detections_k[k]:\n",
    "                if det_k['category'] != ref_det['category']:\n",
    "                    continue\n",
    "                iou = compute_iou(ref_det['bbox'], det_k['bbox'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_score = det_k['score']\n",
    "            \n",
    "            if best_iou >= 0.5 and best_score is not None:\n",
    "                scores_aligned.append(best_score)\n",
    "        \n",
    "        mean_score = np.mean(scores_aligned)\n",
    "        variance = np.var(scores_aligned) if len(scores_aligned) > 1 else 0.0\n",
    "        \n",
    "        mean_score_clipped = np.clip(mean_score, 1e-7, 1 - 1e-7)\n",
    "        logit = np.log(mean_score_clipped / (1 - mean_score_clipped))\n",
    "        \n",
    "        aggregated.append({\n",
    "            'bbox': ref_det['bbox'],\n",
    "            'score': mean_score_clipped,\n",
    "            'logit': logit,\n",
    "            'category': ref_det['category'],\n",
    "            'uncertainty': variance\n",
    "        })\n",
    "    \n",
    "    return apply_nms(aggregated, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"M√©todo 3: MC-Dropout definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be0a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©todo 5: Decoder variance definido (con layer_uncertainties)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 5: Funci√≥n inference_decoder_variance (MODIFICADA)\n",
    "\n",
    "def inference_decoder_variance(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"M√©todo 5: Varianza entre capas del decoder (single-pass)\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.eval()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    h, w = image_source.shape[:2]\n",
    "    \n",
    "    # Hook para capturar logits de cada capa del decoder\n",
    "    layer_logits = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # Capturar salida de cada capa del decoder\n",
    "        if isinstance(output, tuple) and len(output) > 0:\n",
    "            layer_logits.append(output[0].detach() if hasattr(output[0], 'detach') else output[0])\n",
    "        elif hasattr(output, 'detach'):\n",
    "            layer_logits.append(output.detach())\n",
    "    \n",
    "    # Registrar hooks en capas del decoder\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Buscar m√≥dulos como 'transformer.decoder.layers.0', 'transformer.decoder.layers.1', etc.\n",
    "        if 'decoder.layers' in name and name.count('.') == 3 and name.split('.')[-1].isdigit():\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Inferencia\n",
    "    boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "    \n",
    "    # Remover hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for idx, (box, score, phrase) in enumerate(zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases)):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat in CONFIG['categories']:\n",
    "            score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            \n",
    "            # Calcular varianza entre capas si disponible\n",
    "            uncertainty = 0.0\n",
    "            layer_uncertainties_list = []\n",
    "            \n",
    "            if len(layer_logits) > 0:\n",
    "                # Los layer_logits son embeddings [900, 1, 256]\n",
    "                # Necesitamos extraer el embedding de esta detecci√≥n (query idx)\n",
    "                # y calcular similitud/score para cada capa\n",
    "                layer_scores = []\n",
    "                \n",
    "                for layer_emb in layer_logits:\n",
    "                    # layer_emb shape: [num_queries, batch, embed_dim]\n",
    "                    # Extraer embedding de esta query (detecci√≥n)\n",
    "                    if idx < layer_emb.shape[0]:\n",
    "                        query_emb = layer_emb[idx, 0, :]  # [256]\n",
    "                        # Calcular score basado en norma del embedding\n",
    "                        # (queries m√°s confiables tienen normas m√°s altas)\n",
    "                        emb_norm = torch.norm(query_emb).item()\n",
    "                        # Normalizar a [0, 1] aproximadamente\n",
    "                        layer_score = 1.0 / (1.0 + np.exp(-emb_norm / 10.0))\n",
    "                        layer_scores.append(layer_score)\n",
    "                \n",
    "                if len(layer_scores) > 1:\n",
    "                    uncertainty = np.var(layer_scores)\n",
    "                    layer_uncertainties_list = layer_scores\n",
    "            \n",
    "            detections.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'category': cat,\n",
    "                'uncertainty': uncertainty,\n",
    "                'layer_uncertainties': layer_uncertainties_list,  # ‚úÖ NUEVO: Incertidumbres por capa\n",
    "                'layer_count': len(layer_uncertainties_list)  # ‚úÖ NUEVO: N√∫mero de capas capturadas\n",
    "            })\n",
    "    \n",
    "    return apply_nms(detections, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"M√©todo 5: Decoder variance definido (con layer_uncertainties)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7d22b",
   "metadata": {},
   "source": [
    "## 4. Inferencia en val_calib para Ajustar Temperaturas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f6fb8",
   "metadata": {},
   "source": [
    "### 4.1 Estrategia de Optimizaci√≥n\n",
    "\n",
    "**Si hay predicciones cacheadas**:\n",
    "- Baseline ‚Üí Se cargan de Fase 2\n",
    "- MC-Dropout ‚Üí Se cargan de Fase 3\n",
    "- Solo se calcula Decoder Variance (m√©todo nuevo)\n",
    "\n",
    "**Si NO hay predicciones cacheadas**:\n",
    "- Se ejecuta inferencia completa para todos los m√©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f931a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "üìä ESTRATEGIA DE SPLITS:\n",
      "  Dataset: val_eval.json (2,000 im√°genes)\n",
      "  ‚îú‚îÄ Calibraci√≥n: 500 im√°genes (primeras 500)\n",
      "  ‚îî‚îÄ Evaluaci√≥n:  1500 im√°genes (restantes 1,500)\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Procesando 500 im√°genes para ajustar temperaturas...\n",
      "\n",
      "‚úÖ Usando predicciones Baseline cacheadas de Fase 2\n",
      "   ‚Üí 1988 im√°genes indexadas\n",
      "‚úÖ Usando predicciones MC-Dropout cacheadas de Fase 3\n",
      "   ‚Üí 1996 im√°genes indexadas\n",
      "\n",
      "üîç OVERLAP CON CALIBRACI√ìN (primeras 500 de val_eval):\n",
      "   Baseline cacheado: 497/500 im√°genes (99.4%)\n",
      "   MC-Dropout cacheado: 498/500 im√°genes (99.6%)\n",
      "   ‚ö†Ô∏è  3 im√°genes de baseline se calcular√°n desde cero\n",
      "   ‚ö†Ô∏è  2 im√°genes de MC-Dropout se calcular√°n desde cero\n",
      "   ‚è±Ô∏è  Tiempo estimado: ~0.1 minutos\n"
     ]
    }
   ],
   "source": [
    "val_eval_json = DATA_DIR / 'bdd100k_coco/val_eval.json'\n",
    "image_dir = DATA_DIR / 'bdd100k/bdd100k/bdd100k/images/100k/val'\n",
    "\n",
    "coco_eval_full = COCO(str(val_eval_json))\n",
    "img_ids_all = coco_eval_full.getImgIds()\n",
    "\n",
    "# Split inteligente de val_eval (2000 im√°genes):\n",
    "# - Primeras 500 para calibraci√≥n (ajustar temperaturas)\n",
    "# - Restantes 1500 para evaluaci√≥n final\n",
    "img_ids_calib = img_ids_all[:500]\n",
    "img_ids_eval_final = img_ids_all[500:]\n",
    "\n",
    "print(f\"üìä ESTRATEGIA DE SPLITS:\")\n",
    "print(f\"  Dataset: val_eval.json (2,000 im√°genes)\")\n",
    "print(f\"  ‚îú‚îÄ Calibraci√≥n: {len(img_ids_calib)} im√°genes (primeras 500)\")\n",
    "print(f\"  ‚îî‚îÄ Evaluaci√≥n:  {len(img_ids_eval_final)} im√°genes (restantes 1,500)\")\n",
    "\n",
    "# Crear COCO object para calibraci√≥n\n",
    "coco_calib = COCO(str(val_eval_json))\n",
    "\n",
    "print(f\"\\nProcesando {len(img_ids_calib)} im√°genes para ajustar temperaturas...\")\n",
    "\n",
    "methods_calib_data = {\n",
    "    'baseline': [],\n",
    "    'mc_dropout': [],\n",
    "    'decoder_variance': []\n",
    "}\n",
    "\n",
    "# Contadores para diagn√≥stico\n",
    "counters = {\n",
    "    'baseline_cached': 0,\n",
    "    'baseline_computed': 0,\n",
    "    'mc_cached': 0,\n",
    "    'mc_computed': 0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZACI√ìN: Usar predicciones cacheadas si est√°n disponibles\n",
    "# ============================================================================\n",
    "\n",
    "# Convertir predicciones cacheadas a formato √∫til\n",
    "baseline_by_img = {}\n",
    "mc_by_img = {}\n",
    "\n",
    "if cached_predictions['baseline']:\n",
    "    print(\"\\n‚úÖ Usando predicciones Baseline cacheadas de Fase 2\")\n",
    "    baseline_by_img = convert_baseline_predictions(cached_predictions['baseline'], {})\n",
    "    print(f\"   ‚Üí {len(baseline_by_img)} im√°genes indexadas\")\n",
    "    \n",
    "if cached_predictions['mc_dropout']:\n",
    "    print(\"‚úÖ Usando predicciones MC-Dropout cacheadas de Fase 3\")\n",
    "    mc_by_img = convert_mc_predictions(cached_predictions['mc_dropout'], {})\n",
    "    print(f\"   ‚Üí {len(mc_by_img)} im√°genes indexadas\")\n",
    "\n",
    "# Verificar overlap con primeras 500 im√°genes de val_eval\n",
    "calib_500 = set(img_ids_calib)\n",
    "baseline_overlap = set(baseline_by_img.keys()) & calib_500\n",
    "mc_overlap = set(mc_by_img.keys()) & calib_500\n",
    "\n",
    "print(f\"\\nüîç OVERLAP CON CALIBRACI√ìN (primeras 500 de val_eval):\")\n",
    "print(f\"   Baseline cacheado: {len(baseline_overlap)}/500 im√°genes ({len(baseline_overlap)/500*100:.1f}%)\")\n",
    "print(f\"   MC-Dropout cacheado: {len(mc_overlap)}/500 im√°genes ({len(mc_overlap)/500*100:.1f}%)\")\n",
    "\n",
    "if len(baseline_overlap) < 500:\n",
    "    print(f\"   ‚ö†Ô∏è  {500 - len(baseline_overlap)} im√°genes de baseline se calcular√°n desde cero\")\n",
    "if len(mc_overlap) < 500:\n",
    "    print(f\"   ‚ö†Ô∏è  {500 - len(mc_overlap)} im√°genes de MC-Dropout se calcular√°n desde cero\")\n",
    "    print(f\"   ‚è±Ô∏è  Tiempo estimado: ~{(500 - len(mc_overlap)) * 1.8 / 60:.1f} minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a5727",
   "metadata": {},
   "source": [
    "### üêõ DEBUG: Probar inference_decoder_variance con 1 imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f95cfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêõ DEBUG: Probando inference_decoder_variance con una imagen...\n",
      "   Imagen de prueba: c8c97803-657086fb.jpg (ID: 9306)\n",
      "   Ruta: ../data/bdd100k/bdd100k/bdd100k/images/100k/val/c8c97803-657086fb.jpg\n",
      "   Existe: True\n",
      "\n",
      "üîç Ejecutando inference_decoder_variance...\n",
      "\n",
      "üìä RESULTADOS:\n",
      "   Detecciones: 22\n",
      "\n",
      "   Primera detecci√≥n:\n",
      "      bbox: [246.6544952392578, 344.82891845703125, 390.66827392578125, 422.9021301269531]\n",
      "      score: 0.6032488942146301\n",
      "      logit: 0.4190207249455299\n",
      "      category: car\n",
      "      uncertainty: 0.0012345988001037553\n",
      "      layer_uncertainties: [0.7025667871442604, 0.7570377492065832, 0.7397240831970635, 0.7811286757848407, 0.8028580751556345, 0.7985424860655854]\n",
      "      layer_count: 6\n",
      "\n",
      "   ‚úÖ layer_uncertainties presente: 6 valores\n",
      "      Valores: [0.7025667871442604, 0.7570377492065832, 0.7397240831970635, 0.7811286757848407, 0.8028580751556345, 0.7985424860655854]\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'corrido'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üêõ CELDA DE DEBUG - Ejecutar solo UNA imagen para diagnosticar hooks\n",
    "\n",
    "print(\"üêõ DEBUG: Probando inference_decoder_variance con una imagen...\")\n",
    "\n",
    "# Tomar la primera imagen de calibraci√≥n\n",
    "test_img_id = img_ids_calib[0]\n",
    "test_img_info = coco_calib.loadImgs(test_img_id)[0]\n",
    "test_img_path = image_dir / test_img_info['file_name']\n",
    "\n",
    "print(f\"   Imagen de prueba: {test_img_info['file_name']} (ID: {test_img_id})\")\n",
    "print(f\"   Ruta: {test_img_path}\")\n",
    "print(f\"   Existe: {test_img_path.exists()}\")\n",
    "\n",
    "if test_img_path.exists():\n",
    "    print(f\"\\nüîç Ejecutando inference_decoder_variance...\")\n",
    "    test_preds = inference_decoder_variance(model, test_img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS:\")\n",
    "    print(f\"   Detecciones: {len(test_preds)}\")\n",
    "    \n",
    "    if len(test_preds) > 0:\n",
    "        print(f\"\\n   Primera detecci√≥n:\")\n",
    "        first_pred = test_preds[0]\n",
    "        for key, value in first_pred.items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "        \n",
    "        # Verificar layer_uncertainties\n",
    "        if 'layer_uncertainties' in first_pred:\n",
    "            layer_unc = first_pred['layer_uncertainties']\n",
    "            print(f\"\\n   ‚úÖ layer_uncertainties presente: {len(layer_unc)} valores\")\n",
    "            if len(layer_unc) > 0:\n",
    "                print(f\"      Valores: {layer_unc}\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è  VAC√çO - Este es el problema que debemos solucionar\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ùå layer_uncertainties NO est√° en la salida\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No se detectaron objetos en esta imagen\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Imagen no encontrada\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\"corrido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059cc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando calibraci√≥n: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:50<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ESTAD√çSTICAS DE PROCESAMIENTO:\n",
      "   Baseline: 497 cacheadas, 3 calculadas\n",
      "   MC-Dropout: 498 cacheadas, 2 calculadas\n",
      "\n",
      "baseline: 5457 detecciones, TP=3558\n",
      "\n",
      "mc_dropout: 7387 detecciones, TP=51\n",
      "\n",
      "decoder_variance: 7453 detecciones, TP=4336\n",
      "\n",
      "‚úÖ Datos de calibraci√≥n guardados\n",
      "\n",
      "üîç DIAGN√ìSTICO FINAL - Verificando CSVs de calibraci√≥n:\n",
      "   Baseline: 5457 registros, uncertainty media=0.000000\n",
      "   MC-Dropout: 7387 registros, uncertainty media=0.000089\n",
      "\n",
      "   Primeras 5 logits de cada m√©todo:\n",
      "   Baseline:   [0.3380101929716149, -0.1017526020559519, 0.4190207249455299, -0.2801209492000665, -0.0834355056962531]\n",
      "   MC-Dropout: [-0.1081413067481472, -0.2560700530227012, -0.4148847215280034, -0.5178735333913581, -0.7217361723679339]\n",
      "   ‚úÖ Los logits son diferentes\n",
      "   ‚úÖ Las incertidumbres son diferentes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Procesar im√°genes de calibraci√≥n\n",
    "for img_id in tqdm(img_ids_calib, desc=\"Procesando calibraci√≥n\"):\n",
    "    img_info = coco_calib.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    gt_anns = coco_calib.loadAnns(coco_calib.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # M√©todo 1: Baseline\n",
    "    # ========================================================================\n",
    "    if img_id in baseline_by_img:\n",
    "        # Usar predicciones cacheadas\n",
    "        preds_baseline = baseline_by_img[img_id]\n",
    "        counters['baseline_cached'] += 1\n",
    "    else:\n",
    "        # Calcular desde cero\n",
    "        counters['baseline_computed'] += 1\n",
    "        preds_baseline_raw = inference_baseline(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "        preds_baseline = []\n",
    "        for pred in preds_baseline_raw:\n",
    "            cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "            preds_baseline.append({\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': pred['score'],\n",
    "                'logit': pred['logit'],\n",
    "                'category_id': cat_id,\n",
    "                'uncertainty': pred['uncertainty']\n",
    "            })\n",
    "    \n",
    "    # Etiquetar como TP/FP\n",
    "    for pred in preds_baseline:\n",
    "        is_tp = 0\n",
    "        cat_id = pred['category_id']\n",
    "        cat = CONFIG['categories'][cat_id - 1] if 1 <= cat_id <= len(CONFIG['categories']) else ''\n",
    "        \n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_calib_data['baseline'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': cat,\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # M√©todo 3: MC-Dropout\n",
    "    # ========================================================================\n",
    "    if img_id in mc_by_img:\n",
    "        # Usar predicciones cacheadas\n",
    "        preds_mc = mc_by_img[img_id]\n",
    "        counters['mc_cached'] += 1\n",
    "    else:\n",
    "        # Calcular desde cero\n",
    "        counters['mc_computed'] += 1\n",
    "        preds_mc_raw = inference_mc_dropout(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'], CONFIG['K_mc'])\n",
    "        preds_mc = []\n",
    "        for pred in preds_mc_raw:\n",
    "            cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "            preds_mc.append({\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': pred['score'],\n",
    "                'logit': pred['logit'],\n",
    "                'category_id': cat_id,\n",
    "                'uncertainty': pred['uncertainty']\n",
    "            })\n",
    "    \n",
    "    # Etiquetar como TP/FP\n",
    "    for pred in preds_mc:\n",
    "        is_tp = 0\n",
    "        cat_id = pred['category_id']\n",
    "        cat = CONFIG['categories'][cat_id - 1] if 1 <= cat_id <= len(CONFIG['categories']) else ''\n",
    "        \n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_calib_data['mc_dropout'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': cat,\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # M√©todo 5: Decoder Variance (siempre se calcula, es nuevo)\n",
    "    # ========================================================================\n",
    "    preds_dec = inference_decoder_variance(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_dec:\n",
    "        is_tp = 0\n",
    "        cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "        cat = pred['category']\n",
    "        \n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_calib_data['decoder_variance'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': cat,\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "\n",
    "# Mostrar contadores\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE PROCESAMIENTO:\")\n",
    "print(f\"   Baseline: {counters['baseline_cached']} cacheadas, {counters['baseline_computed']} calculadas\")\n",
    "print(f\"   MC-Dropout: {counters['mc_cached']} cacheadas, {counters['mc_computed']} calculadas\")\n",
    "\n",
    "# Guardar datos de calibraci√≥n\n",
    "for method_name, data in methods_calib_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(OUTPUT_DIR / f'calib_{method_name}.csv', index=False)\n",
    "    print(f\"\\n{method_name}: {len(df)} detecciones, TP={df['is_tp'].sum()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Datos de calibraci√≥n guardados\")\n",
    "\n",
    "# DIAGN√ìSTICO FINAL: Verificar si los CSVs son diferentes\n",
    "print(f\"\\nüîç DIAGN√ìSTICO FINAL - Verificando CSVs de calibraci√≥n:\")\n",
    "df_baseline = pd.read_csv(OUTPUT_DIR / 'calib_baseline.csv')\n",
    "df_mc = pd.read_csv(OUTPUT_DIR / 'calib_mc_dropout.csv')\n",
    "print(f\"   Baseline: {len(df_baseline)} registros, uncertainty media={df_baseline['uncertainty'].mean():.6f}\")\n",
    "print(f\"   MC-Dropout: {len(df_mc)} registros, uncertainty media={df_mc['uncertainty'].mean():.6f}\")\n",
    "\n",
    "# Comparar primeras 5 logits\n",
    "print(f\"\\n   Primeras 5 logits de cada m√©todo:\")\n",
    "print(f\"   Baseline:   {df_baseline['logit'].head().tolist()}\")\n",
    "print(f\"   MC-Dropout: {df_mc['logit'].head().tolist()}\")\n",
    "\n",
    "if df_baseline['logit'].head(10).equals(df_mc['logit'].head(10)):\n",
    "    print(f\"   ‚ö†Ô∏è  Los primeros 10 logits son id√©nticos (puede ser coincidencia o problema)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Los logits son diferentes\")\n",
    "    \n",
    "if df_baseline['uncertainty'].equals(df_mc['uncertainty']):\n",
    "    print(f\"   ‚ö†Ô∏è  Las incertidumbres son id√©nticas\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Las incertidumbres son diferentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5a229",
   "metadata": {},
   "source": [
    "## 5. Optimizar Temperaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281ae767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Calculando temperaturas desde cero...\n",
      "  mc_dropout: T=0.3192, NLL: 0.5123 ‚Üí 0.4001\n",
      "  decoder_variance: T=2.6534, NLL: 0.7061 ‚Üí 0.6850\n",
      "‚öôÔ∏è  Calculando temperatura para baseline...\n",
      "  baseline: T=4.2128, NLL: 0.7107 ‚Üí 0.6912\n",
      "\n",
      "‚úÖ Temperaturas guardadas en: outputs/comparison/temperatures.json\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def nll_loss(T, logits, labels):\n",
    "    T = max(T, 0.01)\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZACI√ìN: Usar temperaturas de Fase 4 si est√°n disponibles\n",
    "# ============================================================================\n",
    "\n",
    "if cached_predictions['temperatures'] and 'optimal_temperature' in cached_predictions['temperatures']:\n",
    "    print(\"‚úÖ Usando temperatura optimizada de Fase 4\")\n",
    "    T_baseline = cached_predictions['temperatures']['optimal_temperature']\n",
    "    \n",
    "    # Calcular NLL antes y despu√©s con esta temperatura\n",
    "    df_baseline = pd.read_csv(OUTPUT_DIR / 'calib_baseline.csv')\n",
    "    logits_baseline = df_baseline['logit'].values\n",
    "    labels_baseline = df_baseline['is_tp'].values\n",
    "    \n",
    "    nll_before = nll_loss(1.0, logits_baseline, labels_baseline)\n",
    "    nll_after = nll_loss(T_baseline, logits_baseline, labels_baseline)\n",
    "    \n",
    "    temperatures = {\n",
    "        'baseline': {\n",
    "            'T': T_baseline,\n",
    "            'nll_before': nll_before,\n",
    "            'nll_after': nll_after,\n",
    "            'source': 'cached_from_fase4'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"  baseline: T={T_baseline:.4f}, NLL: {nll_before:.4f} ‚Üí {nll_after:.4f} (cacheada)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚öôÔ∏è  Calculando temperaturas desde cero...\")\n",
    "    temperatures = {}\n",
    "\n",
    "# Calcular temperaturas para MC-Dropout y Decoder Variance (siempre, son nuevos)\n",
    "for method_name in ['mc_dropout', 'decoder_variance']:\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'calib_{method_name}.csv')\n",
    "    logits = df['logit'].values\n",
    "    labels = df['is_tp'].values\n",
    "    \n",
    "    nll_before = nll_loss(1.0, logits, labels)\n",
    "    result = minimize(lambda T: nll_loss(T, logits, labels), x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "    T_opt = result.x[0]\n",
    "    nll_after = result.fun\n",
    "    \n",
    "    temperatures[method_name] = {\n",
    "        'T': T_opt,\n",
    "        'nll_before': nll_before,\n",
    "        'nll_after': nll_after,\n",
    "        'source': 'calculated'\n",
    "    }\n",
    "    \n",
    "    print(f\"  {method_name}: T={T_opt:.4f}, NLL: {nll_before:.4f} ‚Üí {nll_after:.4f}\")\n",
    "\n",
    "# Si no hab√≠a temperatura cacheada para baseline, calcularla\n",
    "if 'baseline' not in temperatures:\n",
    "    print(\"‚öôÔ∏è  Calculando temperatura para baseline...\")\n",
    "    df = pd.read_csv(OUTPUT_DIR / 'calib_baseline.csv')\n",
    "    logits = df['logit'].values\n",
    "    labels = df['is_tp'].values\n",
    "    \n",
    "    nll_before = nll_loss(1.0, logits, labels)\n",
    "    result = minimize(lambda T: nll_loss(T, logits, labels), x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "    T_opt = result.x[0]\n",
    "    nll_after = result.fun\n",
    "    \n",
    "    temperatures['baseline'] = {\n",
    "        'T': T_opt,\n",
    "        'nll_before': nll_before,\n",
    "        'nll_after': nll_after,\n",
    "        'source': 'calculated'\n",
    "    }\n",
    "    \n",
    "    print(f\"  baseline: T={T_opt:.4f}, NLL: {nll_before:.4f} ‚Üí {nll_after:.4f}\")\n",
    "\n",
    "with open(OUTPUT_DIR / 'temperatures.json', 'w') as f:\n",
    "    json.dump(temperatures, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Temperaturas guardadas en: {OUTPUT_DIR / 'temperatures.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b5c79",
   "metadata": {},
   "source": [
    "## 6. Evaluaci√≥n en val_eval con COCO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45966d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUACI√ìN EN VAL_EVAL (1,500 im√°genes restantes)\n",
      "======================================================================\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Procesando 1500 im√°genes para evaluaci√≥n final...\n",
      "\n",
      "üìä Temperaturas a aplicar:\n",
      "   mc_dropout: T=0.3192\n",
      "   decoder_variance: T=2.6534\n",
      "   baseline: T=4.2128\n",
      "\n",
      "‚úÖ Indexando predicciones Baseline cacheadas para evaluaci√≥n\n",
      "   ‚Üí 1491 im√°genes con predicciones cacheadas\n",
      "‚úÖ Indexando predicciones MC-Dropout cacheadas para evaluaci√≥n\n",
      "   ‚Üí 1498 im√°genes con predicciones cacheadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando evaluaci√≥n: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [08:37<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ESTAD√çSTICAS DE EVALUACI√ìN:\n",
      "   Baseline: 1491 cacheadas, 9 calculadas\n",
      "   MC-Dropout: 1498 cacheadas, 2 calculadas\n",
      "\n",
      "baseline: 16724 detecciones\n",
      "\n",
      "baseline_ts: 16724 detecciones\n",
      "\n",
      "mc_dropout: 22527 detecciones\n",
      "\n",
      "mc_dropout_ts: 22527 detecciones\n",
      "\n",
      "decoder_variance: 22793 detecciones\n",
      "\n",
      "decoder_variance_ts: 22793 detecciones\n",
      "\n",
      "‚úÖ Resultados de evaluaci√≥n guardados (CSV + JSON)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ EJECUTAR PARA RQ1 - Celda 6: Ejecutar inferencia en val_eval (1500 im√°genes)\n",
    "# Esta celda ejecuta decoder_variance en TODAS las im√°genes de evaluaci√≥n\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUACI√ìN: Usar restantes 1500 im√°genes de val_eval\n",
    "# ============================================================================\n",
    "# Las primeras 500 se usaron para calibraci√≥n, ahora usamos el resto\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUACI√ìN EN VAL_EVAL (1,500 im√°genes restantes)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "coco_eval = COCO(str(val_eval_json))\n",
    "\n",
    "print(f\"Procesando {len(img_ids_eval_final)} im√°genes para evaluaci√≥n final...\")\n",
    "\n",
    "methods_results = {\n",
    "    'baseline': [],\n",
    "    'baseline_ts': [],\n",
    "    'mc_dropout': [],\n",
    "    'mc_dropout_ts': [],\n",
    "    'decoder_variance': [],\n",
    "    'decoder_variance_ts': []\n",
    "}\n",
    "\n",
    "# Cargar temperaturas\n",
    "with open(OUTPUT_DIR / 'temperatures.json', 'r') as f:\n",
    "    temps = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Temperaturas a aplicar:\")\n",
    "for method, temp_info in temps.items():\n",
    "    print(f\"   {method}: T={temp_info['T']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZACI√ìN: Construir √≠ndices de predicciones cacheadas para evaluaci√≥n\n",
    "# ============================================================================\n",
    "\n",
    "baseline_eval_by_img = {}\n",
    "mc_eval_by_img = {}\n",
    "\n",
    "if cached_predictions['baseline']:\n",
    "    print(\"\\n‚úÖ Indexando predicciones Baseline cacheadas para evaluaci√≥n\")\n",
    "    for pred in cached_predictions['baseline']:\n",
    "        img_id = pred.get('image_id')\n",
    "        if img_id in img_ids_eval_final:  # Solo las de evaluaci√≥n (no calibraci√≥n)\n",
    "            if img_id not in baseline_eval_by_img:\n",
    "                baseline_eval_by_img[img_id] = []\n",
    "            baseline_eval_by_img[img_id].append(pred)\n",
    "    print(f\"   ‚Üí {len(baseline_eval_by_img)} im√°genes con predicciones cacheadas\")\n",
    "\n",
    "if cached_predictions['mc_dropout']:\n",
    "    print(\"‚úÖ Indexando predicciones MC-Dropout cacheadas para evaluaci√≥n\")\n",
    "    for pred in cached_predictions['mc_dropout']:\n",
    "        img_id = pred.get('image_id')\n",
    "        if img_id in img_ids_eval_final:  # Solo las de evaluaci√≥n (no calibraci√≥n)\n",
    "            if img_id not in mc_eval_by_img:\n",
    "                mc_eval_by_img[img_id] = []\n",
    "            mc_eval_by_img[img_id].append(pred)\n",
    "    print(f\"   ‚Üí {len(mc_eval_by_img)} im√°genes con predicciones cacheadas\")\n",
    "\n",
    "# Contadores\n",
    "eval_counters = {\n",
    "    'baseline_cached': 0,\n",
    "    'baseline_computed': 0,\n",
    "    'mc_cached': 0,\n",
    "    'mc_computed': 0\n",
    "}\n",
    "\n",
    "# Procesar im√°genes de evaluaci√≥n\n",
    "for img_id in tqdm(img_ids_eval_final, desc=\"Procesando evaluaci√≥n\"):\n",
    "    img_info = coco_eval.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    gt_anns = coco_eval.loadAnns(coco_eval.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Baseline (sin TS y con TS)\n",
    "    # ========================================================================\n",
    "    if img_id in baseline_eval_by_img:\n",
    "        # Usar cacheadas\n",
    "        eval_counters['baseline_cached'] += 1\n",
    "        for pred in baseline_eval_by_img[img_id]:\n",
    "            is_tp = 0\n",
    "            bbox = pred['bbox']\n",
    "            bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "            \n",
    "            for gt in gt_anns:\n",
    "                if gt['category_id'] != pred['category_id']:\n",
    "                    continue\n",
    "                gt_box = gt['bbox']\n",
    "                gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "                if compute_iou(bbox_xyxy, gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                    is_tp = 1\n",
    "                    break\n",
    "            \n",
    "            score = pred['score']\n",
    "            score_clipped = np.clip(score, 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            \n",
    "            methods_results['baseline'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': pred['category_id'],\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'uncertainty': pred.get('uncertainty', 0.0),\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "            \n",
    "            # Con TS\n",
    "            score_ts = sigmoid(logit / temps['baseline']['T'])\n",
    "            methods_results['baseline_ts'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': pred['category_id'],\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': score_ts,\n",
    "                'logit': logit,\n",
    "                'uncertainty': pred.get('uncertainty', 0.0),\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "    else:\n",
    "        # Calcular desde cero\n",
    "        eval_counters['baseline_computed'] += 1\n",
    "        preds_baseline = inference_baseline(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "        for pred in preds_baseline:\n",
    "            cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "            is_tp = 0\n",
    "            for gt in gt_anns:\n",
    "                if gt['category_id'] != cat_id:\n",
    "                    continue\n",
    "                gt_box = gt['bbox']\n",
    "                gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "                if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                    is_tp = 1\n",
    "                    break\n",
    "            \n",
    "            methods_results['baseline'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cat_id,\n",
    "                'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "                'score': pred['score'],\n",
    "                'logit': pred['logit'],\n",
    "                'uncertainty': pred['uncertainty'],\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "            \n",
    "            score_ts = sigmoid(pred['logit'] / temps['baseline']['T'])\n",
    "            methods_results['baseline_ts'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cat_id,\n",
    "                'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "                'score': score_ts,\n",
    "                'logit': pred['logit'],\n",
    "                'uncertainty': pred['uncertainty'],\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MC-Dropout (sin TS y con TS)\n",
    "    # ========================================================================\n",
    "    if img_id in mc_eval_by_img:\n",
    "        # Usar cacheadas\n",
    "        eval_counters['mc_cached'] += 1\n",
    "        for pred in mc_eval_by_img[img_id]:\n",
    "            is_tp = 0\n",
    "            bbox = pred['bbox']\n",
    "            bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "            \n",
    "            for gt in gt_anns:\n",
    "                if gt['category_id'] != pred['category_id']:\n",
    "                    continue\n",
    "                gt_box = gt['bbox']\n",
    "                gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "                if compute_iou(bbox_xyxy, gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                    is_tp = 1\n",
    "                    break\n",
    "            \n",
    "            score = pred['score']\n",
    "            score_clipped = np.clip(score, 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            \n",
    "            methods_results['mc_dropout'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': pred['category_id'],\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'uncertainty': pred.get('uncertainty', 0.0),\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "            \n",
    "            # Con TS\n",
    "            score_ts = sigmoid(logit / temps['mc_dropout']['T'])\n",
    "            methods_results['mc_dropout_ts'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': pred['category_id'],\n",
    "                'bbox': pred['bbox'],\n",
    "                'score': score_ts,\n",
    "                'logit': logit,\n",
    "                'uncertainty': pred.get('uncertainty', 0.0),\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "    else:\n",
    "        # Calcular desde cero\n",
    "        eval_counters['mc_computed'] += 1\n",
    "        preds_mc = inference_mc_dropout(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'], CONFIG['K_mc'])\n",
    "        for pred in preds_mc:\n",
    "            cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "            is_tp = 0\n",
    "            for gt in gt_anns:\n",
    "                if gt['category_id'] != cat_id:\n",
    "                    continue\n",
    "                gt_box = gt['bbox']\n",
    "                gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "                if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                    is_tp = 1\n",
    "                    break\n",
    "            \n",
    "            methods_results['mc_dropout'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cat_id,\n",
    "                'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "                'score': pred['score'],\n",
    "                'logit': pred['logit'],\n",
    "                'uncertainty': pred['uncertainty'],\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "            \n",
    "            score_ts = sigmoid(pred['logit'] / temps['mc_dropout']['T'])\n",
    "            methods_results['mc_dropout_ts'].append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': cat_id,\n",
    "                'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "                'score': score_ts,\n",
    "                'logit': pred['logit'],\n",
    "                'uncertainty': pred['uncertainty'],\n",
    "                'is_tp': is_tp\n",
    "            })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Decoder variance (siempre se calcula, es nuevo)\n",
    "    # ========================================================================\n",
    "    preds_dec = inference_decoder_variance(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_dec:\n",
    "        cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_results['decoder_variance'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': pred['score'],\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'layer_uncertainties': pred.get('layer_uncertainties', []),\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "        \n",
    "        score_ts = sigmoid(pred['logit'] / temps['decoder_variance']['T'])\n",
    "        methods_results['decoder_variance_ts'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': score_ts,\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'layer_uncertainties': pred.get('layer_uncertainties', []),\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE EVALUACI√ìN:\")\n",
    "print(f\"   Baseline: {eval_counters['baseline_cached']} cacheadas, {eval_counters['baseline_computed']} calculadas\")\n",
    "print(f\"   MC-Dropout: {eval_counters['mc_cached']} cacheadas, {eval_counters['mc_computed']} calculadas\")\n",
    "\n",
    "# Guardar resultados (CSV y JSON)\n",
    "for method_name, results in methods_results.items():\n",
    "    # Guardar CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_DIR / f'eval_{method_name}.csv', index=False)\n",
    "    \n",
    "    # Guardar JSON (formato compatible con RQ1)\n",
    "    with open(OUTPUT_DIR / f'eval_{method_name}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{method_name}: {len(df)} detecciones\")\n",
    "\n",
    "print(f\"\\n‚úÖ Resultados de evaluaci√≥n guardados (CSV + JSON)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d072e6e",
   "metadata": {},
   "source": [
    "## 7. Calcular M√©tricas de Detecci√≥n (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ec63fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando baseline...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.50s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.55s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.170\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.171\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.063\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.284\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.285\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.36s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.58s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.17s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.50s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.09s).\n",
      "\n",
      "Evaluando baseline_ts...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.41s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.54s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.170\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.171\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.063\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.284\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.285\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.53s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.16s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.06s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.59s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.49s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.09s).\n",
      "\n",
      "Evaluando mc_dropout...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.32s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.94s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.67s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.181\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.43s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.94s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.66s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.63s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      "\n",
      "Evaluando mc_dropout_ts...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.64s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.181\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.44s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.91s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.68s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.13s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      "\n",
      "Evaluando decoder_variance...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.14s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.63s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.180\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.40s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.99s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.66s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.56s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      "\n",
      "Evaluando decoder_variance_ts...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.32s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.77s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.64s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.180\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.42s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.87s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.06s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.73s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.13s).\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.58s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      "\n",
      "M√©tricas de detecci√≥n guardadas\n"
     ]
    }
   ],
   "source": [
    "detection_metrics = {}\n",
    "\n",
    "for method_name in methods_results.keys():\n",
    "    print(f\"\\nEvaluando {method_name}...\")\n",
    "    \n",
    "    # Cargar predicciones en formato COCO\n",
    "    preds_file = OUTPUT_DIR / f'eval_{method_name}.json'\n",
    "    \n",
    "    if os.path.getsize(preds_file) > 0:\n",
    "        coco_dt = coco_eval.loadRes(str(preds_file))\n",
    "        coco_eval_obj = COCOeval(coco_eval, coco_dt, 'bbox')\n",
    "        coco_eval_obj.evaluate()\n",
    "        coco_eval_obj.accumulate()\n",
    "        coco_eval_obj.summarize()\n",
    "        \n",
    "        detection_metrics[method_name] = {\n",
    "            'mAP': coco_eval_obj.stats[0],\n",
    "            'AP50': coco_eval_obj.stats[1],\n",
    "            'AP75': coco_eval_obj.stats[2],\n",
    "            'AP_small': coco_eval_obj.stats[3],\n",
    "            'AP_medium': coco_eval_obj.stats[4],\n",
    "            'AP_large': coco_eval_obj.stats[5]\n",
    "        }\n",
    "        \n",
    "        # mAP por clase\n",
    "        per_class_ap = {}\n",
    "        for cat_id, cat_name in enumerate(CONFIG['categories'], 1):\n",
    "            coco_eval_obj.params.catIds = [cat_id]\n",
    "            coco_eval_obj.evaluate()\n",
    "            coco_eval_obj.accumulate()\n",
    "            per_class_ap[cat_name] = coco_eval_obj.stats[0]\n",
    "        \n",
    "        detection_metrics[method_name]['per_class'] = per_class_ap\n",
    "    else:\n",
    "        detection_metrics[method_name] = {'mAP': 0.0, 'AP50': 0.0, 'AP75': 0.0}\n",
    "\n",
    "with open(OUTPUT_DIR / 'detection_metrics.json', 'w') as f:\n",
    "    json.dump(detection_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nM√©tricas de detecci√≥n guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a3e0a",
   "metadata": {},
   "source": [
    "## 8. Tabla Comparativa de Detecci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c836a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLA COMPARATIVA DE DETECCI√ìN\n",
      "================================================================================\n",
      "             Method      mAP     AP50     AP75  AP_person   AP_car  AP_truck  AP_traffic_light  AP_traffic_sign\n",
      "           baseline 0.170481 0.278535 0.170542   0.170481 0.170481  0.170481          0.170481         0.170481\n",
      "        baseline_ts 0.170481 0.278535 0.170542   0.170481 0.170481  0.170481          0.170481         0.170481\n",
      "         mc_dropout 0.182274 0.302312 0.181113   0.182274 0.182274  0.182274          0.182274         0.182274\n",
      "      mc_dropout_ts 0.182274 0.302312 0.181113   0.182274 0.182274  0.182274          0.182274         0.182274\n",
      "   decoder_variance 0.181892 0.302048 0.180095   0.181892 0.181892  0.181892          0.181892         0.181892\n",
      "decoder_variance_ts 0.181892 0.302048 0.180095   0.181892 0.181892  0.181892          0.181892         0.181892\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR / 'detection_metrics.json', 'r') as f:\n",
    "    det_metrics = json.load(f)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "rows = []\n",
    "for method_name, metrics in det_metrics.items():\n",
    "    row = {\n",
    "        'Method': method_name,\n",
    "        'mAP': metrics.get('mAP', 0.0),\n",
    "        'AP50': metrics.get('AP50', 0.0),\n",
    "        'AP75': metrics.get('AP75', 0.0)\n",
    "    }\n",
    "    \n",
    "    # Agregar mAP por clase principal\n",
    "    if 'per_class' in metrics:\n",
    "        for cat in ['person', 'car', 'truck', 'traffic_light', 'traffic_sign']:\n",
    "            cat_key = cat.replace('_', ' ')\n",
    "            row[f'AP_{cat}'] = metrics['per_class'].get(cat_key, 0.0)\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "df_detection = pd.DataFrame(rows)\n",
    "df_detection.to_csv(OUTPUT_DIR / 'detection_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE DETECCI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(df_detection.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441867bd",
   "metadata": {},
   "source": [
    "## 9. Calcular M√©tricas de Calibraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5662f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: NLL=0.7180, Brier=0.2618, ECE=0.2410\n",
      "baseline_ts: NLL=0.6930, Brier=0.2499, ECE=0.1868\n",
      "mc_dropout: NLL=0.7069, Brier=0.2561, ECE=0.2034\n",
      "mc_dropout_ts: NLL=1.0070, Brier=0.3365, ECE=0.3428\n",
      "decoder_variance: NLL=0.7093, Brier=0.2572, ECE=0.2065\n",
      "decoder_variance_ts: NLL=0.6863, Brier=0.2466, ECE=0.1409\n",
      "\n",
      "M√©tricas de calibraci√≥n guardadas\n"
     ]
    }
   ],
   "source": [
    "def compute_calibration_metrics(logits, labels, T=1.0, n_bins=10):\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # NLL\n",
    "    nll = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    \n",
    "    # Brier\n",
    "    brier = np.mean((probs - labels) ** 2)\n",
    "    \n",
    "    # ECE\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    digitized = np.digitize(probs, bins) - 1\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_data = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = digitized == i\n",
    "        if mask.sum() > 0:\n",
    "            conf = probs[mask].mean()\n",
    "            acc = labels[mask].mean()\n",
    "            gap = abs(conf - acc)\n",
    "            ece += gap * mask.sum() / len(probs)\n",
    "            bin_data.append({\n",
    "                'bin': i,\n",
    "                'confidence': conf,\n",
    "                'accuracy': acc,\n",
    "                'count': mask.sum()\n",
    "            })\n",
    "    \n",
    "    return {'NLL': nll, 'Brier': brier, 'ECE': ece, 'bin_data': bin_data}\n",
    "\n",
    "calibration_metrics = {}\n",
    "\n",
    "for method_name in methods_results.keys():\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    logits = df['logit'].values\n",
    "    labels = df['is_tp'].values\n",
    "    \n",
    "    # Sin TS (T=1)\n",
    "    if '_ts' not in method_name:\n",
    "        metrics = compute_calibration_metrics(logits, labels, T=1.0, n_bins=CONFIG['n_bins'])\n",
    "        calibration_metrics[method_name] = metrics\n",
    "    else:\n",
    "        # Con TS\n",
    "        base_method = method_name.replace('_ts', '')\n",
    "        T = temps[base_method]['T']\n",
    "        metrics = compute_calibration_metrics(logits, labels, T=T, n_bins=CONFIG['n_bins'])\n",
    "        calibration_metrics[method_name] = metrics\n",
    "    \n",
    "    print(f\"{method_name}: NLL={metrics['NLL']:.4f}, Brier={metrics['Brier']:.4f}, ECE={metrics['ECE']:.4f}\")\n",
    "\n",
    "# Guardar\n",
    "with open(OUTPUT_DIR / 'calibration_metrics.json', 'w') as f:\n",
    "    # Convertir para JSON serializable\n",
    "    cal_save = {}\n",
    "    for k, v in calibration_metrics.items():\n",
    "        cal_save[k] = {\n",
    "            'NLL': v['NLL'],\n",
    "            'Brier': v['Brier'],\n",
    "            'ECE': v['ECE']\n",
    "        }\n",
    "    json.dump(cal_save, f, indent=2)\n",
    "\n",
    "print(\"\\nM√©tricas de calibraci√≥n guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4cdd1",
   "metadata": {},
   "source": [
    "## 10. Tabla Comparativa de Calibraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db03397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLA COMPARATIVA DE CALIBRACI√ìN\n",
      "================================================================================\n",
      "             Method      NLL    Brier      ECE\n",
      "           baseline 0.718032 0.261844 0.240970\n",
      "        baseline_ts 0.693014 0.249935 0.186833\n",
      "         mc_dropout 0.706870 0.256100 0.203429\n",
      "      mc_dropout_ts 1.007017 0.336512 0.342814\n",
      "   decoder_variance 0.709267 0.257221 0.206473\n",
      "decoder_variance_ts 0.686261 0.246594 0.140935\n",
      "================================================================================\n",
      "\n",
      "Interpretaci√≥n:\n",
      "  ‚Üì Menor es mejor para NLL, Brier, ECE\n",
      "  Si m√©todo+TS < m√©todo: TS mejor√≥ calibraci√≥n\n"
     ]
    }
   ],
   "source": [
    "rows_calib = []\n",
    "for method_name, metrics in calibration_metrics.items():\n",
    "    rows_calib.append({\n",
    "        'Method': method_name,\n",
    "        'NLL': metrics['NLL'],\n",
    "        'Brier': metrics['Brier'],\n",
    "        'ECE': metrics['ECE']\n",
    "    })\n",
    "\n",
    "df_calibration = pd.DataFrame(rows_calib)\n",
    "df_calibration.to_csv(OUTPUT_DIR / 'calibration_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE CALIBRACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(df_calibration.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInterpretaci√≥n:\")\n",
    "print(\"  ‚Üì Menor es mejor para NLL, Brier, ECE\")\n",
    "print(\"  Si m√©todo+TS < m√©todo: TS mejor√≥ calibraci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0630882",
   "metadata": {},
   "source": [
    "## 11. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37663ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability diagrams guardados en: outputs/comparison/reliability_diagrams.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "method_pairs = [\n",
    "    ('baseline', 'baseline_ts'),\n",
    "    ('mc_dropout', 'mc_dropout_ts'),\n",
    "    ('decoder_variance', 'decoder_variance_ts')\n",
    "]\n",
    "\n",
    "for idx, (method_before, method_after) in enumerate(method_pairs):\n",
    "    ax = axes[idx * 2]\n",
    "    \n",
    "    # Sin TS\n",
    "    bin_data = calibration_metrics[method_before]['bin_data']\n",
    "    if len(bin_data) > 0:\n",
    "        confidences = [b['confidence'] for b in bin_data]\n",
    "        accuracies = [b['accuracy'] for b in bin_data]\n",
    "        counts = [b['count'] for b in bin_data]\n",
    "        \n",
    "        ax.bar(range(len(confidences)), accuracies, alpha=0.3, label='Accuracy', color='blue')\n",
    "        ax.plot(range(len(confidences)), confidences, 'o-', label='Confidence', color='red', markersize=8)\n",
    "        ax.plot([0, len(confidences)-1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "        ax.set_xlabel('Confidence bin')\n",
    "        ax.set_ylabel('Proportion')\n",
    "        ax.set_title(f'{method_before}\\nECE={calibration_metrics[method_before][\"ECE\"]:.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Con TS\n",
    "    ax = axes[idx * 2 + 1]\n",
    "    bin_data = calibration_metrics[method_after]['bin_data']\n",
    "    if len(bin_data) > 0:\n",
    "        confidences = [b['confidence'] for b in bin_data]\n",
    "        accuracies = [b['accuracy'] for b in bin_data]\n",
    "        \n",
    "        ax.bar(range(len(confidences)), accuracies, alpha=0.3, label='Accuracy', color='blue')\n",
    "        ax.plot(range(len(confidences)), confidences, 'o-', label='Confidence', color='red', markersize=8)\n",
    "        ax.plot([0, len(confidences)-1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "        ax.set_xlabel('Confidence bin')\n",
    "        ax.set_ylabel('Proportion')\n",
    "        ax.set_title(f'{method_after}\\nECE={calibration_metrics[method_after][\"ECE\"]:.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'reliability_diagrams.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Reliability diagrams guardados en: {OUTPUT_DIR / 'reliability_diagrams.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109a217",
   "metadata": {},
   "source": [
    "## 12. Risk-Coverage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a937a",
   "metadata": {},
   "source": [
    "## 13. M√©tricas de Incertidumbre: AUROC TP vs FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c6ebffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUROC: ¬øLa incertidumbre detecta errores (FP)?\n",
      "================================================================================\n",
      "\n",
      "Objetivo: Usar incertidumbre para distinguir FP (errores) de TP (aciertos)\n",
      "Interpretaci√≥n: AUROC > 0.5 (random), ideal ‚â• 0.7\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "mc_dropout:\n",
      "  AUROC (FP detection): 0.6335\n",
      "  Mean uncertainty TP:  0.000061\n",
      "  Mean uncertainty FP:  0.000126\n",
      "  Ratio (FP/TP):        2.07x\n",
      "  Samples: 13317 TP, 9210 FP\n",
      "\n",
      "mc_dropout_ts:\n",
      "  AUROC (FP detection): 0.6335\n",
      "  Mean uncertainty TP:  0.000061\n",
      "  Mean uncertainty FP:  0.000126\n",
      "  Ratio (FP/TP):        2.07x\n",
      "  Samples: 13317 TP, 9210 FP\n",
      "\n",
      "decoder_variance:\n",
      "  AUROC (FP detection): 0.5000\n",
      "  Mean uncertainty TP:  0.000000\n",
      "  Mean uncertainty FP:  0.000000\n",
      "  Ratio (FP/TP):        0.00x\n",
      "  Samples: 13508 TP, 9285 FP\n",
      "\n",
      "decoder_variance_ts:\n",
      "  AUROC (FP detection): 0.5000\n",
      "  Mean uncertainty TP:  0.000000\n",
      "  Mean uncertainty FP:  0.000000\n",
      "  Ratio (FP/TP):        0.00x\n",
      "  Samples: 13508 TP, 9285 FP\n",
      "\n",
      "================================================================================\n",
      "Resultados guardados en: outputs/comparison/uncertainty_auroc.json\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "uncertainty_auroc = {}\n",
    "\n",
    "# Solo m√©todos con incertidumbre (MC-Dropout y Decoder Variance)\n",
    "uncertainty_methods = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AUROC: ¬øLa incertidumbre detecta errores (FP)?\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjetivo: Usar incertidumbre para distinguir FP (errores) de TP (aciertos)\")\n",
    "print(\"Interpretaci√≥n: AUROC > 0.5 (random), ideal ‚â• 0.7\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for method_name in uncertainty_methods:\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    \n",
    "    if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "        uncertainties = df['uncertainty'].values\n",
    "        is_tp = df['is_tp'].values\n",
    "        \n",
    "        # Verificar que hay TPs y FPs\n",
    "        if len(np.unique(is_tp)) > 1 and len(uncertainties) > 0:\n",
    "            # AUROC: predecir FP (error) usando incertidumbre\n",
    "            # Invertir labels: 1=FP (error), 0=TP (correcto)\n",
    "            is_fp = 1 - is_tp\n",
    "            \n",
    "            try:\n",
    "                auroc = roc_auc_score(is_fp, uncertainties)\n",
    "                \n",
    "                # Estad√≠sticas de incertidumbre\n",
    "                unc_tp = uncertainties[is_tp == 1]\n",
    "                unc_fp = uncertainties[is_tp == 0]\n",
    "                \n",
    "                mean_unc_tp = unc_tp.mean() if len(unc_tp) > 0 else 0.0\n",
    "                mean_unc_fp = unc_fp.mean() if len(unc_fp) > 0 else 0.0\n",
    "                \n",
    "                uncertainty_auroc[method_name] = {\n",
    "                    'auroc': auroc,\n",
    "                    'mean_unc_tp': mean_unc_tp,\n",
    "                    'mean_unc_fp': mean_unc_fp,\n",
    "                    'n_tp': int(is_tp.sum()),\n",
    "                    'n_fp': int((1 - is_tp).sum())\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{method_name}:\")\n",
    "                print(f\"  AUROC (FP detection): {auroc:.4f}\")\n",
    "                print(f\"  Mean uncertainty TP:  {mean_unc_tp:.6f}\")\n",
    "                print(f\"  Mean uncertainty FP:  {mean_unc_fp:.6f}\")\n",
    "                print(f\"  Ratio (FP/TP):        {mean_unc_fp/mean_unc_tp if mean_unc_tp > 0 else 0:.2f}x\")\n",
    "                print(f\"  Samples: {int(is_tp.sum())} TP, {int((1-is_tp).sum())} FP\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n{method_name}: Error calculando AUROC - {e}\")\n",
    "        else:\n",
    "            print(f\"\\n{method_name}: Datos insuficientes para AUROC\")\n",
    "\n",
    "# Guardar resultados\n",
    "with open(OUTPUT_DIR / 'uncertainty_auroc.json', 'w') as f:\n",
    "    json.dump(uncertainty_auroc, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Resultados guardados en: {OUTPUT_DIR / 'uncertainty_auroc.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38b117b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLA COMPARATIVA: AUROC TP vs FP\n",
      "================================================================================\n",
      "             Method  AUROC (FP detection) ‚Üë  Mean Unc. TP  Mean Unc. FP  Ratio (FP/TP)\n",
      "         mc_dropout                0.633462      0.000061      0.000126       2.070721\n",
      "      mc_dropout_ts                0.633462      0.000061      0.000126       2.070721\n",
      "   decoder_variance                0.500000      0.000000      0.000000       0.000000\n",
      "decoder_variance_ts                0.500000      0.000000      0.000000       0.000000\n",
      "================================================================================\n",
      "\n",
      "Interpretaci√≥n:\n",
      "  ‚Üë Mayor AUROC = mejor detecci√≥n de errores\n",
      "  Ratio (FP/TP) > 1 = incertidumbre mayor en errores (deseable)\n",
      "  AUROC ‚â• 0.7 = incertidumbre √∫til para rechazo selectivo\n"
     ]
    }
   ],
   "source": [
    "# Tabla comparativa AUROC\n",
    "rows_auroc = []\n",
    "for method_name, metrics in uncertainty_auroc.items():\n",
    "    rows_auroc.append({\n",
    "        'Method': method_name,\n",
    "        'AUROC (FP detection) ‚Üë': metrics['auroc'],\n",
    "        'Mean Unc. TP': metrics['mean_unc_tp'],\n",
    "        'Mean Unc. FP': metrics['mean_unc_fp'],\n",
    "        'Ratio (FP/TP)': metrics['mean_unc_fp'] / metrics['mean_unc_tp'] if metrics['mean_unc_tp'] > 0 else 0\n",
    "    })\n",
    "\n",
    "df_auroc = pd.DataFrame(rows_auroc)\n",
    "df_auroc.to_csv(OUTPUT_DIR / 'uncertainty_auroc_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA: AUROC TP vs FP\")\n",
    "print(\"=\"*80)\n",
    "print(df_auroc.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInterpretaci√≥n:\")\n",
    "print(\"  ‚Üë Mayor AUROC = mejor detecci√≥n de errores\")\n",
    "print(\"  Ratio (FP/TP) > 1 = incertidumbre mayor en errores (deseable)\")\n",
    "print(\"  AUROC ‚â• 0.7 = incertidumbre √∫til para rechazo selectivo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51dc21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizaci√≥n de incertidumbre guardada en: outputs/comparison/uncertainty_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# Visualizaci√≥n: Distribuciones de incertidumbre y ROC curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "methods_to_plot = ['mc_dropout', 'decoder_variance']\n",
    "colors_methods = {'mc_dropout': 'blue', 'decoder_variance': 'green'}\n",
    "\n",
    "for idx, method_name in enumerate(methods_to_plot):\n",
    "    # Fila 1: Distribuciones de incertidumbre (TP vs FP)\n",
    "    ax_dist = axes[0, idx]\n",
    "    \n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "        unc_tp = df[df['is_tp'] == 1]['uncertainty'].values\n",
    "        unc_fp = df[df['is_tp'] == 0]['uncertainty'].values\n",
    "        \n",
    "        ax_dist.hist(unc_tp, bins=50, alpha=0.6, label=f'TP (n={len(unc_tp)})', color='green', density=True)\n",
    "        ax_dist.hist(unc_fp, bins=50, alpha=0.6, label=f'FP (n={len(unc_fp)})', color='red', density=True)\n",
    "        ax_dist.axvline(unc_tp.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean TP: {unc_tp.mean():.4f}')\n",
    "        ax_dist.axvline(unc_fp.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean FP: {unc_fp.mean():.4f}')\n",
    "        ax_dist.set_xlabel('Uncertainty', fontsize=11)\n",
    "        ax_dist.set_ylabel('Density', fontsize=11)\n",
    "        ax_dist.set_title(f'{method_name.replace(\"_\", \" \").title()}\\nDistribuci√≥n de Incertidumbre', fontsize=12, fontweight='bold')\n",
    "        ax_dist.legend(fontsize=9)\n",
    "        ax_dist.grid(alpha=0.3)\n",
    "    \n",
    "    # Fila 2: ROC curves\n",
    "    ax_roc = axes[1, idx]\n",
    "    \n",
    "    if method_name in uncertainty_auroc:\n",
    "        is_tp = df['is_tp'].values\n",
    "        is_fp = 1 - is_tp\n",
    "        uncertainties = df['uncertainty'].values\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(is_fp, uncertainties)\n",
    "        auroc = uncertainty_auroc[method_name]['auroc']\n",
    "        \n",
    "        ax_roc.plot(fpr, tpr, linewidth=2, label=f'AUROC = {auroc:.4f}', color=colors_methods[method_name])\n",
    "        ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random (0.5)')\n",
    "        ax_roc.set_xlabel('False Positive Rate', fontsize=11)\n",
    "        ax_roc.set_ylabel('True Positive Rate', fontsize=11)\n",
    "        ax_roc.set_title(f'{method_name.replace(\"_\", \" \").title()}\\nROC Curve (FP Detection)', fontsize=12, fontweight='bold')\n",
    "        ax_roc.legend(fontsize=10)\n",
    "        ax_roc.grid(alpha=0.3)\n",
    "        ax_roc.set_xlim([0, 1])\n",
    "        ax_roc.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'uncertainty_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nVisualizaci√≥n de incertidumbre guardada en: {OUTPUT_DIR / 'uncertainty_analysis.png'}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627ff1f",
   "metadata": {},
   "source": [
    "## 12. Risk-Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4ca145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk-coverage curves guardadas en: outputs/comparison/risk_coverage_curves.png\n",
      "\n",
      "Risk-Coverage AUC:\n",
      "  mc_dropout: 0.5245\n",
      "  mc_dropout_ts: 0.5245\n",
      "  decoder_variance: 0.4101\n",
      "  decoder_variance_ts: 0.4101\n"
     ]
    }
   ],
   "source": [
    "def compute_risk_coverage(df, uncertainty_col='uncertainty'):\n",
    "    \"\"\"Calcula curva risk-coverage\"\"\"\n",
    "    df_sorted = df.sort_values(uncertainty_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    coverages = []\n",
    "    risks = []\n",
    "    \n",
    "    for i in range(1, len(df_sorted) + 1):\n",
    "        coverage = i / len(df_sorted)\n",
    "        risk = 1 - df_sorted.iloc[:i]['is_tp'].mean()\n",
    "        coverages.append(coverage)\n",
    "        risks.append(risk)\n",
    "    \n",
    "    # AUC (√°rea bajo la curva)\n",
    "    auc = np.trapz(risks, coverages)\n",
    "    \n",
    "    return coverages, risks, auc\n",
    "\n",
    "# Calcular risk-coverage para m√©todos con incertidumbre\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "methods_with_uncertainty = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "colors = ['blue', 'cyan', 'red', 'orange']\n",
    "\n",
    "risk_coverage_results = {}\n",
    "\n",
    "for ax_idx, method_name in enumerate(['mc_dropout', 'decoder_variance']):\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    for variant, color in [(method_name, 'blue'), (f'{method_name}_ts', 'red')]:\n",
    "        df = pd.read_csv(OUTPUT_DIR / f'eval_{variant}.csv')\n",
    "        \n",
    "        if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "            coverages, risks, auc = compute_risk_coverage(df, 'uncertainty')\n",
    "            \n",
    "            label = variant.replace('_', ' ').title()\n",
    "            ax.plot(coverages, risks, label=f'{label} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "            \n",
    "            risk_coverage_results[variant] = {\n",
    "                'coverages': coverages,\n",
    "                'risks': risks,\n",
    "                'auc': auc\n",
    "            }\n",
    "    \n",
    "    ax.set_xlabel('Coverage', fontsize=12)\n",
    "    ax.set_ylabel('Risk (1 - Accuracy)', fontsize=12)\n",
    "    ax.set_title(f'Risk-Coverage: {method_name.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'risk_coverage_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Risk-coverage curves guardadas en: {OUTPUT_DIR / 'risk_coverage_curves.png'}\")\n",
    "plt.close()\n",
    "\n",
    "# Guardar AUC\n",
    "auc_summary = {k: v['auc'] for k, v in risk_coverage_results.items()}\n",
    "with open(OUTPUT_DIR / 'risk_coverage_auc.json', 'w') as f:\n",
    "    json.dump(auc_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nRisk-Coverage AUC:\")\n",
    "for method, auc in auc_summary.items():\n",
    "    print(f\"  {method}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f56cf",
   "metadata": {},
   "source": [
    "## 14. Resumen Final y Reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e6ea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN FINAL - COMPARACI√ìN DE M√âTODOS\n",
      "================================================================================\n",
      "\n",
      "1. M√âTRICAS DE DETECCI√ìN (mAP@[0.5:0.95])\n",
      "--------------------------------------------------------------------------------\n",
      "baseline                   mAP=0.1705  AP50=0.2785  AP75=0.1705\n",
      "baseline_ts                mAP=0.1705  AP50=0.2785  AP75=0.1705\n",
      "mc_dropout                 mAP=0.1823  AP50=0.3023  AP75=0.1811\n",
      "mc_dropout_ts              mAP=0.1823  AP50=0.3023  AP75=0.1811\n",
      "decoder_variance           mAP=0.1819  AP50=0.3020  AP75=0.1801\n",
      "decoder_variance_ts        mAP=0.1819  AP50=0.3020  AP75=0.1801\n",
      "\n",
      "2. M√âTRICAS DE CALIBRACI√ìN\n",
      "--------------------------------------------------------------------------------\n",
      "Method                         NLL ‚Üì    Brier ‚Üì      ECE ‚Üì\n",
      "--------------------------------------------------------------------------------\n",
      "baseline                      0.7180     0.2618     0.2410\n",
      "baseline_ts                   0.6930     0.2499     0.1868\n",
      "mc_dropout                    0.7069     0.2561     0.2034\n",
      "mc_dropout_ts                 1.0070     0.3365     0.3428\n",
      "decoder_variance              0.7093     0.2572     0.2065\n",
      "decoder_variance_ts           0.6863     0.2466     0.1409\n",
      "\n",
      "3. TEMPERATURAS OPTIMIZADAS\n",
      "--------------------------------------------------------------------------------\n",
      "baseline              T=4.2128  NLL: 0.7107 ‚Üí 0.6912 (Œî=0.0195)\n",
      "mc_dropout            T=0.3192  NLL: 0.5123 ‚Üí 0.4001 (Œî=0.1122)\n",
      "decoder_variance      T=2.6534  NLL: 0.7061 ‚Üí 0.6850 (Œî=0.0212)\n",
      "\n",
      "4. RISK-COVERAGE AUC (menor es mejor)\n",
      "--------------------------------------------------------------------------------\n",
      "mc_dropout                 AUC=0.5245\n",
      "mc_dropout_ts              AUC=0.5245\n",
      "decoder_variance           AUC=0.4101\n",
      "decoder_variance_ts        AUC=0.4101\n",
      "\n",
      "5. INCERTIDUMBRE: AUROC TP vs FP (mayor es mejor)\n",
      "--------------------------------------------------------------------------------\n",
      "Method                       AUROC ‚Üë     Mean Unc TP     Mean Unc FP      Ratio\n",
      "--------------------------------------------------------------------------------\n",
      "mc_dropout                    0.6335        0.000061        0.000126       2.07x\n",
      "mc_dropout_ts                 0.6335        0.000061        0.000126       2.07x\n",
      "decoder_variance              0.5000        0.000000        0.000000       0.00x\n",
      "decoder_variance_ts           0.5000        0.000000        0.000000       0.00x\n",
      "\n",
      "================================================================================\n",
      "CONCLUSIONES\n",
      "================================================================================\n",
      "‚úì Baseline: rendimiento de referencia sin incertidumbre\n",
      "‚úì Temperature Scaling: mejora calibraci√≥n sin afectar mAP\n",
      "‚úì MC-Dropout: proporciona incertidumbre epist√©mica (K pases)\n",
      "‚úì Decoder variance: incertidumbre en single-pass (m√°s eficiente)\n",
      "‚úì M√©todos+TS: mejor calibraci√≥n manteniendo detecci√≥n\n",
      "‚úì AUROC TP vs FP: valida que incertidumbre detecta errores\n",
      "================================================================================\n",
      "\n",
      "Reporte final guardado en: outputs/comparison/final_report.json\n",
      "Todos los artefactos en: outputs/comparison\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL - COMPARACI√ìN DE M√âTODOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cargar todas las m√©tricas\n",
    "det_metrics = json.load(open(OUTPUT_DIR / 'detection_metrics.json'))\n",
    "cal_metrics = json.load(open(OUTPUT_DIR / 'calibration_metrics.json'))\n",
    "temps = json.load(open(OUTPUT_DIR / 'temperatures.json'))\n",
    "auc_summary = json.load(open(OUTPUT_DIR / 'risk_coverage_auc.json'))\n",
    "uncertainty_auroc_data = json.load(open(OUTPUT_DIR / 'uncertainty_auroc.json'))\n",
    "\n",
    "print(\"\\n1. M√âTRICAS DE DETECCI√ìN (mAP@[0.5:0.95])\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']:\n",
    "    mAP = det_metrics[method].get('mAP', 0.0)\n",
    "    AP50 = det_metrics[method].get('AP50', 0.0)\n",
    "    AP75 = det_metrics[method].get('AP75', 0.0)\n",
    "    print(f\"{method:25s}  mAP={mAP:.4f}  AP50={AP50:.4f}  AP75={AP75:.4f}\")\n",
    "\n",
    "print(\"\\n2. M√âTRICAS DE CALIBRACI√ìN\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<25s} {'NLL ‚Üì':>10s} {'Brier ‚Üì':>10s} {'ECE ‚Üì':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']:\n",
    "    nll = cal_metrics[method]['NLL']\n",
    "    brier = cal_metrics[method]['Brier']\n",
    "    ece = cal_metrics[method]['ECE']\n",
    "    print(f\"{method:<25s} {nll:>10.4f} {brier:>10.4f} {ece:>10.4f}\")\n",
    "\n",
    "print(\"\\n3. TEMPERATURAS OPTIMIZADAS\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'mc_dropout', 'decoder_variance']:\n",
    "    T = temps[method]['T']\n",
    "    nll_before = temps[method]['nll_before']\n",
    "    nll_after = temps[method]['nll_after']\n",
    "    improvement = nll_before - nll_after\n",
    "    print(f\"{method:20s}  T={T:.4f}  NLL: {nll_before:.4f} ‚Üí {nll_after:.4f} (Œî={improvement:.4f})\")\n",
    "\n",
    "print(\"\\n4. RISK-COVERAGE AUC (menor es mejor)\")\n",
    "print(\"-\" * 80)\n",
    "for method, auc in auc_summary.items():\n",
    "    print(f\"{method:25s}  AUC={auc:.4f}\")\n",
    "\n",
    "print(\"\\n5. INCERTIDUMBRE: AUROC TP vs FP (mayor es mejor)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<25s} {'AUROC ‚Üë':>10s} {'Mean Unc TP':>15s} {'Mean Unc FP':>15s} {'Ratio':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "for method, data in uncertainty_auroc_data.items():\n",
    "    auroc = data['auroc']\n",
    "    mean_tp = data['mean_unc_tp']\n",
    "    mean_fp = data['mean_unc_fp']\n",
    "    ratio = mean_fp / mean_tp if mean_tp > 0 else 0\n",
    "    print(f\"{method:<25s} {auroc:>10.4f} {mean_tp:>15.6f} {mean_fp:>15.6f} {ratio:>10.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Baseline: rendimiento de referencia sin incertidumbre\")\n",
    "print(\"‚úì Temperature Scaling: mejora calibraci√≥n sin afectar mAP\")\n",
    "print(\"‚úì MC-Dropout: proporciona incertidumbre epist√©mica (K pases)\")\n",
    "print(\"‚úì Decoder variance: incertidumbre en single-pass (m√°s eficiente)\")\n",
    "print(\"‚úì M√©todos+TS: mejor calibraci√≥n manteniendo detecci√≥n\")\n",
    "print(\"‚úì AUROC TP vs FP: valida que incertidumbre detecta errores\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Guardar reporte final\n",
    "final_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'detection_metrics': det_metrics,\n",
    "    'calibration_metrics': cal_metrics,\n",
    "    'temperatures': temps,\n",
    "    'risk_coverage_auc': auc_summary,\n",
    "    'uncertainty_auroc': uncertainty_auroc_data\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'final_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nReporte final guardado en: {OUTPUT_DIR / 'final_report.json'}\")\n",
    "print(f\"Todos los artefactos en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cad14",
   "metadata": {},
   "source": [
    "## 15. Visualizaci√≥n Final Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402ead03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizaci√≥n final guardada en: outputs/comparison/final_comparison_summary.png\n",
      "\n",
      "================================================================================\n",
      "FASE 5 COMPLETADA\n",
      "================================================================================\n",
      "Todos los resultados guardados en: outputs/comparison\n",
      "\n",
      "Archivos generados:\n",
      "  - config.yaml\n",
      "  - temperatures.json\n",
      "  - detection_metrics.json\n",
      "  - calibration_metrics.json\n",
      "  - risk_coverage_auc.json\n",
      "  - uncertainty_auroc.json\n",
      "  - uncertainty_auroc_comparison.csv\n",
      "  - final_report.json\n",
      "  - detection_comparison.csv\n",
      "  - calibration_comparison.csv\n",
      "  - reliability_diagrams.png\n",
      "  - risk_coverage_curves.png\n",
      "  - uncertainty_analysis.png\n",
      "  - final_comparison_summary.png\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. mAP Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "methods = ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "mAPs = [det_metrics[m].get('mAP', 0.0) for m in methods]\n",
    "colors_map = ['lightblue', 'blue', 'lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars = ax1.bar(range(len(methods)), mAPs, color=colors_map, alpha=0.7)\n",
    "ax1.set_xticks(range(len(methods)))\n",
    "ax1.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=10)\n",
    "ax1.set_ylabel('mAP@[0.5:0.95]', fontsize=12)\n",
    "ax1.set_title('Comparaci√≥n de mAP entre M√©todos', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height, f'{mAPs[i]:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Calibration Metrics Comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "nlls = [cal_metrics[m]['NLL'] for m in methods]\n",
    "ax2.bar(range(len(methods)), nlls, color=colors_map, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods)))\n",
    "ax2.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax2.set_ylabel('NLL ‚Üì', fontsize=11)\n",
    "ax2.set_title('Negative Log-Likelihood', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "briers = [cal_metrics[m]['Brier'] for m in methods]\n",
    "ax3.bar(range(len(methods)), briers, color=colors_map, alpha=0.7)\n",
    "ax3.set_xticks(range(len(methods)))\n",
    "ax3.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax3.set_ylabel('Brier Score ‚Üì', fontsize=11)\n",
    "ax3.set_title('Brier Score', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "eces = [cal_metrics[m]['ECE'] for m in methods]\n",
    "ax4.bar(range(len(methods)), eces, color=colors_map, alpha=0.7)\n",
    "ax4.set_xticks(range(len(methods)))\n",
    "ax4.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax4.set_ylabel('ECE ‚Üì', fontsize=11)\n",
    "ax4.set_title('Expected Calibration Error', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Temperature Scaling Effect\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "base_methods = ['baseline', 'mc_dropout', 'decoder_variance']\n",
    "Ts = [temps[m]['T'] for m in base_methods]\n",
    "ax5.bar(range(len(base_methods)), Ts, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "ax5.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='T=1 (sin calibrar)')\n",
    "ax5.set_xticks(range(len(base_methods)))\n",
    "ax5.set_xticklabels([m.replace('_', '\\n') for m in base_methods], fontsize=10)\n",
    "ax5.set_ylabel('Temperature T', fontsize=11)\n",
    "ax5.set_title('Temperaturas √ìptimas', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Risk-Coverage AUC\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "unc_methods = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "aucs = [auc_summary.get(m, 0.0) for m in unc_methods]\n",
    "colors_unc = ['lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars_auc = ax6.bar(range(len(unc_methods)), aucs, color=colors_unc, alpha=0.7)\n",
    "ax6.set_xticks(range(len(unc_methods)))\n",
    "ax6.set_xticklabels([m.replace('_', '\\n') for m in unc_methods], fontsize=9)\n",
    "ax6.set_ylabel('AUC (Risk-Coverage) ‚Üì', fontsize=11)\n",
    "ax6.set_title('Risk-Coverage AUC', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars_auc):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height, f'{aucs[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 5. AUROC TP vs FP (Nueva secci√≥n)\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "auroc_methods = list(uncertainty_auroc_data.keys())\n",
    "aurocs = [uncertainty_auroc_data[m]['auroc'] for m in auroc_methods]\n",
    "colors_auroc = ['lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars_auroc = ax7.bar(range(len(auroc_methods)), aurocs, color=colors_auroc, alpha=0.7)\n",
    "ax7.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Random')\n",
    "ax7.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax7.set_xticks(range(len(auroc_methods)))\n",
    "ax7.set_xticklabels([m.replace('_', '\\n') for m in auroc_methods], fontsize=9)\n",
    "ax7.set_ylabel('AUROC (FP detection) ‚Üë', fontsize=11)\n",
    "ax7.set_title('AUROC: Detecci√≥n de Errores', fontsize=12, fontweight='bold')\n",
    "ax7.legend(fontsize=8)\n",
    "ax7.grid(axis='y', alpha=0.3)\n",
    "ax7.set_ylim([0, 1])\n",
    "for i, bar in enumerate(bars_auroc):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height, f'{aurocs[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 6. Resumen de incertidumbre (ratio FP/TP)\n",
    "ax8 = fig.add_subplot(gs[3, :])\n",
    "ratios = [uncertainty_auroc_data[m]['mean_unc_fp'] / uncertainty_auroc_data[m]['mean_unc_tp'] \n",
    "          if uncertainty_auroc_data[m]['mean_unc_tp'] > 0 else 0 \n",
    "          for m in auroc_methods]\n",
    "bars_ratio = ax8.bar(range(len(auroc_methods)), ratios, color=colors_auroc, alpha=0.7)\n",
    "ax8.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Ratio = 1 (sin diferencia)')\n",
    "ax8.set_xticks(range(len(auroc_methods)))\n",
    "ax8.set_xticklabels([m.replace('_', '\\n') for m in auroc_methods], fontsize=10)\n",
    "ax8.set_ylabel('Ratio Mean(Unc_FP) / Mean(Unc_TP)', fontsize=11)\n",
    "ax8.set_title('Ratio de Incertidumbre: FP vs TP (>1 es deseable)', fontsize=12, fontweight='bold')\n",
    "ax8.legend()\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars_ratio):\n",
    "    height = bar.get_height()\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2., height, f'{ratios[i]:.2f}x', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Fase 5: Comparaci√≥n Completa de M√©todos de Incertidumbre y Calibraci√≥n', \n",
    "             fontsize=16, fontweight='bold', y=0.997)\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'final_comparison_summary.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nVisualizaci√≥n final guardada en: {OUTPUT_DIR / 'final_comparison_summary.png'}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 5 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Todos los resultados guardados en: {OUTPUT_DIR}\")\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(\"  - config.yaml\")\n",
    "print(\"  - temperatures.json\")\n",
    "print(\"  - detection_metrics.json\")\n",
    "print(\"  - calibration_metrics.json\")\n",
    "print(\"  - risk_coverage_auc.json\")\n",
    "print(\"  - uncertainty_auroc.json\")\n",
    "print(\"  - uncertainty_auroc_comparison.csv\")\n",
    "print(\"  - final_report.json\")\n",
    "print(\"  - detection_comparison.csv\")\n",
    "print(\"  - calibration_comparison.csv\")\n",
    "print(\"  - reliability_diagrams.png\")\n",
    "print(\"  - risk_coverage_curves.png\")\n",
    "print(\"  - uncertainty_analysis.png\")\n",
    "print(\"  - final_comparison_summary.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea0ef223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[94m======================================================================\n",
      "VERIFICACI√ìN DE OPTIMIZACIONES - FASE 5\n",
      "======================================================================\u001b[0m\n",
      "\n",
      "\u001b[1m1. VERIFICACI√ìN DE ARCHIVOS DE FASE 2 (Baseline)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "\u001b[92m‚úÖ Predicciones Baseline\u001b[0m\n",
      "   Ubicaci√≥n: ../fase 2/outputs/baseline/preds_raw.json\n",
      "   Tama√±o: 3.23 MB\n",
      "   \u001b[92mFormato: ‚úÖ Correcto (22162 registros)\u001b[0m\n",
      "\n",
      "\u001b[1m2. VERIFICACI√ìN DE ARCHIVOS DE FASE 3 (MC-Dropout)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "\u001b[92m‚úÖ Predicciones MC-Dropout\u001b[0m\n",
      "   Ubicaci√≥n: ../fase 3/outputs/mc_dropout/preds_mc_aggregated.json\n",
      "   Tama√±o: 4.35 MB\n",
      "   \u001b[93mFormato: ‚ö†Ô∏è  Faltan keys: ['uncertainty']\u001b[0m\n",
      "\n",
      "\u001b[1m3. VERIFICACI√ìN DE ARCHIVOS DE FASE 4 (Temperature)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "\u001b[92m‚úÖ Temperaturas Optimizadas\u001b[0m\n",
      "   Ubicaci√≥n: ../fase 4/outputs/temperature_scaling/temperature.json\n",
      "   Tama√±o: 0.00 MB\n",
      "   \u001b[93mFormato: ‚ö†Ô∏è  Falta 'optimal_temperature'\u001b[0m\n",
      "\n",
      "\u001b[1m======================================================================\n",
      "RESUMEN\n",
      "======================================================================\u001b[0m\n",
      "\n",
      "\u001b[1mArchivos encontrados:\u001b[0m 3/3\n",
      "\u001b[92m‚úÖ TODOS los archivos est√°n disponibles\u001b[0m\n",
      "\n",
      "\u001b[1mTiempo estimado ahorrado:\u001b[0m\n",
      "   \u001b[92m‚ö° ~2h 17min\u001b[0m\n",
      "\n",
      "\u001b[1mTiempo de ejecuci√≥n esperado:\u001b[0m\n",
      "   \u001b[92müìä ~15-20 minutos\u001b[0m (solo Decoder Variance)\n",
      "\n",
      "\u001b[1m======================================================================\n",
      "RECOMENDACIONES\n",
      "======================================================================\u001b[0m\n",
      "\u001b[92m‚úÖ Perfecto! Puedes ejecutar Fase 5 directamente.\u001b[0m\n",
      "   El notebook usar√° todos los resultados cacheados.\n",
      "\n",
      "\u001b[1m======================================================================\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script de verificaci√≥n de optimizaciones de Fase 5\n",
    "===================================================\n",
    "\n",
    "Este script verifica que:\n",
    "1. Los archivos de fases anteriores existen\n",
    "2. Los formatos de datos son correctos\n",
    "3. Las predicciones son compatibles\n",
    "4. Estima el tiempo que se ahorrar√°\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Colores para terminal\n",
    "class Colors:\n",
    "    GREEN = \"\\033[92m\"\n",
    "    YELLOW = \"\\033[93m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    END = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def check_file(path, description):\n",
    "    \"\"\"Verifica si un archivo existe y retorna su info\"\"\"\n",
    "    path = Path(path)\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"{Colors.GREEN}‚úÖ {description}{Colors.END}\")\n",
    "        print(f\"   Ubicaci√≥n: {path}\")\n",
    "        print(f\"   Tama√±o: {size_mb:.2f} MB\")\n",
    "        return True, size_mb\n",
    "    else:\n",
    "        print(f\"{Colors.RED}‚ùå {description}{Colors.END}\")\n",
    "        print(f\"   {Colors.YELLOW}No encontrado: {path}{Colors.END}\")\n",
    "        return False, 0\n",
    "\n",
    "\n",
    "def verify_json_format(path, expected_keys):\n",
    "    \"\"\"Verifica que un JSON tenga el formato esperado\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not isinstance(data, list) or len(data) == 0:\n",
    "            return False, \"No es una lista o est√° vac√≠a\"\n",
    "\n",
    "        sample = data[0]\n",
    "        missing_keys = [k for k in expected_keys if k not in sample]\n",
    "\n",
    "        if missing_keys:\n",
    "            return False, f\"Faltan keys: {missing_keys}\"\n",
    "\n",
    "        return True, f\"{len(data)} registros\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{Colors.BOLD}{Colors.BLUE}{'='*70}\")\n",
    "    print(\"VERIFICACI√ìN DE OPTIMIZACIONES - FASE 5\")\n",
    "    print(f\"{'='*70}{Colors.END}\\n\")\n",
    "\n",
    "    # Paths\n",
    "    base_dir = Path(\"..\")\n",
    "    fase2_preds = base_dir / \"fase 2\" / \"outputs\" / \"baseline\" / \"preds_raw.json\"\n",
    "    fase3_preds = (\n",
    "        base_dir / \"fase 3\" / \"outputs\" / \"mc_dropout\" / \"preds_mc_aggregated.json\"\n",
    "    )\n",
    "    fase4_temp = (\n",
    "        base_dir / \"fase 4\" / \"outputs\" / \"temperature_scaling\" / \"temperature.json\"\n",
    "    )\n",
    "\n",
    "    # Contadores\n",
    "    files_found = 0\n",
    "    total_files = 3\n",
    "    time_saved = 0\n",
    "\n",
    "    # ========================================================================\n",
    "    print(f\"{Colors.BOLD}1. VERIFICACI√ìN DE ARCHIVOS DE FASE 2 (Baseline){Colors.END}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    exists, size = check_file(fase2_preds, \"Predicciones Baseline\")\n",
    "    if exists:\n",
    "        files_found += 1\n",
    "        time_saved += 45  # 45 minutos ahorrados\n",
    "\n",
    "        # Verificar formato\n",
    "        valid, info = verify_json_format(\n",
    "            fase2_preds, [\"image_id\", \"category_id\", \"bbox\", \"score\"]\n",
    "        )\n",
    "        if valid:\n",
    "            print(f\"   {Colors.GREEN}Formato: ‚úÖ Correcto ({info}){Colors.END}\")\n",
    "        else:\n",
    "            print(f\"   {Colors.YELLOW}Formato: ‚ö†Ô∏è  {info}{Colors.END}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # ========================================================================\n",
    "    print(\n",
    "        f\"{Colors.BOLD}2. VERIFICACI√ìN DE ARCHIVOS DE FASE 3 (MC-Dropout){Colors.END}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    exists, size = check_file(fase3_preds, \"Predicciones MC-Dropout\")\n",
    "    if exists:\n",
    "        files_found += 1\n",
    "        time_saved += 90  # 90 minutos ahorrados (K=5 es costoso)\n",
    "\n",
    "        # Verificar formato\n",
    "        valid, info = verify_json_format(\n",
    "            fase3_preds, [\"image_id\", \"category_id\", \"bbox\", \"score\", \"uncertainty\"]\n",
    "        )\n",
    "        if valid:\n",
    "            print(f\"   {Colors.GREEN}Formato: ‚úÖ Correcto ({info}){Colors.END}\")\n",
    "        else:\n",
    "            print(f\"   {Colors.YELLOW}Formato: ‚ö†Ô∏è  {info}{Colors.END}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # ========================================================================\n",
    "    print(\n",
    "        f\"{Colors.BOLD}3. VERIFICACI√ìN DE ARCHIVOS DE FASE 4 (Temperature){Colors.END}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    exists, size = check_file(fase4_temp, \"Temperaturas Optimizadas\")\n",
    "    if exists:\n",
    "        files_found += 1\n",
    "        time_saved += 2  # 2 minutos ahorrados\n",
    "\n",
    "        # Verificar formato\n",
    "        try:\n",
    "            with open(fase4_temp, \"r\") as f:\n",
    "                temps = json.load(f)\n",
    "\n",
    "            if \"optimal_temperature\" in temps:\n",
    "                T = temps[\"optimal_temperature\"]\n",
    "                print(f\"   {Colors.GREEN}Formato: ‚úÖ Correcto (T={T:.4f}){Colors.END}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"   {Colors.YELLOW}Formato: ‚ö†Ô∏è  Falta 'optimal_temperature'{Colors.END}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"   {Colors.YELLOW}Formato: ‚ö†Ô∏è  Error: {e}{Colors.END}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # ========================================================================\n",
    "    print(f\"{Colors.BOLD}{'='*70}\")\n",
    "    print(\"RESUMEN\")\n",
    "    print(f\"{'='*70}{Colors.END}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n{Colors.BOLD}Archivos encontrados:{Colors.END} {files_found}/{total_files}\"\n",
    "    )\n",
    "\n",
    "    if files_found == total_files:\n",
    "        print(f\"{Colors.GREEN}‚úÖ TODOS los archivos est√°n disponibles{Colors.END}\")\n",
    "    elif files_found > 0:\n",
    "        print(f\"{Colors.YELLOW}‚ö†Ô∏è  Algunos archivos est√°n disponibles{Colors.END}\")\n",
    "    else:\n",
    "        print(f\"{Colors.RED}‚ùå NO hay archivos disponibles{Colors.END}\")\n",
    "\n",
    "    # Estimaci√≥n de tiempo\n",
    "    print(f\"\\n{Colors.BOLD}Tiempo estimado ahorrado:{Colors.END}\")\n",
    "\n",
    "    if time_saved > 0:\n",
    "        td = timedelta(minutes=time_saved)\n",
    "        hours = td.seconds // 3600\n",
    "        minutes = (td.seconds % 3600) // 60\n",
    "\n",
    "        print(f\"   {Colors.GREEN}‚ö° ~{hours}h {minutes}min{Colors.END}\")\n",
    "\n",
    "        if files_found == total_files:\n",
    "            print(f\"\\n{Colors.BOLD}Tiempo de ejecuci√≥n esperado:{Colors.END}\")\n",
    "            print(\n",
    "                f\"   {Colors.GREEN}üìä ~15-20 minutos{Colors.END} (solo Decoder Variance)\"\n",
    "            )\n",
    "        else:\n",
    "            missing = total_files - files_found\n",
    "            est_time = 137 - time_saved  # 137 min total original\n",
    "            print(f\"\\n{Colors.BOLD}Tiempo de ejecuci√≥n esperado:{Colors.END}\")\n",
    "            print(\n",
    "                f\"   {Colors.YELLOW}üìä ~{est_time} minutos{Colors.END} (calcular {missing} m√©todo(s) faltante(s))\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   {Colors.RED}‚ùå 0 minutos{Colors.END}\")\n",
    "        print(f\"\\n{Colors.BOLD}Tiempo de ejecuci√≥n esperado:{Colors.END}\")\n",
    "        print(f\"   {Colors.RED}üìä ~2 horas{Colors.END} (inferencia completa)\")\n",
    "\n",
    "    # Recomendaciones\n",
    "    print(f\"\\n{Colors.BOLD}{'='*70}\")\n",
    "    print(\"RECOMENDACIONES\")\n",
    "    print(f\"{'='*70}{Colors.END}\")\n",
    "\n",
    "    if files_found == total_files:\n",
    "        print(\n",
    "            f\"{Colors.GREEN}‚úÖ Perfecto! Puedes ejecutar Fase 5 directamente.{Colors.END}\"\n",
    "        )\n",
    "        print(f\"   El notebook usar√° todos los resultados cacheados.\")\n",
    "    elif files_found == 0:\n",
    "        print(f\"{Colors.YELLOW}‚ö†Ô∏è  Ejecuta las siguientes fases primero:{Colors.END}\")\n",
    "        print(f\"   1. Fase 2: Genera predicciones baseline\")\n",
    "        print(f\"   2. Fase 3: Genera predicciones MC-Dropout\")\n",
    "        print(f\"   3. Fase 4: Optimiza temperaturas\")\n",
    "        print(f\"\\n   O ejecuta Fase 5 directamente (tardar√° ~2 horas)\")\n",
    "    else:\n",
    "        print(f\"{Colors.YELLOW}‚ö†Ô∏è  Tienes optimizaci√≥n parcial.{Colors.END}\")\n",
    "\n",
    "        if not (base_dir / fase2_preds).exists():\n",
    "            print(f\"   ‚Ä¢ Ejecuta Fase 2 para predicciones baseline\")\n",
    "        if not (base_dir / fase3_preds).exists():\n",
    "            print(f\"   ‚Ä¢ Ejecuta Fase 3 para predicciones MC-Dropout\")\n",
    "        if not (base_dir / fase4_temp).exists():\n",
    "            print(f\"   ‚Ä¢ Ejecuta Fase 4 para temperaturas\")\n",
    "\n",
    "        print(f\"\\n   O ejecuta Fase 5 ahora (ahorrar√° ~{time_saved} min)\")\n",
    "\n",
    "    print(f\"\\n{Colors.BOLD}{'='*70}{Colors.END}\\n\")\n",
    "\n",
    "    # Exit code\n",
    "    return 0 if files_found == total_files else 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
