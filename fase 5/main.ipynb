{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6846700",
   "metadata": {},
   "source": [
    "# Fase 5: Comparación Completa de Métodos de Incertidumbre y Calibración\n",
    "\n",
    "**Objetivo**: Comparar 6 métodos lado a lado en detección, calibración y risk-coverage.\n",
    "\n",
    "**Métodos evaluados**:\n",
    "1. Baseline (sin incertidumbre, sin calibración)\n",
    "2. Baseline + TS\n",
    "3. MC-Dropout K=5\n",
    "4. MC-Dropout K=5 + TS\n",
    "5. Varianza entre capas (single-pass)\n",
    "6. Varianza entre capas + TS\n",
    "\n",
    "**Splits**:\n",
    "- val_calib: ajustar temperaturas\n",
    "- val_eval: evaluación final\n",
    "\n",
    "**Métricas**:\n",
    "- Detección: mAP@[0.5:0.95], AP50, AP75, por clase\n",
    "- Calibración: NLL, Brier, ECE, Reliability Diagrams\n",
    "- Risk-Coverage: curvas y AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5375caf",
   "metadata": {},
   "source": [
    "## 1. Configuración e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04069526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = Path('./outputs/comparison')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'categories': ['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'],\n",
    "    'iou_matching': 0.5,\n",
    "    'conf_threshold': 0.25,\n",
    "    'nms_threshold': 0.65,\n",
    "    'K_mc': 5,\n",
    "    'n_bins': 10\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "with open(OUTPUT_DIR / 'config.yaml', 'w') as f:\n",
    "    yaml.dump(CONFIG, f)\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Config guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ed4b",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelo y Preparar Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483452c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from groundingdino.util import box_ops\n",
    "\n",
    "model_config = '/opt/program/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_weights = '/opt/program/GroundingDINO/weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "model = load_model(model_config, model_weights)\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "TEXT_PROMPT = '. '.join(CONFIG['categories']) + '.'\n",
    "\n",
    "print(f\"Modelo cargado en {CONFIG['device']}\")\n",
    "print(f\"Prompt: {TEXT_PROMPT}\")\n",
    "\n",
    "# Guardar referencias de módulos dropout\n",
    "dropout_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Dropout) and ('class_embed' in name or 'bbox_embed' in name):\n",
    "        dropout_modules.append(module)\n",
    "\n",
    "print(f\"Módulos dropout en cabeza: {len(dropout_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91608a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(label):\n",
    "    synonyms = {'bike': 'bicycle', 'motorbike': 'motorcycle', 'pedestrian': 'person', \n",
    "                'stop sign': 'traffic sign', 'red light': 'traffic light'}\n",
    "    label_lower = label.lower().strip()\n",
    "    if label_lower in synonyms:\n",
    "        return synonyms[label_lower]\n",
    "    for cat in CONFIG['categories']:\n",
    "        if cat in label_lower:\n",
    "            return cat\n",
    "    return label_lower\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def apply_nms(detections, iou_thresh=0.65):\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "    boxes_t = torch.tensor([d['bbox'] for d in detections], dtype=torch.float32)\n",
    "    scores_t = torch.tensor([d['score'] for d in detections], dtype=torch.float32)\n",
    "    keep = torchvision.ops.nms(boxes_t, scores_t, iou_thresh)\n",
    "    return [detections[i] for i in keep.numpy()]\n",
    "\n",
    "print(\"Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58514ed6",
   "metadata": {},
   "source": [
    "## 3. Métodos de Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89986b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_baseline(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"Método 1: Baseline single-pass sin incertidumbre\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.eval()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    h, w = image_source.shape[:2]\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for box, score, phrase in zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat in CONFIG['categories']:\n",
    "            score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            detections.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'category': cat,\n",
    "                'uncertainty': 0.0  # Sin incertidumbre\n",
    "            })\n",
    "    \n",
    "    return apply_nms(detections, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"Método 1: Baseline definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64328901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mc_dropout(model, image_path, text_prompt, conf_thresh, device, K=5):\n",
    "    \"\"\"Método 3: MC-Dropout con K pases\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.train()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    h, w = image_source.shape[:2]\n",
    "    \n",
    "    all_detections_k = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in range(K):\n",
    "            boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "            \n",
    "            if len(boxes) == 0:\n",
    "                all_detections_k.append([])\n",
    "                continue\n",
    "            \n",
    "            boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "            \n",
    "            dets_k = []\n",
    "            for box, score, phrase in zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases):\n",
    "                cat = normalize_label(phrase)\n",
    "                if cat in CONFIG['categories']:\n",
    "                    score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "                    dets_k.append({\n",
    "                        'bbox': box.tolist(),\n",
    "                        'score': score_clipped,\n",
    "                        'category': cat\n",
    "                    })\n",
    "            all_detections_k.append(dets_k)\n",
    "    \n",
    "    # Alinear detecciones entre pases\n",
    "    if len(all_detections_k) == 0 or all(len(d) == 0 for d in all_detections_k):\n",
    "        return []\n",
    "    \n",
    "    # Usar primer pase como referencia\n",
    "    ref_dets = all_detections_k[0]\n",
    "    \n",
    "    aggregated = []\n",
    "    for ref_det in ref_dets:\n",
    "        scores_aligned = [ref_det['score']]\n",
    "        \n",
    "        for k in range(1, K):\n",
    "            best_iou = 0\n",
    "            best_score = None\n",
    "            for det_k in all_detections_k[k]:\n",
    "                if det_k['category'] != ref_det['category']:\n",
    "                    continue\n",
    "                iou = compute_iou(ref_det['bbox'], det_k['bbox'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_score = det_k['score']\n",
    "            \n",
    "            if best_iou >= 0.5 and best_score is not None:\n",
    "                scores_aligned.append(best_score)\n",
    "        \n",
    "        mean_score = np.mean(scores_aligned)\n",
    "        variance = np.var(scores_aligned) if len(scores_aligned) > 1 else 0.0\n",
    "        \n",
    "        mean_score_clipped = np.clip(mean_score, 1e-7, 1 - 1e-7)\n",
    "        logit = np.log(mean_score_clipped / (1 - mean_score_clipped))\n",
    "        \n",
    "        aggregated.append({\n",
    "            'bbox': ref_det['bbox'],\n",
    "            'score': mean_score_clipped,\n",
    "            'logit': logit,\n",
    "            'category': ref_det['category'],\n",
    "            'uncertainty': variance\n",
    "        })\n",
    "    \n",
    "    return apply_nms(aggregated, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"Método 3: MC-Dropout definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoder_variance(model, image_path, text_prompt, conf_thresh, device):\n",
    "    \"\"\"Método 5: Varianza entre capas del decoder (single-pass)\"\"\"\n",
    "    model.eval()\n",
    "    for module in dropout_modules:\n",
    "        module.eval()\n",
    "    \n",
    "    image_source, image = load_image(str(image_path))\n",
    "    h, w = image_source.shape[:2]\n",
    "    \n",
    "    # Hook para capturar logits de cada capa del decoder\n",
    "    layer_logits = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if hasattr(output, 'pred_logits'):\n",
    "            layer_logits.append(output.pred_logits.detach())\n",
    "    \n",
    "    # Registrar hooks en capas del decoder\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if 'decoder.layers' in name and name.endswith(')'):\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Inferencia\n",
    "    boxes, scores, phrases = predict(model, image, text_prompt, conf_thresh, 0.25, device)\n",
    "    \n",
    "    # Remover hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.tensor([w, h, w, h])\n",
    "    \n",
    "    detections = []\n",
    "    for idx, (box, score, phrase) in enumerate(zip(boxes_xyxy.cpu().numpy(), scores.cpu().numpy(), phrases)):\n",
    "        cat = normalize_label(phrase)\n",
    "        if cat in CONFIG['categories']:\n",
    "            score_clipped = np.clip(float(score), 1e-7, 1 - 1e-7)\n",
    "            logit = np.log(score_clipped / (1 - score_clipped))\n",
    "            \n",
    "            # Calcular varianza entre capas si disponible\n",
    "            uncertainty = 0.0\n",
    "            if len(layer_logits) > 1:\n",
    "                # Tomar scores de cada capa para esta detección\n",
    "                layer_scores = []\n",
    "                for layer_out in layer_logits:\n",
    "                    if idx < layer_out.shape[1]:\n",
    "                        layer_score = torch.sigmoid(layer_out[0, idx].max()).item()\n",
    "                        layer_scores.append(layer_score)\n",
    "                \n",
    "                if len(layer_scores) > 1:\n",
    "                    uncertainty = np.var(layer_scores)\n",
    "            \n",
    "            detections.append({\n",
    "                'bbox': box.tolist(),\n",
    "                'score': score_clipped,\n",
    "                'logit': logit,\n",
    "                'category': cat,\n",
    "                'uncertainty': uncertainty\n",
    "            })\n",
    "    \n",
    "    return apply_nms(detections, CONFIG['nms_threshold'])\n",
    "\n",
    "print(\"Método 5: Decoder variance definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7d22b",
   "metadata": {},
   "source": [
    "## 4. Inferencia en val_calib para Ajustar Temperaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f931a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_calib_json = DATA_DIR / 'bdd100k_coco/val_calib.json'\n",
    "val_eval_json = DATA_DIR / 'bdd100k_coco/val_eval.json'\n",
    "image_dir = DATA_DIR / 'bdd100k/bdd100k/bdd100k/images/100k/val'\n",
    "\n",
    "coco_calib = COCO(str(val_calib_json))\n",
    "img_ids_calib = coco_calib.getImgIds()\n",
    "\n",
    "print(f\"Procesando {len(img_ids_calib[:500])} imágenes de val_calib para ajustar temperaturas...\")\n",
    "\n",
    "methods_calib_data = {\n",
    "    'baseline': [],\n",
    "    'mc_dropout': [],\n",
    "    'decoder_variance': []\n",
    "}\n",
    "\n",
    "for img_id in tqdm(img_ids_calib[:500]):\n",
    "    img_info = coco_calib.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    gt_anns = coco_calib.loadAnns(coco_calib.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    # Método 1: Baseline\n",
    "    preds_baseline = inference_baseline(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_baseline:\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            gt_cat_id = gt['category_id']\n",
    "            gt_cat = CONFIG['categories'][gt_cat_id - 1] if 1 <= gt_cat_id <= len(CONFIG['categories']) else ''\n",
    "            if gt_cat != pred['category']:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        methods_calib_data['baseline'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': pred['category'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # Método 3: MC-Dropout\n",
    "    preds_mc = inference_mc_dropout(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'], CONFIG['K_mc'])\n",
    "    for pred in preds_mc:\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            gt_cat_id = gt['category_id']\n",
    "            gt_cat = CONFIG['categories'][gt_cat_id - 1] if 1 <= gt_cat_id <= len(CONFIG['categories']) else ''\n",
    "            if gt_cat != pred['category']:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        methods_calib_data['mc_dropout'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': pred['category'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # Método 5: Decoder variance\n",
    "    preds_dec = inference_decoder_variance(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_dec:\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            gt_cat_id = gt['category_id']\n",
    "            gt_cat = CONFIG['categories'][gt_cat_id - 1] if 1 <= gt_cat_id <= len(CONFIG['categories']) else ''\n",
    "            if gt_cat != pred['category']:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        methods_calib_data['decoder_variance'].append({\n",
    "            'logit': pred['logit'],\n",
    "            'score': pred['score'],\n",
    "            'category': pred['category'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "\n",
    "# Guardar datos de calibración\n",
    "for method_name, data in methods_calib_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(OUTPUT_DIR / f'calib_{method_name}.csv', index=False)\n",
    "    print(f\"{method_name}: {len(df)} detecciones, TP={df['is_tp'].sum()}\")\n",
    "\n",
    "print(\"Datos de calibración guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5a229",
   "metadata": {},
   "source": [
    "## 5. Optimizar Temperaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ae767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def nll_loss(T, logits, labels):\n",
    "    T = max(T, 0.01)\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "\n",
    "temperatures = {}\n",
    "\n",
    "for method_name in ['baseline', 'mc_dropout', 'decoder_variance']:\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'calib_{method_name}.csv')\n",
    "    logits = df['logit'].values\n",
    "    labels = df['is_tp'].values\n",
    "    \n",
    "    nll_before = nll_loss(1.0, logits, labels)\n",
    "    result = minimize(lambda T: nll_loss(T, logits, labels), x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "    T_opt = result.x[0]\n",
    "    nll_after = result.fun\n",
    "    \n",
    "    temperatures[method_name] = {\n",
    "        'T': T_opt,\n",
    "        'nll_before': nll_before,\n",
    "        'nll_after': nll_after\n",
    "    }\n",
    "    \n",
    "    print(f\"{method_name}: T={T_opt:.4f}, NLL: {nll_before:.4f} → {nll_after:.4f}\")\n",
    "\n",
    "with open(OUTPUT_DIR / 'temperatures.json', 'w') as f:\n",
    "    json.dump(temperatures, f, indent=2)\n",
    "\n",
    "print(f\"\\nTemperaturas guardadas en: {OUTPUT_DIR / 'temperatures.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b5c79",
   "metadata": {},
   "source": [
    "## 6. Evaluación en val_eval con COCO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45966d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_eval = COCO(str(val_eval_json))\n",
    "img_ids_eval = coco_eval.getImgIds()\n",
    "\n",
    "print(f\"Procesando {len(img_ids_eval)} imágenes de val_eval...\")\n",
    "\n",
    "methods_results = {\n",
    "    'baseline': [],\n",
    "    'baseline_ts': [],\n",
    "    'mc_dropout': [],\n",
    "    'mc_dropout_ts': [],\n",
    "    'decoder_variance': [],\n",
    "    'decoder_variance_ts': []\n",
    "}\n",
    "\n",
    "# Cargar temperaturas\n",
    "with open(OUTPUT_DIR / 'temperatures.json', 'r') as f:\n",
    "    temps = json.load(f)\n",
    "\n",
    "for img_id in tqdm(img_ids_eval):\n",
    "    img_info = coco_eval.loadImgs(img_id)[0]\n",
    "    img_path = image_dir / img_info['file_name']\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    gt_anns = coco_eval.loadAnns(coco_eval.getAnnIds(imgIds=img_id))\n",
    "    \n",
    "    # Baseline\n",
    "    preds_baseline = inference_baseline(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_baseline:\n",
    "        cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_results['baseline'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': pred['score'],\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "        \n",
    "        # Con TS\n",
    "        score_ts = sigmoid(pred['logit'] / temps['baseline']['T'])\n",
    "        methods_results['baseline_ts'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': score_ts,\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # MC-Dropout\n",
    "    preds_mc = inference_mc_dropout(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'], CONFIG['K_mc'])\n",
    "    for pred in preds_mc:\n",
    "        cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_results['mc_dropout'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': pred['score'],\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "        \n",
    "        score_ts = sigmoid(pred['logit'] / temps['mc_dropout']['T'])\n",
    "        methods_results['mc_dropout_ts'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': score_ts,\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "    \n",
    "    # Decoder variance\n",
    "    preds_dec = inference_decoder_variance(model, img_path, TEXT_PROMPT, CONFIG['conf_threshold'], CONFIG['device'])\n",
    "    for pred in preds_dec:\n",
    "        cat_id = CONFIG['categories'].index(pred['category']) + 1\n",
    "        is_tp = 0\n",
    "        for gt in gt_anns:\n",
    "            if gt['category_id'] != cat_id:\n",
    "                continue\n",
    "            gt_box = gt['bbox']\n",
    "            gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "            if compute_iou(pred['bbox'], gt_box_xyxy) >= CONFIG['iou_matching']:\n",
    "                is_tp = 1\n",
    "                break\n",
    "        \n",
    "        methods_results['decoder_variance'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': pred['score'],\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "        \n",
    "        score_ts = sigmoid(pred['logit'] / temps['decoder_variance']['T'])\n",
    "        methods_results['decoder_variance_ts'].append({\n",
    "            'image_id': img_id,\n",
    "            'category_id': cat_id,\n",
    "            'bbox': [pred['bbox'][0], pred['bbox'][1], pred['bbox'][2] - pred['bbox'][0], pred['bbox'][3] - pred['bbox'][1]],\n",
    "            'score': score_ts,\n",
    "            'logit': pred['logit'],\n",
    "            'uncertainty': pred['uncertainty'],\n",
    "            'is_tp': is_tp\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "for method_name, results in methods_results.items():\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_DIR / f'eval_{method_name}.csv', index=False)\n",
    "    with open(OUTPUT_DIR / f'eval_{method_name}.json', 'w') as f:\n",
    "        json.dump([{k: v for k, v in r.items() if k != 'is_tp'} for r in results], f)\n",
    "    print(f\"{method_name}: {len(df)} detecciones\")\n",
    "\n",
    "print(\"Evaluación completada y guardada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d072e6e",
   "metadata": {},
   "source": [
    "## 7. Calcular Métricas de Detección (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_metrics = {}\n",
    "\n",
    "for method_name in methods_results.keys():\n",
    "    print(f\"\\nEvaluando {method_name}...\")\n",
    "    \n",
    "    # Cargar predicciones en formato COCO\n",
    "    preds_file = OUTPUT_DIR / f'eval_{method_name}.json'\n",
    "    \n",
    "    if os.path.getsize(preds_file) > 0:\n",
    "        coco_dt = coco_eval.loadRes(str(preds_file))\n",
    "        coco_eval_obj = COCOeval(coco_eval, coco_dt, 'bbox')\n",
    "        coco_eval_obj.evaluate()\n",
    "        coco_eval_obj.accumulate()\n",
    "        coco_eval_obj.summarize()\n",
    "        \n",
    "        detection_metrics[method_name] = {\n",
    "            'mAP': coco_eval_obj.stats[0],\n",
    "            'AP50': coco_eval_obj.stats[1],\n",
    "            'AP75': coco_eval_obj.stats[2],\n",
    "            'AP_small': coco_eval_obj.stats[3],\n",
    "            'AP_medium': coco_eval_obj.stats[4],\n",
    "            'AP_large': coco_eval_obj.stats[5]\n",
    "        }\n",
    "        \n",
    "        # mAP por clase\n",
    "        per_class_ap = {}\n",
    "        for cat_id, cat_name in enumerate(CONFIG['categories'], 1):\n",
    "            coco_eval_obj.params.catIds = [cat_id]\n",
    "            coco_eval_obj.evaluate()\n",
    "            coco_eval_obj.accumulate()\n",
    "            per_class_ap[cat_name] = coco_eval_obj.stats[0]\n",
    "        \n",
    "        detection_metrics[method_name]['per_class'] = per_class_ap\n",
    "    else:\n",
    "        detection_metrics[method_name] = {'mAP': 0.0, 'AP50': 0.0, 'AP75': 0.0}\n",
    "\n",
    "with open(OUTPUT_DIR / 'detection_metrics.json', 'w') as f:\n",
    "    json.dump(detection_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nMétricas de detección guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a3e0a",
   "metadata": {},
   "source": [
    "## 8. Tabla Comparativa de Detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c836a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'detection_metrics.json', 'r') as f:\n",
    "    det_metrics = json.load(f)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "rows = []\n",
    "for method_name, metrics in det_metrics.items():\n",
    "    row = {\n",
    "        'Method': method_name,\n",
    "        'mAP': metrics.get('mAP', 0.0),\n",
    "        'AP50': metrics.get('AP50', 0.0),\n",
    "        'AP75': metrics.get('AP75', 0.0)\n",
    "    }\n",
    "    \n",
    "    # Agregar mAP por clase principal\n",
    "    if 'per_class' in metrics:\n",
    "        for cat in ['person', 'car', 'truck', 'traffic_light', 'traffic_sign']:\n",
    "            cat_key = cat.replace('_', ' ')\n",
    "            row[f'AP_{cat}'] = metrics['per_class'].get(cat_key, 0.0)\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "df_detection = pd.DataFrame(rows)\n",
    "df_detection.to_csv(OUTPUT_DIR / 'detection_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE DETECCIÓN\")\n",
    "print(\"=\"*80)\n",
    "print(df_detection.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441867bd",
   "metadata": {},
   "source": [
    "## 9. Calcular Métricas de Calibración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration_metrics(logits, labels, T=1.0, n_bins=10):\n",
    "    probs = sigmoid(logits / T)\n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # NLL\n",
    "    nll = -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "    \n",
    "    # Brier\n",
    "    brier = np.mean((probs - labels) ** 2)\n",
    "    \n",
    "    # ECE\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    digitized = np.digitize(probs, bins) - 1\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_data = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = digitized == i\n",
    "        if mask.sum() > 0:\n",
    "            conf = probs[mask].mean()\n",
    "            acc = labels[mask].mean()\n",
    "            gap = abs(conf - acc)\n",
    "            ece += gap * mask.sum() / len(probs)\n",
    "            bin_data.append({\n",
    "                'bin': i,\n",
    "                'confidence': conf,\n",
    "                'accuracy': acc,\n",
    "                'count': mask.sum()\n",
    "            })\n",
    "    \n",
    "    return {'NLL': nll, 'Brier': brier, 'ECE': ece, 'bin_data': bin_data}\n",
    "\n",
    "calibration_metrics = {}\n",
    "\n",
    "for method_name in methods_results.keys():\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    logits = df['logit'].values\n",
    "    labels = df['is_tp'].values\n",
    "    \n",
    "    # Sin TS (T=1)\n",
    "    if '_ts' not in method_name:\n",
    "        metrics = compute_calibration_metrics(logits, labels, T=1.0, n_bins=CONFIG['n_bins'])\n",
    "        calibration_metrics[method_name] = metrics\n",
    "    else:\n",
    "        # Con TS\n",
    "        base_method = method_name.replace('_ts', '')\n",
    "        T = temps[base_method]['T']\n",
    "        metrics = compute_calibration_metrics(logits, labels, T=T, n_bins=CONFIG['n_bins'])\n",
    "        calibration_metrics[method_name] = metrics\n",
    "    \n",
    "    print(f\"{method_name}: NLL={metrics['NLL']:.4f}, Brier={metrics['Brier']:.4f}, ECE={metrics['ECE']:.4f}\")\n",
    "\n",
    "# Guardar\n",
    "with open(OUTPUT_DIR / 'calibration_metrics.json', 'w') as f:\n",
    "    # Convertir para JSON serializable\n",
    "    cal_save = {}\n",
    "    for k, v in calibration_metrics.items():\n",
    "        cal_save[k] = {\n",
    "            'NLL': v['NLL'],\n",
    "            'Brier': v['Brier'],\n",
    "            'ECE': v['ECE']\n",
    "        }\n",
    "    json.dump(cal_save, f, indent=2)\n",
    "\n",
    "print(\"\\nMétricas de calibración guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4cdd1",
   "metadata": {},
   "source": [
    "## 10. Tabla Comparativa de Calibración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_calib = []\n",
    "for method_name, metrics in calibration_metrics.items():\n",
    "    rows_calib.append({\n",
    "        'Method': method_name,\n",
    "        'NLL': metrics['NLL'],\n",
    "        'Brier': metrics['Brier'],\n",
    "        'ECE': metrics['ECE']\n",
    "    })\n",
    "\n",
    "df_calibration = pd.DataFrame(rows_calib)\n",
    "df_calibration.to_csv(OUTPUT_DIR / 'calibration_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE CALIBRACIÓN\")\n",
    "print(\"=\"*80)\n",
    "print(df_calibration.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInterpretación:\")\n",
    "print(\"  ↓ Menor es mejor para NLL, Brier, ECE\")\n",
    "print(\"  Si método+TS < método: TS mejoró calibración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0630882",
   "metadata": {},
   "source": [
    "## 11. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37663ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "method_pairs = [\n",
    "    ('baseline', 'baseline_ts'),\n",
    "    ('mc_dropout', 'mc_dropout_ts'),\n",
    "    ('decoder_variance', 'decoder_variance_ts')\n",
    "]\n",
    "\n",
    "for idx, (method_before, method_after) in enumerate(method_pairs):\n",
    "    ax = axes[idx * 2]\n",
    "    \n",
    "    # Sin TS\n",
    "    bin_data = calibration_metrics[method_before]['bin_data']\n",
    "    if len(bin_data) > 0:\n",
    "        confidences = [b['confidence'] for b in bin_data]\n",
    "        accuracies = [b['accuracy'] for b in bin_data]\n",
    "        counts = [b['count'] for b in bin_data]\n",
    "        \n",
    "        ax.bar(range(len(confidences)), accuracies, alpha=0.3, label='Accuracy', color='blue')\n",
    "        ax.plot(range(len(confidences)), confidences, 'o-', label='Confidence', color='red', markersize=8)\n",
    "        ax.plot([0, len(confidences)-1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "        ax.set_xlabel('Confidence bin')\n",
    "        ax.set_ylabel('Proportion')\n",
    "        ax.set_title(f'{method_before}\\nECE={calibration_metrics[method_before][\"ECE\"]:.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Con TS\n",
    "    ax = axes[idx * 2 + 1]\n",
    "    bin_data = calibration_metrics[method_after]['bin_data']\n",
    "    if len(bin_data) > 0:\n",
    "        confidences = [b['confidence'] for b in bin_data]\n",
    "        accuracies = [b['accuracy'] for b in bin_data]\n",
    "        \n",
    "        ax.bar(range(len(confidences)), accuracies, alpha=0.3, label='Accuracy', color='blue')\n",
    "        ax.plot(range(len(confidences)), confidences, 'o-', label='Confidence', color='red', markersize=8)\n",
    "        ax.plot([0, len(confidences)-1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "        ax.set_xlabel('Confidence bin')\n",
    "        ax.set_ylabel('Proportion')\n",
    "        ax.set_title(f'{method_after}\\nECE={calibration_metrics[method_after][\"ECE\"]:.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'reliability_diagrams.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Reliability diagrams guardados en: {OUTPUT_DIR / 'reliability_diagrams.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109a217",
   "metadata": {},
   "source": [
    "## 12. Risk-Coverage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a937a",
   "metadata": {},
   "source": [
    "## 13. Métricas de Incertidumbre: AUROC TP vs FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ebffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "uncertainty_auroc = {}\n",
    "\n",
    "# Solo métodos con incertidumbre (MC-Dropout y Decoder Variance)\n",
    "uncertainty_methods = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AUROC: ¿La incertidumbre detecta errores (FP)?\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjetivo: Usar incertidumbre para distinguir FP (errores) de TP (aciertos)\")\n",
    "print(\"Interpretación: AUROC > 0.5 (random), ideal ≥ 0.7\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for method_name in uncertainty_methods:\n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    \n",
    "    if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "        uncertainties = df['uncertainty'].values\n",
    "        is_tp = df['is_tp'].values\n",
    "        \n",
    "        # Verificar que hay TPs y FPs\n",
    "        if len(np.unique(is_tp)) > 1 and len(uncertainties) > 0:\n",
    "            # AUROC: predecir FP (error) usando incertidumbre\n",
    "            # Invertir labels: 1=FP (error), 0=TP (correcto)\n",
    "            is_fp = 1 - is_tp\n",
    "            \n",
    "            try:\n",
    "                auroc = roc_auc_score(is_fp, uncertainties)\n",
    "                \n",
    "                # Estadísticas de incertidumbre\n",
    "                unc_tp = uncertainties[is_tp == 1]\n",
    "                unc_fp = uncertainties[is_tp == 0]\n",
    "                \n",
    "                mean_unc_tp = unc_tp.mean() if len(unc_tp) > 0 else 0.0\n",
    "                mean_unc_fp = unc_fp.mean() if len(unc_fp) > 0 else 0.0\n",
    "                \n",
    "                uncertainty_auroc[method_name] = {\n",
    "                    'auroc': auroc,\n",
    "                    'mean_unc_tp': mean_unc_tp,\n",
    "                    'mean_unc_fp': mean_unc_fp,\n",
    "                    'n_tp': int(is_tp.sum()),\n",
    "                    'n_fp': int((1 - is_tp).sum())\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{method_name}:\")\n",
    "                print(f\"  AUROC (FP detection): {auroc:.4f}\")\n",
    "                print(f\"  Mean uncertainty TP:  {mean_unc_tp:.6f}\")\n",
    "                print(f\"  Mean uncertainty FP:  {mean_unc_fp:.6f}\")\n",
    "                print(f\"  Ratio (FP/TP):        {mean_unc_fp/mean_unc_tp if mean_unc_tp > 0 else 0:.2f}x\")\n",
    "                print(f\"  Samples: {int(is_tp.sum())} TP, {int((1-is_tp).sum())} FP\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n{method_name}: Error calculando AUROC - {e}\")\n",
    "        else:\n",
    "            print(f\"\\n{method_name}: Datos insuficientes para AUROC\")\n",
    "\n",
    "# Guardar resultados\n",
    "with open(OUTPUT_DIR / 'uncertainty_auroc.json', 'w') as f:\n",
    "    json.dump(uncertainty_auroc, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Resultados guardados en: {OUTPUT_DIR / 'uncertainty_auroc.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b117b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa AUROC\n",
    "rows_auroc = []\n",
    "for method_name, metrics in uncertainty_auroc.items():\n",
    "    rows_auroc.append({\n",
    "        'Method': method_name,\n",
    "        'AUROC (FP detection) ↑': metrics['auroc'],\n",
    "        'Mean Unc. TP': metrics['mean_unc_tp'],\n",
    "        'Mean Unc. FP': metrics['mean_unc_fp'],\n",
    "        'Ratio (FP/TP)': metrics['mean_unc_fp'] / metrics['mean_unc_tp'] if metrics['mean_unc_tp'] > 0 else 0\n",
    "    })\n",
    "\n",
    "df_auroc = pd.DataFrame(rows_auroc)\n",
    "df_auroc.to_csv(OUTPUT_DIR / 'uncertainty_auroc_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA: AUROC TP vs FP\")\n",
    "print(\"=\"*80)\n",
    "print(df_auroc.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nInterpretación:\")\n",
    "print(\"  ↑ Mayor AUROC = mejor detección de errores\")\n",
    "print(\"  Ratio (FP/TP) > 1 = incertidumbre mayor en errores (deseable)\")\n",
    "print(\"  AUROC ≥ 0.7 = incertidumbre útil para rechazo selectivo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Distribuciones de incertidumbre y ROC curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "methods_to_plot = ['mc_dropout', 'decoder_variance']\n",
    "colors_methods = {'mc_dropout': 'blue', 'decoder_variance': 'green'}\n",
    "\n",
    "for idx, method_name in enumerate(methods_to_plot):\n",
    "    # Fila 1: Distribuciones de incertidumbre (TP vs FP)\n",
    "    ax_dist = axes[0, idx]\n",
    "    \n",
    "    df = pd.read_csv(OUTPUT_DIR / f'eval_{method_name}.csv')\n",
    "    if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "        unc_tp = df[df['is_tp'] == 1]['uncertainty'].values\n",
    "        unc_fp = df[df['is_tp'] == 0]['uncertainty'].values\n",
    "        \n",
    "        ax_dist.hist(unc_tp, bins=50, alpha=0.6, label=f'TP (n={len(unc_tp)})', color='green', density=True)\n",
    "        ax_dist.hist(unc_fp, bins=50, alpha=0.6, label=f'FP (n={len(unc_fp)})', color='red', density=True)\n",
    "        ax_dist.axvline(unc_tp.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean TP: {unc_tp.mean():.4f}')\n",
    "        ax_dist.axvline(unc_fp.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean FP: {unc_fp.mean():.4f}')\n",
    "        ax_dist.set_xlabel('Uncertainty', fontsize=11)\n",
    "        ax_dist.set_ylabel('Density', fontsize=11)\n",
    "        ax_dist.set_title(f'{method_name.replace(\"_\", \" \").title()}\\nDistribución de Incertidumbre', fontsize=12, fontweight='bold')\n",
    "        ax_dist.legend(fontsize=9)\n",
    "        ax_dist.grid(alpha=0.3)\n",
    "    \n",
    "    # Fila 2: ROC curves\n",
    "    ax_roc = axes[1, idx]\n",
    "    \n",
    "    if method_name in uncertainty_auroc:\n",
    "        is_tp = df['is_tp'].values\n",
    "        is_fp = 1 - is_tp\n",
    "        uncertainties = df['uncertainty'].values\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(is_fp, uncertainties)\n",
    "        auroc = uncertainty_auroc[method_name]['auroc']\n",
    "        \n",
    "        ax_roc.plot(fpr, tpr, linewidth=2, label=f'AUROC = {auroc:.4f}', color=colors_methods[method_name])\n",
    "        ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random (0.5)')\n",
    "        ax_roc.set_xlabel('False Positive Rate', fontsize=11)\n",
    "        ax_roc.set_ylabel('True Positive Rate', fontsize=11)\n",
    "        ax_roc.set_title(f'{method_name.replace(\"_\", \" \").title()}\\nROC Curve (FP Detection)', fontsize=12, fontweight='bold')\n",
    "        ax_roc.legend(fontsize=10)\n",
    "        ax_roc.grid(alpha=0.3)\n",
    "        ax_roc.set_xlim([0, 1])\n",
    "        ax_roc.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'uncertainty_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nVisualización de incertidumbre guardada en: {OUTPUT_DIR / 'uncertainty_analysis.png'}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627ff1f",
   "metadata": {},
   "source": [
    "## 12. Risk-Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ca145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_coverage(df, uncertainty_col='uncertainty'):\n",
    "    \"\"\"Calcula curva risk-coverage\"\"\"\n",
    "    df_sorted = df.sort_values(uncertainty_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    coverages = []\n",
    "    risks = []\n",
    "    \n",
    "    for i in range(1, len(df_sorted) + 1):\n",
    "        coverage = i / len(df_sorted)\n",
    "        risk = 1 - df_sorted.iloc[:i]['is_tp'].mean()\n",
    "        coverages.append(coverage)\n",
    "        risks.append(risk)\n",
    "    \n",
    "    # AUC (área bajo la curva)\n",
    "    auc = np.trapz(risks, coverages)\n",
    "    \n",
    "    return coverages, risks, auc\n",
    "\n",
    "# Calcular risk-coverage para métodos con incertidumbre\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "methods_with_uncertainty = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "colors = ['blue', 'cyan', 'red', 'orange']\n",
    "\n",
    "risk_coverage_results = {}\n",
    "\n",
    "for ax_idx, method_name in enumerate(['mc_dropout', 'decoder_variance']):\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    for variant, color in [(method_name, 'blue'), (f'{method_name}_ts', 'red')]:\n",
    "        df = pd.read_csv(OUTPUT_DIR / f'eval_{variant}.csv')\n",
    "        \n",
    "        if len(df) > 0 and 'uncertainty' in df.columns:\n",
    "            coverages, risks, auc = compute_risk_coverage(df, 'uncertainty')\n",
    "            \n",
    "            label = variant.replace('_', ' ').title()\n",
    "            ax.plot(coverages, risks, label=f'{label} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "            \n",
    "            risk_coverage_results[variant] = {\n",
    "                'coverages': coverages,\n",
    "                'risks': risks,\n",
    "                'auc': auc\n",
    "            }\n",
    "    \n",
    "    ax.set_xlabel('Coverage', fontsize=12)\n",
    "    ax.set_ylabel('Risk (1 - Accuracy)', fontsize=12)\n",
    "    ax.set_title(f'Risk-Coverage: {method_name.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'risk_coverage_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Risk-coverage curves guardadas en: {OUTPUT_DIR / 'risk_coverage_curves.png'}\")\n",
    "plt.close()\n",
    "\n",
    "# Guardar AUC\n",
    "auc_summary = {k: v['auc'] for k, v in risk_coverage_results.items()}\n",
    "with open(OUTPUT_DIR / 'risk_coverage_auc.json', 'w') as f:\n",
    "    json.dump(auc_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nRisk-Coverage AUC:\")\n",
    "for method, auc in auc_summary.items():\n",
    "    print(f\"  {method}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f56cf",
   "metadata": {},
   "source": [
    "## 14. Resumen Final y Reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL - COMPARACIÓN DE MÉTODOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cargar todas las métricas\n",
    "det_metrics = json.load(open(OUTPUT_DIR / 'detection_metrics.json'))\n",
    "cal_metrics = json.load(open(OUTPUT_DIR / 'calibration_metrics.json'))\n",
    "temps = json.load(open(OUTPUT_DIR / 'temperatures.json'))\n",
    "auc_summary = json.load(open(OUTPUT_DIR / 'risk_coverage_auc.json'))\n",
    "uncertainty_auroc_data = json.load(open(OUTPUT_DIR / 'uncertainty_auroc.json'))\n",
    "\n",
    "print(\"\\n1. MÉTRICAS DE DETECCIÓN (mAP@[0.5:0.95])\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']:\n",
    "    mAP = det_metrics[method].get('mAP', 0.0)\n",
    "    AP50 = det_metrics[method].get('AP50', 0.0)\n",
    "    AP75 = det_metrics[method].get('AP75', 0.0)\n",
    "    print(f\"{method:25s}  mAP={mAP:.4f}  AP50={AP50:.4f}  AP75={AP75:.4f}\")\n",
    "\n",
    "print(\"\\n2. MÉTRICAS DE CALIBRACIÓN\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<25s} {'NLL ↓':>10s} {'Brier ↓':>10s} {'ECE ↓':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']:\n",
    "    nll = cal_metrics[method]['NLL']\n",
    "    brier = cal_metrics[method]['Brier']\n",
    "    ece = cal_metrics[method]['ECE']\n",
    "    print(f\"{method:<25s} {nll:>10.4f} {brier:>10.4f} {ece:>10.4f}\")\n",
    "\n",
    "print(\"\\n3. TEMPERATURAS OPTIMIZADAS\")\n",
    "print(\"-\" * 80)\n",
    "for method in ['baseline', 'mc_dropout', 'decoder_variance']:\n",
    "    T = temps[method]['T']\n",
    "    nll_before = temps[method]['nll_before']\n",
    "    nll_after = temps[method]['nll_after']\n",
    "    improvement = nll_before - nll_after\n",
    "    print(f\"{method:20s}  T={T:.4f}  NLL: {nll_before:.4f} → {nll_after:.4f} (Δ={improvement:.4f})\")\n",
    "\n",
    "print(\"\\n4. RISK-COVERAGE AUC (menor es mejor)\")\n",
    "print(\"-\" * 80)\n",
    "for method, auc in auc_summary.items():\n",
    "    print(f\"{method:25s}  AUC={auc:.4f}\")\n",
    "\n",
    "print(\"\\n5. INCERTIDUMBRE: AUROC TP vs FP (mayor es mejor)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<25s} {'AUROC ↑':>10s} {'Mean Unc TP':>15s} {'Mean Unc FP':>15s} {'Ratio':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "for method, data in uncertainty_auroc_data.items():\n",
    "    auroc = data['auroc']\n",
    "    mean_tp = data['mean_unc_tp']\n",
    "    mean_fp = data['mean_unc_fp']\n",
    "    ratio = mean_fp / mean_tp if mean_tp > 0 else 0\n",
    "    print(f\"{method:<25s} {auroc:>10.4f} {mean_tp:>15.6f} {mean_fp:>15.6f} {ratio:>10.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Baseline: rendimiento de referencia sin incertidumbre\")\n",
    "print(\"✓ Temperature Scaling: mejora calibración sin afectar mAP\")\n",
    "print(\"✓ MC-Dropout: proporciona incertidumbre epistémica (K pases)\")\n",
    "print(\"✓ Decoder variance: incertidumbre en single-pass (más eficiente)\")\n",
    "print(\"✓ Métodos+TS: mejor calibración manteniendo detección\")\n",
    "print(\"✓ AUROC TP vs FP: valida que incertidumbre detecta errores\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Guardar reporte final\n",
    "final_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'detection_metrics': det_metrics,\n",
    "    'calibration_metrics': cal_metrics,\n",
    "    'temperatures': temps,\n",
    "    'risk_coverage_auc': auc_summary,\n",
    "    'uncertainty_auroc': uncertainty_auroc_data\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'final_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nReporte final guardado en: {OUTPUT_DIR / 'final_report.json'}\")\n",
    "print(f\"Todos los artefactos en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cad14",
   "metadata": {},
   "source": [
    "## 15. Visualización Final Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ead03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. mAP Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "methods = ['baseline', 'baseline_ts', 'mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "mAPs = [det_metrics[m].get('mAP', 0.0) for m in methods]\n",
    "colors_map = ['lightblue', 'blue', 'lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars = ax1.bar(range(len(methods)), mAPs, color=colors_map, alpha=0.7)\n",
    "ax1.set_xticks(range(len(methods)))\n",
    "ax1.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=10)\n",
    "ax1.set_ylabel('mAP@[0.5:0.95]', fontsize=12)\n",
    "ax1.set_title('Comparación de mAP entre Métodos', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height, f'{mAPs[i]:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Calibration Metrics Comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "nlls = [cal_metrics[m]['NLL'] for m in methods]\n",
    "ax2.bar(range(len(methods)), nlls, color=colors_map, alpha=0.7)\n",
    "ax2.set_xticks(range(len(methods)))\n",
    "ax2.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax2.set_ylabel('NLL ↓', fontsize=11)\n",
    "ax2.set_title('Negative Log-Likelihood', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "briers = [cal_metrics[m]['Brier'] for m in methods]\n",
    "ax3.bar(range(len(methods)), briers, color=colors_map, alpha=0.7)\n",
    "ax3.set_xticks(range(len(methods)))\n",
    "ax3.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax3.set_ylabel('Brier Score ↓', fontsize=11)\n",
    "ax3.set_title('Brier Score', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "eces = [cal_metrics[m]['ECE'] for m in methods]\n",
    "ax4.bar(range(len(methods)), eces, color=colors_map, alpha=0.7)\n",
    "ax4.set_xticks(range(len(methods)))\n",
    "ax4.set_xticklabels([m.replace('_', '\\n') for m in methods], fontsize=8)\n",
    "ax4.set_ylabel('ECE ↓', fontsize=11)\n",
    "ax4.set_title('Expected Calibration Error', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Temperature Scaling Effect\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "base_methods = ['baseline', 'mc_dropout', 'decoder_variance']\n",
    "Ts = [temps[m]['T'] for m in base_methods]\n",
    "ax5.bar(range(len(base_methods)), Ts, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "ax5.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='T=1 (sin calibrar)')\n",
    "ax5.set_xticks(range(len(base_methods)))\n",
    "ax5.set_xticklabels([m.replace('_', '\\n') for m in base_methods], fontsize=10)\n",
    "ax5.set_ylabel('Temperature T', fontsize=11)\n",
    "ax5.set_title('Temperaturas Óptimas', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Risk-Coverage AUC\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "unc_methods = ['mc_dropout', 'mc_dropout_ts', 'decoder_variance', 'decoder_variance_ts']\n",
    "aucs = [auc_summary.get(m, 0.0) for m in unc_methods]\n",
    "colors_unc = ['lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars_auc = ax6.bar(range(len(unc_methods)), aucs, color=colors_unc, alpha=0.7)\n",
    "ax6.set_xticks(range(len(unc_methods)))\n",
    "ax6.set_xticklabels([m.replace('_', '\\n') for m in unc_methods], fontsize=9)\n",
    "ax6.set_ylabel('AUC (Risk-Coverage) ↓', fontsize=11)\n",
    "ax6.set_title('Risk-Coverage AUC', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars_auc):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height, f'{aucs[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 5. AUROC TP vs FP (Nueva sección)\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "auroc_methods = list(uncertainty_auroc_data.keys())\n",
    "aurocs = [uncertainty_auroc_data[m]['auroc'] for m in auroc_methods]\n",
    "colors_auroc = ['lightcoral', 'red', 'lightgreen', 'green']\n",
    "bars_auroc = ax7.bar(range(len(auroc_methods)), aurocs, color=colors_auroc, alpha=0.7)\n",
    "ax7.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Random')\n",
    "ax7.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax7.set_xticks(range(len(auroc_methods)))\n",
    "ax7.set_xticklabels([m.replace('_', '\\n') for m in auroc_methods], fontsize=9)\n",
    "ax7.set_ylabel('AUROC (FP detection) ↑', fontsize=11)\n",
    "ax7.set_title('AUROC: Detección de Errores', fontsize=12, fontweight='bold')\n",
    "ax7.legend(fontsize=8)\n",
    "ax7.grid(axis='y', alpha=0.3)\n",
    "ax7.set_ylim([0, 1])\n",
    "for i, bar in enumerate(bars_auroc):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height, f'{aurocs[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 6. Resumen de incertidumbre (ratio FP/TP)\n",
    "ax8 = fig.add_subplot(gs[3, :])\n",
    "ratios = [uncertainty_auroc_data[m]['mean_unc_fp'] / uncertainty_auroc_data[m]['mean_unc_tp'] \n",
    "          if uncertainty_auroc_data[m]['mean_unc_tp'] > 0 else 0 \n",
    "          for m in auroc_methods]\n",
    "bars_ratio = ax8.bar(range(len(auroc_methods)), ratios, color=colors_auroc, alpha=0.7)\n",
    "ax8.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Ratio = 1 (sin diferencia)')\n",
    "ax8.set_xticks(range(len(auroc_methods)))\n",
    "ax8.set_xticklabels([m.replace('_', '\\n') for m in auroc_methods], fontsize=10)\n",
    "ax8.set_ylabel('Ratio Mean(Unc_FP) / Mean(Unc_TP)', fontsize=11)\n",
    "ax8.set_title('Ratio de Incertidumbre: FP vs TP (>1 es deseable)', fontsize=12, fontweight='bold')\n",
    "ax8.legend()\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars_ratio):\n",
    "    height = bar.get_height()\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2., height, f'{ratios[i]:.2f}x', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Fase 5: Comparación Completa de Métodos de Incertidumbre y Calibración', \n",
    "             fontsize=16, fontweight='bold', y=0.997)\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'final_comparison_summary.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nVisualización final guardada en: {OUTPUT_DIR / 'final_comparison_summary.png'}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 5 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Todos los resultados guardados en: {OUTPUT_DIR}\")\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(\"  - config.yaml\")\n",
    "print(\"  - temperatures.json\")\n",
    "print(\"  - detection_metrics.json\")\n",
    "print(\"  - calibration_metrics.json\")\n",
    "print(\"  - risk_coverage_auc.json\")\n",
    "print(\"  - uncertainty_auroc.json\")\n",
    "print(\"  - uncertainty_auroc_comparison.csv\")\n",
    "print(\"  - final_report.json\")\n",
    "print(\"  - detection_comparison.csv\")\n",
    "print(\"  - calibration_comparison.csv\")\n",
    "print(\"  - reliability_diagrams.png\")\n",
    "print(\"  - risk_coverage_curves.png\")\n",
    "print(\"  - uncertainty_analysis.png\")\n",
    "print(\"  - final_comparison_summary.png\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
