# Proceso del Proyecto: Detecci√≥n de Objetos y Cuantificaci√≥n de Incertidumbre

## Descripci√≥n General

Este documento describe el **proceso paso a paso** de c√≥mo se desarroll√≥ el proyecto de detecci√≥n de objetos con vocabulario abierto (OVD) y cuantificaci√≥n de incertidumbre epist√©mica para conducci√≥n aut√≥noma. El proyecto se ejecut√≥ en **6 fases**, desde la preparaci√≥n de datos hasta la comparaci√≥n final de m√©todos.

---

## **FASE 1: Preparaci√≥n de Datos**

### ¬øQu√© se hizo?
Se prepar√≥ el dataset BDD100K para ser utilizado con el modelo OWLv2.

### Pasos realizados:
1. **Descarga del dataset**: Se obtuvo BDD100K con 70,000 im√°genes de conducci√≥n y sus anotaciones.
2. **Conversi√≥n de formato**: Se convirtieron las anotaciones de BDD100K al formato COCO (m√°s compatible con modelos modernos).
3. **Divisi√≥n de datos**: Se crearon tres conjuntos:
   - **Train**: 60% de las im√°genes (para entrenamiento si fuera necesario)
   - **Validation**: 20% (para ajustes y validaci√≥n)
   - **Test**: 20% (para evaluaci√≥n final)
4. **Preparaci√≥n de vocabulario**: Se definieron las 10 clases de objetos a detectar:
   - Peatones, ciclistas, veh√≠culos, sem√°foros, se√±ales de tr√°fico, motos, autobuses, camiones, trenes, y jinetes.

### Resultado:
Dataset listo para ser procesado por el modelo OWLv2.

---

## **FASE 2: Evaluaci√≥n Baseline (L√≠nea Base)**

### ¬øQu√© se hizo?
Se evalu√≥ el rendimiento del modelo OWLv2 **sin ninguna modificaci√≥n**, para establecer una l√≠nea base de comparaci√≥n.

### Pasos realizados:
1. **Carga del modelo**: Se carg√≥ OWLv2 (google/owlv2-large-patch14-ensemble) preentrenado.
2. **Inferencia est√°ndar**: Se procesaron 5,000 im√°genes del conjunto de validaci√≥n, generando predicciones (cajas delimitadoras, clases, y puntajes de confianza).
3. **C√°lculo de m√©tricas de detecci√≥n**:
   - **mAP** (mean Average Precision): 22.68%
   - **AP50**: 36.03%
   - **AP75**: 24.13%
4. **An√°lisis de rendimiento por clase**: Se identific√≥ qu√© clases se detectan mejor (ej. veh√≠culos) y cu√°les peor (ej. jinetes).

### Resultado:
- Se estableci√≥ el **rendimiento base** del modelo.
- **mAP = 22.68%**: Este es el valor de referencia para comparar mejoras futuras.

---

## **FASE 3: Implementaci√≥n de MC-Dropout**

### ¬øQu√© se hizo?
Se implement√≥ **MC-Dropout** para cuantificar la incertidumbre epist√©mica del modelo.

### Pasos realizados:

#### 3.1. Activaci√≥n de Dropout
1. Se modific√≥ el modelo OWLv2 para **mantener el dropout activo durante la inferencia** (normalmente est√° desactivado).
2. Se identificaron las capas con dropout en el transformer visual y de texto.

#### 3.2. Inferencia Estoc√°stica
1. **Se ejecutaron 50 pasadas forward** (forward passes) para cada imagen.
2. En cada pasada, el dropout enmascara diferentes neuronas aleatoriamente, generando predicciones ligeramente diferentes.
3. **Resultado por imagen**: 50 conjuntos de predicciones (cajas, clases, scores).

#### 3.3. C√°lculo de Incertidumbre
Para cada predicci√≥n final, se calcul√≥:
1. **Media de los scores**: Promedio de los 50 scores de confianza.
2. **Varianza de los scores**: Dispersi√≥n de los 50 scores (indica incertidumbre epist√©mica).
3. **Decoder Variance**: Varianza en las posiciones de las cajas predichas (indica incertidumbre espacial).

#### 3.4. An√°lisis de Resultados
Se realiz√≥ un an√°lisis exhaustivo:
1. **True Positives (TP)**: Detecciones correctas.
2. **False Positives (FP)**: Detecciones incorrectas.
3. **Distribuciones de incertidumbre**:
   - FPs tienen **mayor incertidumbre** que TPs.
   - Media de varianza en FPs: 0.000127
   - Media de varianza en TPs: 0.000063
4. **AUROC** (capacidad de discriminar TP/FP usando incertidumbre):
   - **Score Variance**: AUROC = 0.614 (√∫til, pero no excelente)
   - **Decoder Variance**: AUROC = 0.604

### Resultado:
- **MC-Dropout funciona**: La incertidumbre distingue parcialmente entre TP y FP.
- **mAP = 22.68%**: Se mantuvo igual que baseline (MC-Dropout no mejora precisi√≥n, solo a√±ade incertidumbre).
- **Datos guardados**: Se guardaron todas las predicciones con sus incertidumbres en archivos Parquet.

---

## **FASE 4: Calibraci√≥n con Temperature Scaling**

### ¬øQu√© se hizo?
Se calibr√≥ el modelo **baseline** usando Temperature Scaling para mejorar la confiabilidad de los scores de confianza.

### Pasos realizados:

#### 4.1. C√°lculo de ECE Inicial
1. Se evalu√≥ el **Expected Calibration Error (ECE)** del modelo baseline:
   - **ECE = 18.64%**: El modelo es bastante mal calibrado (los scores no reflejan bien la probabilidad real de acierto).

#### 4.2. B√∫squeda de Temperatura √ìptima
1. Se probaron diferentes valores de temperatura (T) en un rango de 0.1 a 5.0.
2. Se dividi√≥ el conjunto de validaci√≥n en:
   - **Calibration set**: Para encontrar la mejor T.
   - **Test set**: Para evaluar la mejora.
3. **Temperatura √≥ptima encontrada**: T = 1.52
   - Minimiza el ECE en el calibration set.

#### 4.3. Aplicaci√≥n de Temperature Scaling
1. Se ajustaron los scores de confianza dividiendo los logits por T = 1.52.
2. **Efecto**: Los scores se "suavizan" (menos extremos), mejorando la calibraci√≥n.

#### 4.4. Evaluaci√≥n de Resultados
1. **ECE despu√©s de TS**: 5.29% (reducci√≥n de 71.6% respecto al baseline).
2. **mAP**: Se mantuvo en 22.68% (TS no cambia la precisi√≥n, solo calibra los scores).
3. **Diagrama de confiabilidad**: Se observ√≥ que las predicciones calibradas est√°n m√°s cerca de la diagonal ideal.

### Resultado:
- **Baseline + TS est√° bien calibrado**: ECE = 5.29%.
- Los scores de confianza ahora reflejan mejor la probabilidad real de acierto.

---

## **FASE 5: Comparaci√≥n Final de M√©todos**

### ¬øQu√© se hizo?
Se compararon tres m√©todos:
1. **Baseline**: Modelo original sin modificaciones.
2. **Baseline + TS**: Modelo calibrado con Temperature Scaling (T = 1.52).
3. **MC-Dropout**: Modelo con cuantificaci√≥n de incertidumbre (50 pasadas).

### Pasos realizados:

#### 5.1. Evaluaci√≥n de Detecci√≥n (mAP)
Se midi√≥ la precisi√≥n de cada m√©todo:
- **Baseline**: mAP = 22.68%
- **Baseline + TS**: mAP = 22.68% (igual, TS no cambia detecci√≥n)
- **MC-Dropout**: mAP = 22.68% (igual, MC-Dropout no mejora precisi√≥n)

**Conclusi√≥n**: Todos los m√©todos tienen la misma precisi√≥n de detecci√≥n.

#### 5.2. Evaluaci√≥n de Calibraci√≥n (ECE)
Se midi√≥ qu√© tan bien calibrados est√°n los scores:
- **Baseline**: ECE = 18.64% (mal calibrado)
- **Baseline + TS**: ECE = 5.29% (bien calibrado)
- **MC-Dropout**: ECE = 18.82% (mal calibrado, similar a baseline)

**Conclusi√≥n**: Solo Baseline + TS est√° bien calibrado.

#### 5.3. Cuantificaci√≥n de Incertidumbre (AUROC)
Se midi√≥ la capacidad de identificar FPs usando incertidumbre:
- **Baseline**: No tiene medida de incertidumbre expl√≠cita.
- **Baseline + TS**: No a√±ade incertidumbre, solo calibra.
- **MC-Dropout**: AUROC = 0.614 (puede identificar FPs moderadamente bien).

**Conclusi√≥n**: Solo MC-Dropout cuantifica incertidumbre epist√©mica.

#### 5.4. An√°lisis de Costo Computacional
Se midi√≥ el tiempo de inferencia:
- **Baseline**: ~1.5 segundos/imagen (1x)
- **Baseline + TS**: ~1.5 segundos/imagen (1x, solo ajusta scores despu√©s)
- **MC-Dropout**: ~75 segundos/imagen (50x m√°s lento, por las 50 pasadas)

**Conclusi√≥n**: MC-Dropout es muy costoso computacionalmente.

#### 5.5. Recomendaciones Pr√°cticas
Se generaron recomendaciones basadas en el escenario de uso:

**Para sistemas de alerta al conductor** (requieren rapidez):
- Usar **Baseline + TS** (bien calibrado y r√°pido).
- Filtrar predicciones con score < 0.3.

**Para sistemas cr√≠ticos de seguridad** (requieren confiabilidad):
- Usar **MC-Dropout** (cuantifica incertidumbre).
- Filtrar predicciones con score_variance > 0.00009.
- Combinar con Baseline + TS si se necesita calibraci√≥n.

**Para mapeo/percepci√≥n no cr√≠tica**:
- Usar **Baseline** (r√°pido, sin calibraci√≥n necesaria).

### Resultado:
- **Comparaci√≥n completa** de los tres m√©todos.
- **Recomendaciones claras** para cada escenario de uso.
- **Visualizaciones generadas**: Diagramas de confiabilidad, comparaci√≥n de ECE, distribuciones de incertidumbre, etc.

---

## **FASE 6: Verificaci√≥n y Documentaci√≥n**

### ¬øQu√© se hizo?
Se verific√≥ que todo el proyecto estuviera completo y se documentaron todos los resultados.

### Pasos realizados:
1. **Verificaci√≥n de variables**: Se confirm√≥ que todas las variables clave estuvieran guardadas correctamente.
2. **Verificaci√≥n de resultados**: Se valid√≥ que todos los archivos de resultados (JSON, Parquet, im√°genes) existieran.
3. **Documentaci√≥n completa**: Se crearon m√∫ltiples archivos markdown con:
   - Explicaciones detalladas de conceptos.
   - Resultados num√©ricos y visualizaciones.
   - Recomendaciones pr√°cticas.
4. **Resumen ejecutivo**: Se gener√≥ un informe final consolidando todos los hallazgos.

### Resultado:
- Proyecto completado y documentado.
- Todos los resultados verificados y reproducibles.

---

## Resumen del Flujo de Trabajo

```
1. PREPARACI√ìN DE DATOS (Fase 1)
   ‚îî‚îÄ> Dataset BDD100K listo (70K im√°genes, 10 clases)

2. EVALUACI√ìN BASELINE (Fase 2)
   ‚îî‚îÄ> mAP = 22.68%, ECE = 18.64%
   
3. MC-DROPOUT (Fase 3)
   ‚îî‚îÄ> 50 pasadas forward por imagen
   ‚îî‚îÄ> Incertidumbre calculada (varianza de scores y decoder)
   ‚îî‚îÄ> AUROC = 0.614 (identifica FPs moderadamente)
   ‚îî‚îÄ> mAP = 22.68% (sin cambios en precisi√≥n)

4. CALIBRACI√ìN (Fase 4)
   ‚îî‚îÄ> Temperature Scaling aplicado (T = 1.52)
   ‚îî‚îÄ> ECE = 5.29% (mejora del 71.6%)
   ‚îî‚îÄ> mAP = 22.68% (sin cambios en precisi√≥n)

5. COMPARACI√ìN FINAL (Fase 5)
   ‚îî‚îÄ> Baseline: R√°pido, mal calibrado, sin incertidumbre
   ‚îî‚îÄ> Baseline + TS: R√°pido, bien calibrado, sin incertidumbre
   ‚îî‚îÄ> MC-Dropout: Lento, mal calibrado, con incertidumbre
   ‚îî‚îÄ> Recomendaciones por escenario de uso

6. VERIFICACI√ìN (Fase 6)
   ‚îî‚îÄ> Proyecto completo, documentado y verificado
```

---

## Archivos Clave Generados

### Datos y M√©tricas
- `fase 2/outputs/baseline_results.json`: M√©tricas del baseline
- `fase 3/outputs/mc_dropout/mc_stats_labeled.parquet`: Predicciones con incertidumbre
- `fase 3/outputs/mc_dropout/tp_fp_analysis.json`: An√°lisis TP/FP
- `fase 4/outputs/calibration/calibration_metrics.json`: M√©tricas de calibraci√≥n
- `fase 5/outputs/comparison/final_report.json`: Comparaci√≥n final de m√©todos
- `fase 5/outputs/comparison/temperatures.json`: Temperaturas √≥ptimas

### Visualizaciones
- `fase 5/outputs/comparison/final_comparison_summary.png`: Resumen visual de m√©todos
- `fase 5/outputs/comparison/reliability_diagrams.png`: Diagramas de confiabilidad
- `fase 5/outputs/comparison/ece_comparison.png`: Comparaci√≥n de ECE
- `fase 3/outputs/mc_dropout/uncertainty_distributions.png`: Distribuciones de incertidumbre

### Documentaci√≥n
- `README.md`: Descripci√≥n general del proyecto
- `FINAL_SUMMARY.md`: Resumen ejecutivo final
- `PROYECTO_COMPLETADO_FINAL.md`: Informe completo de resultados
- `resultados.md`: Resultados completos con explicaciones detalladas
- `resultados_2.md`: Este documento (proceso del proyecto)

---

## Conclusiones del Proceso

### ‚úÖ Lo que funcion√≥ bien:
1. **Divisi√≥n del trabajo en fases**: Facilit√≥ el seguimiento y la validaci√≥n.
2. **MC-Dropout**: Cuantifica incertidumbre de manera efectiva (AUROC = 0.614).
3. **Temperature Scaling**: Mejora significativamente la calibraci√≥n (ECE: 18.64% ‚Üí 5.29%).
4. **Documentaci√≥n exhaustiva**: Todos los pasos est√°n documentados y reproducibles.

### ‚ö†Ô∏è Limitaciones encontradas:
1. **MC-Dropout es muy lento**: 50x m√°s tiempo de inferencia (no viable para tiempo real).
2. **MC-Dropout mal calibrado**: ECE alto (18.82%), requerir√≠a calibraci√≥n adicional.
3. **mAP modesto**: 22.68% indica que el modelo podr√≠a mejorar con fine-tuning.

### üí° Aprendizajes clave:
1. **Calibraci√≥n ‚â† Precisi√≥n**: TS mejora calibraci√≥n pero no mAP.
2. **Incertidumbre ‚â† Precisi√≥n**: MC-Dropout a√±ade incertidumbre pero no mejora mAP.
3. **Trade-offs importantes**: Velocidad vs. incertidumbre, calibraci√≥n vs. costo computacional.
4. **Importancia del contexto**: La elecci√≥n del m√©todo depende del escenario de uso espec√≠fico.

---

## Pr√≥ximos Pasos Potenciales

Si se quisiera extender este proyecto:

1. **Fine-tuning del modelo**: Entrenar OWLv2 espec√≠ficamente en BDD100K para mejorar mAP.
2. **Optimizaci√≥n de MC-Dropout**: Reducir el n√∫mero de pasadas (ej. 10 en lugar de 50) para balance entre velocidad e incertidumbre.
3. **M√©todos h√≠bridos**: Combinar MC-Dropout con Temperature Scaling para tener tanto incertidumbre como calibraci√≥n.
4. **Ensembles**: Usar m√∫ltiples modelos en lugar de MC-Dropout para cuantificar incertidumbre.
5. **Pruebas en tiempo real**: Implementar en hardware de conducci√≥n aut√≥noma real para evaluar viabilidad pr√°ctica.
6. **An√°lisis de casos extremos**: Evaluar rendimiento en condiciones adversas (lluvia, noche, oclusiones).

---

**Fecha de finalizaci√≥n del proyecto**: Enero 2025  
**Autor**: Proyecto de detecci√≥n de objetos y cuantificaci√≥n de incertidumbre para conducci√≥n aut√≥noma  
**Objetivo alcanzado**: ‚úÖ Evaluaci√≥n completa de m√©todos de cuantificaci√≥n de incertidumbre y calibraci√≥n en detecci√≥n de objetos con vocabulario abierto.
