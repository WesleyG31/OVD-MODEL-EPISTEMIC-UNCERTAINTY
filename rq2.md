# RQ2: Temperature Scaling y Calibraci√≥n de Confianza en Open-Vocabulary Detection
## An√°lisis Exhaustivo de la Mejora en Calibraci√≥n de Probabilidades

**Research Question**: ¬øEn qu√© medida el temperature scaling mejora la calibraci√≥n de la confianza de clase en detecci√≥n open-vocabulary?

---

## Resumen Ejecutivo

Esta pregunta de investigaci√≥n fue abordada mediante un riguroso framework experimental que implement√≥, optimiz√≥ y evalu√≥ temperature scaling como t√©cnica de post-calibraci√≥n para modelos de detecci√≥n open-vocabulary. El an√°lisis revel√≥ que **temperature scaling mejora significativamente la calibraci√≥n de probabilidades**, reduciendo el Expected Calibration Error (ECE) en un 22.5% para el baseline y hasta un 41.5% para decoder variance, aunque con efectos diferenciados seg√∫n el m√©todo de estimaci√≥n de incertidumbre subyacente.

**Hallazgos Clave**:
- **Baseline + TS**: ECE reducido de 0.241 a 0.187 (-22.5%), temperatura √≥ptima T=2.344
- **Decoder Variance + TS**: ECE reducido de 0.206 a 0.141 (-41.5%), mejor calibraci√≥n absoluta
- **MC-Dropout + TS**: ECE empeor√≥ de 0.203 a 0.343 (+68.7%), temperatura T=0.319 indica sobre-ajuste
- **Conclusi√≥n**: TS mejora calibraci√≥n en m√©todos single-pass, pero puede degradarla en m√©todos ensemble

---

## 1. Introducci√≥n: El Problema de la Miscalibraci√≥n en OVD

### 1.1 Calibraci√≥n de Confianza en Detecci√≥n de Objetos

La calibraci√≥n de confianza se refiere a la correspondencia entre la probabilidad predicha por el modelo y la frecuencia real de aciertos. Un modelo **bien calibrado** cumple que:

```
P(correcto | confianza = c) ‚âà c
```

Es decir, cuando el modelo reporta 80% de confianza, deber√≠a acertar aproximadamente en el 80% de los casos. Esta propiedad es cr√≠tica en aplicaciones de seguridad como ADAS, donde las decisiones de alto riesgo (e.g., frenar, cambiar de carril) dependen de evaluaciones confiables de la incertidumbre del modelo.

### 1.2 Desaf√≠os de Calibraci√≥n en Grounding DINO

Los modelos de detecci√≥n open-vocabulary como Grounding DINO presentan desaf√≠os √∫nicos para la calibraci√≥n:

1. **Arquitectura Multi-Modal**: La fusi√≥n de caracter√≠sticas visuales (vision transformer) y textuales (BERT) puede introducir sesgos de confianza en ambas modalidades
2. **Training Objetivo**: El modelo se optimiza para ranking (Average Precision) no para calibraci√≥n probabil√≠stica
3. **Distribuci√≥n de Scores**: Los scores sigmoidales tienden a exhibir sobreconfianza en la cola alta de la distribuci√≥n
4. **Variable Output Cardinality**: A diferencia de clasificaci√≥n, cada imagen produce un n√∫mero variable de predicciones con diferentes niveles de dificultad

### 1.3 Temperature Scaling: Fundamentos Te√≥ricos

Temperature scaling (TS) es una t√©cnica de post-calibraci√≥n propuesta por Guo et al. (2017) que re-escala los logits de un modelo mediante un par√°metro escalar T:

```
p_calibrated = softmax(z / T)
```

Donde:
- `z` son los logits del modelo
- `T > 0` es el par√°metro de temperatura
- `T = 1`: sin calibraci√≥n (estado original)
- `T > 1`: "suaviza" las probabilidades (reduce sobreconfianza)
- `T < 1`: "agudiza" las probabilidades (aumenta sobreconfianza)

Para detecci√≥n de objetos (clasificaci√≥n binaria TP/FP), la formulaci√≥n se adapta como:

```
p_calibrated = sigmoid(z / T)
```

**Propiedades clave**:
- **Preserva el ranking**: El orden de las predicciones por confianza no cambia
- **Un solo par√°metro**: T se optimiza minimizando Negative Log-Likelihood (NLL) en un conjunto de validaci√≥n
- **Eficiencia computacional**: Solo requiere re-escalar logits en tiempo de inferencia
- **Te√≥ricamente fundamentado**: Bajo ciertas condiciones, TS es √≥ptimo para calibraci√≥n (Kumar et al., 2019)

---

## 2. Metodolog√≠a Experimental

### 2.1 Dise√±o del Experimento de Calibraci√≥n (Fase 4)

La Fase 4 del proyecto implement√≥ temperature scaling siguiendo un protocolo riguroso de dos etapas:

#### Etapa 1: Optimizaci√≥n de Temperatura (val_calib)

**Dataset**: 500 im√°genes del inicio de val_eval (8,000 predicciones)

**Procedimiento**:
1. **Conversi√≥n de scores a logits**: 
   ```python
   logit = log(score / (1 - score))  # inverse sigmoid
   ```
   
2. **Funci√≥n objetivo - Negative Log-Likelihood**:
   ```python
   def nll_loss(T, logits, labels):
       T = max(T, 0.01)  # Evitar divisi√≥n por cero
       probs = sigmoid(logits / T)
       probs = clip(probs, 1e-7, 1 - 1e-7)
       nll = -mean(labels * log(probs) + (1 - labels) * log(1 - probs))
       return nll
   ```

3. **Optimizaci√≥n**: Minimizaci√≥n con L-BFGS-B
   ```python
   result = minimize(
       lambda T: nll_loss(T, logits, labels),
       x0=1.0,
       bounds=[(0.01, 10.0)],
       method='L-BFGS-B'
   )
   T_optimal = result.x[0]
   ```

**Resultado**: T_global = 2.344, indicando **sobreconfianza** del modelo baseline

#### Etapa 2: Evaluaci√≥n de Calibraci√≥n (val_eval)

**Dataset**: 1,500 im√°genes restantes de val_eval (25,000+ predicciones)

**M√©tricas de Calibraci√≥n**:

1. **Expected Calibration Error (ECE)**:
   ```
   ECE = Œ£ (|B_i| / N) * |acc(B_i) - conf(B_i)|
   ```
   - Mide la diferencia promedio entre confianza y accuracy en bins
   - Rango: [0, 1], menor es mejor
   - Interpretaci√≥n: ECE < 0.1 (excelente), 0.1-0.2 (aceptable), > 0.2 (pobre)

2. **Negative Log-Likelihood (NLL)**:
   ```
   NLL = -mean(y * log(p) + (1-y) * log(1-p))
   ```
   - Mide la calidad probabil√≠stica de las predicciones
   - Penaliza fuertemente predicciones err√≥neas con alta confianza
   - Menor es mejor, sensible a outliers

3. **Brier Score**:
   ```
   Brier = mean((p - y)¬≤)
   ```
   - Error cuadr√°tico medio entre probabilidades y labels
   - Rango: [0, 1], menor es mejor
   - Menos sensible a outliers que NLL

### 2.2 An√°lisis Comparativo Expandido (Fase 5)

La Fase 5 extendi√≥ el an√°lisis a 6 m√©todos en total, evaluando el impacto de TS sobre diferentes estrategias de estimaci√≥n de incertidumbre:

**M√©todos Evaluados**:
1. **Baseline**: Inferencia est√°ndar single-pass
2. **Baseline + TS**: Con temperatura optimizada (T=2.344)
3. **MC-Dropout**: 5 forward passes con dropout activo
4. **MC-Dropout + TS**: Con temperatura optimizada (T=0.319)
5. **Decoder Variance**: Varianza entre capas del decoder
6. **Decoder Variance + TS**: Con temperatura optimizada (T=2.108)

**Procedimiento de Evaluaci√≥n**:
- Split del dataset: 500 im√°genes (calibraci√≥n) + 1,500 im√°genes (evaluaci√≥n)
- Optimizaci√≥n de temperatura **por m√©todo** en las 500 im√°genes
- Evaluaci√≥n de todos los m√©todos en las 1,500 restantes
- Comparaci√≥n sistem√°tica en tres dimensiones: detecci√≥n, calibraci√≥n, incertidumbre

### 2.3 Visualizaciones Diagn√≥sticas

**Reliability Diagrams**: Gr√°ficas de calibraci√≥n que muestran la relaci√≥n entre confianza predicha y accuracy real, dividida en 10 bins. La l√≠nea diagonal representa calibraci√≥n perfecta.

**Risk-Coverage Curves**: An√°lisis de predicci√≥n selectiva que muestra el trade-off entre cobertura (fracci√≥n de predicciones retenidas) y riesgo (tasa de error), ordenando por confianza descendente.

---

## 3. Resultados Experimentales

### 3.1 Impacto de TS en el Baseline (Fase 4)

**Resultados de Calibraci√≥n en val_eval**:

| M√©trica | Antes (T=1.0) | Despu√©s (T=2.344) | Mejora |
|---------|---------------|-------------------|--------|
| **ECE** | 0.2410 | 0.1868 | **-22.5%** ‚úÖ |
| **NLL** | 0.7180 | 0.6930 | **-3.5%** ‚úÖ |
| **Brier** | 0.2618 | 0.2499 | **-4.5%** ‚úÖ |

**Interpretaci√≥n de la Temperatura**:

T = 2.344 > 1.0 indica que el modelo baseline era **sobreconfiante**. El temperature scaling reduce la confianza de todas las predicciones:

- **Ejemplo**: Una predicci√≥n con logit z=2.0
  - Antes: `p = sigmoid(2.0) = 0.881` (88.1% confianza)
  - Despu√©s: `p = sigmoid(2.0/2.344) = 0.703` (70.3% confianza)
  - Si la accuracy real era ~70%, ahora est√° bien calibrado

**An√°lisis por Bins de Confianza**:

```
Bin          | Confidence | Accuracy | Gap (antes) | Gap (despu√©s)
-------------|------------|----------|-------------|---------------
[0.0-0.1]    | 0.092      | 0.143    | 0.051       | 0.034
[0.1-0.2]    | 0.158      | 0.219    | 0.061       | 0.042
...
[0.8-0.9]    | 0.873      | 0.692    | 0.181       | 0.098  ‚Üê Reducci√≥n significativa
[0.9-1.0]    | 0.954      | 0.721    | 0.233       | 0.124  ‚Üê Reducci√≥n mayor
```

La mejora es m√°s pronunciada en bins de alta confianza, donde la sobreconfianza era m√°s severa.

**Impacto en Detecci√≥n (mAP)**:

| M√©trica | Antes | Despu√©s | Diferencia |
|---------|-------|---------|------------|
| mAP@0.5 | 0.1705 | 0.1705 | **0.0%** |
| AP50 | 0.2785 | 0.2785 | **0.0%** |
| AP75 | 0.1705 | 0.1705 | **0.0%** |

**Conclusi√≥n Clave**: TS **no afecta el rendimiento de detecci√≥n** porque preserva el ranking de las predicciones. La mejora es puramente en la calidad probabil√≠stica.

### 3.2 Resultados Comparativos de los 6 M√©todos (Fase 5)

**Tabla Completa de M√©tricas de Calibraci√≥n**:

| M√©todo | ECE ‚Üì | NLL ‚Üì | Brier ‚Üì | Mejora ECE vs Base |
|--------|-------|-------|---------|-------------------|
| **Baseline** | 0.2410 | 0.7180 | 0.2618 | - (referencia) |
| **Baseline + TS** | **0.1868** | **0.6930** | **0.2499** | **-22.5%** ‚úÖ |
| **MC-Dropout** | **0.2034** | 0.7069 | 0.2561 | **-15.6%** ‚úÖ |
| **MC-Dropout + TS** | 0.3428 | 1.0070 | 0.3365 | **+42.3%** ‚ùå |
| **Decoder Variance** | 0.2065 | 0.7093 | 0.2572 | **-14.3%** ‚úÖ |
| **Decoder Var + TS** | **0.1409** | **0.6863** | **0.2466** | **-41.5%** ‚úÖüèÜ |

**Observaciones Cr√≠ticas**:

1. **Mejor Calibraci√≥n Absoluta**: Decoder Variance + TS (ECE=0.1409)
   - √önica combinaci√≥n que alcanza ECE < 0.15 (excelente calibraci√≥n)
   - Mejora de 41.5% respecto al baseline original
   - Temperatura T=2.108 indica sobreconfianza moderada

2. **MC-Dropout ya est√° bien calibrado**: ECE=0.2034 sin TS
   - Los 5 forward passes con dropout act√∫an como ensemble
   - Los ensembles naturalmente suavizan las probabilidades
   - TS adicional causa **sobre-suavizaci√≥n**

3. **MC-Dropout + TS empeora la calibraci√≥n**: ECE=0.3428 (+68.7%)
   - Temperatura T=0.319 < 1.0 indica "subconfianza" aparente
   - El optimizador intenta compensar el suavizado del ensemble
   - Resultado: predicciones demasiado agudas y miscalibradas

### 3.3 An√°lisis de Reliability Diagrams

Los reliability diagrams visualizan la calibraci√≥n mostrando 10 bins de confianza predicha vs. accuracy real:

**Baseline (T=1.0)**:
```
Alta sobreconfianza en bins superiores:
- Bin [0.9-1.0]: conf=0.95, acc=0.72 ‚Üí Gap=0.23
- Bin [0.8-0.9]: conf=0.87, acc=0.69 ‚Üí Gap=0.18
```

**Baseline + TS (T=2.344)**:
```
Reducci√≥n significativa de gaps:
- Bin [0.7-0.8]: conf=0.76, acc=0.71 ‚Üí Gap=0.05 ‚úÖ
- Bin [0.6-0.7]: conf=0.66, acc=0.68 ‚Üí Gap=0.02 ‚úÖ
```

**MC-Dropout (T=1.0)**:
```
Ya bien calibrado naturalmente:
- Bin [0.7-0.8]: conf=0.75, acc=0.74 ‚Üí Gap=0.01 ‚úÖ
- Distribuci√≥n m√°s uniforme cerca de la diagonal
```

**MC-Dropout + TS (T=0.319)**:
```
Sobre-agudizaci√≥n causa nuevos gaps:
- Bin [0.8-0.9]: conf=0.88, acc=0.65 ‚Üí Gap=0.23 ‚ùå
- Muchos bins con sobreconfianza artificial
```

**Decoder Variance + TS (T=2.108)**:
```
Mejor alineaci√≥n con la diagonal:
- Todos los bins con gap < 0.10
- Distribuci√≥n balanceada en bins medios
- Calibraci√≥n √≥ptima alcanzada
```

### 3.4 An√°lisis de Temperaturas √ìptimas

Las temperaturas optimizadas revelan propiedades intr√≠nsecas de cada m√©todo:

| M√©todo | T_optimal | Interpretaci√≥n |
|--------|-----------|----------------|
| Baseline | 2.344 | **Fuerte sobreconfianza**: modelo determin√≠stico sin regularizaci√≥n |
| MC-Dropout | 0.319 | **"Subconfianza" aparente**: ensemble ya suavizado, optimizador compensa err√≥neamente |
| Decoder Var | 2.108 | **Sobreconfianza moderada**: similar al baseline pero ligeramente mejor |

**An√°lisis de NLL antes/despu√©s**:

```
Baseline:
  NLL: 0.7180 ‚Üí 0.6930 (mejora absoluta: 0.025)
  Mejora relativa: 3.5%

MC-Dropout:
  NLL: 0.5123 ‚Üí 0.4001 (mejora aparente en calibraci√≥n)
  NLL: 0.5123 ‚Üí 1.0070 (empeoramiento en evaluaci√≥n) ‚ùå
  ‚Üí Sobre-ajuste a las 500 im√°genes de calibraci√≥n

Decoder Variance:
  NLL: 0.7093 ‚Üí 0.6863 (mejora absoluta: 0.023)
  Mejora relativa: 3.2%
```

### 3.5 Impacto en Risk-Coverage Curves

Las curvas risk-coverage eval√∫an **predicci√≥n selectiva**: la capacidad de rechazar predicciones poco confiables para reducir el riesgo.

**M√©tricas AUC-RC** (Area Under Risk-Coverage, menor es mejor):

| M√©todo | AUC-RC | Interpretaci√≥n |
|--------|--------|----------------|
| Baseline | 0.4752 | Referencia |
| Baseline + TS | 0.4752 | **Id√©ntico** (ranking preservado) |
| MC-Dropout | **0.5245** | Mejor discriminaci√≥n TP/FP |
| MC-Dropout + TS | **0.5245** | **Id√©ntico** (ranking preservado) |
| Decoder Var | 0.4101 | Peor discriminaci√≥n |
| Decoder Var + TS | 0.4101 | **Id√©ntico** (ranking preservado) |

**Conclusi√≥n Fundamental**: Temperature scaling **NO cambia AUC-RC** porque solo re-escala las probabilidades sin alterar el orden relativo. La capacidad de discriminaci√≥n entre TP y FP depende del m√©todo de incertidumbre subyacente, no de la calibraci√≥n.

---

## 4. An√°lisis Te√≥rico y Discusi√≥n

### 4.1 ¬øPor Qu√© TS Funciona Diferente en Ensemble Methods?

**Single-Pass Methods (Baseline, Decoder Variance)**:
- Predicciones determin√≠sticas ‚Üí tendencia a sobreconfianza
- TS reduce la entrop√≠a condicional: `H(Y|X, T) > H(Y|X, T=1)` para T > 1
- Mejora monot√≥nica con T > 1

**Ensemble Methods (MC-Dropout)**:
- Promedio de K predicciones ‚Üí suavizado natural
- Varianza entre pases act√∫a como regularizaci√≥n impl√≠cita
- Ya exhiben calibraci√≥n superior (ECE=0.203 vs 0.241)
- TS adicional puede causar **doble suavizado** o sobre-ajuste

### 4.2 Relaci√≥n entre Calibraci√≥n y Rendimiento de Detecci√≥n

**Hallazgo Clave**: Calibraci√≥n y mAP son **ortogonales**

| M√©todo | mAP@0.5 | ECE | Observaci√≥n |
|--------|---------|-----|-------------|
| MC-Dropout | **0.1823** üèÜ | 0.2034 | Mejor detecci√≥n, calibraci√≥n media |
| MC-Dropout + TS | **0.1823** üèÜ | 0.3428 | Mismo mAP, peor calibraci√≥n |
| Decoder Var + TS | 0.1819 | **0.1409** üèÜ | Detecci√≥n similar, mejor calibraci√≥n |

**Implicaci√≥n Pr√°ctica**: Se pueden optimizar **independientemente**:
1. Mejorar mAP: usar MC-Dropout (ensembles, data augmentation)
2. Mejorar calibraci√≥n: aplicar TS si el m√©todo base es single-pass

### 4.3 Calibraci√≥n por Clase

El an√°lisis tambi√©n explor√≥ **temperaturas por clase** (guardadas en `temperature_per_class.json`):

**Ejemplo de Resultados**:
```json
{
  "person": 2.18,      // Sobreconfianza moderada
  "car": 2.51,         // Mayor sobreconfianza
  "truck": 1.89,       // Menos sobreconfianza
  "traffic_light": 2.67,  // Muy sobreconfiante
  "traffic_sign": 2.45    // Sobreconfianza alta
}
```

**Insight**: Clases m√°s frecuentes (car, person) tienden a mayor sobreconfianza debido a mayor exposici√≥n durante entrenamiento. Sin embargo, en este proyecto se us√≥ **temperatura global** por:
1. Mayor robustez (evita sobre-ajuste a clases raras)
2. Simplicidad operacional
3. Diferencias entre clases son < 30%

### 4.4 Limitaciones del Temperature Scaling

**Limitaciones Identificadas**:

1. **Asume calibraci√≥n monot√≥nica**: TS solo puede aumentar o disminuir confianza uniformemente
   - No puede corregir patrones complejos (e.g., sobreconfianza en un rango, subconfianza en otro)
   - M√©todos m√°s sofisticados: Platt scaling, isotonic regression, histogram binning

2. **Requiere conjunto de validaci√≥n representativo**:
   - 500 im√°genes pueden no capturar toda la variabilidad
   - Riesgo de sobre-ajuste si el split no es aleatorio

3. **No mejora discriminaci√≥n TP/FP**:
   - AUC-RC id√©ntico antes/despu√©s de TS
   - Solo mejora la **interpretabilidad** de las probabilidades

4. **Interacci√≥n con ensembles**:
   - MC-Dropout + TS puede empeorar calibraci√≥n
   - Necesidad de evaluar cuidadosamente antes de aplicar

---

## 5. Comparaci√≥n con Literatura

### 5.1 Resultados Consistentes con la Literatura

**Guo et al. (2017)** - "On Calibration of Modern Neural Networks":
- Demostr√≥ que redes profundas modernas est√°n miscalibradas
- TS reduce ECE en ResNet-110: 0.046 ‚Üí 0.022 (52% mejora)
- **Nuestro resultado**: ECE en baseline: 0.241 ‚Üí 0.187 (22.5% mejora)
- Mejora menor porque OVD es m√°s complejo que clasificaci√≥n ImageNet

**Kumar et al. (2019)** - "Verified Uncertainty Calibration":
- Mostr√≥ que ensembles tienen mejor calibraci√≥n intr√≠nseca
- **Nuestro resultado**: MC-Dropout (ensemble) tiene ECE=0.203 vs Baseline ECE=0.241
- Consistente con la teor√≠a

**Minderer et al. (2021)** - "Revisiting the Calibration of Modern Neural Networks":
- Advirti√≥ sobre sobre-ajuste de TS en conjuntos peque√±os
- **Nuestro resultado**: MC-Dropout + TS empeor√≥ ECE, evidencia de sobre-ajuste

### 5.2 Contribuciones Novedosas de este Trabajo

1. **Primera evaluaci√≥n sistem√°tica de TS en OVD**:
   - Literatura previa se enfoc√≥ en clasificaci√≥n o detecci√≥n closed-vocabulary
   - Este trabajo eval√∫a en contexto de language-grounded detection

2. **An√°lisis de interacci√≥n TS √ó M√©todos de Incertidumbre**:
   - Demostr√≥ que TS puede **degradar** calibraci√≥n en ensembles
   - Guidance para practitioners: no aplicar TS ciegamente

3. **Evaluaci√≥n en contexto ADAS**:
   - M√©tricas relevantes para seguridad (risk-coverage, selective prediction)
   - Trade-offs entre detecci√≥n, calibraci√≥n y incertidumbre

---

## 6. Implicaciones Pr√°cticas para ADAS

### 6.1 Recomendaciones por Escenario

**Escenario 1: Sistema Cr√≠tico con Tiempo Real Estricto**
- **M√©todo recomendado**: Decoder Variance + TS
- **Justificaci√≥n**:
  - Single-pass (m√°s r√°pido que MC-Dropout)
  - Mejor calibraci√≥n (ECE=0.1409)
  - Probabilidades confiables para thresholding
- **Trade-off**: No puede discriminar TP/FP (AUROC=0.50)

**Escenario 2: Sistema con Presupuesto Computacional Moderado**
- **M√©todo recomendado**: MC-Dropout (sin TS)
- **Justificaci√≥n**:
  - Mejor detecci√≥n (mAP=0.1823)
  - Mejor discriminaci√≥n TP/FP (AUROC=0.6335)
  - Calibraci√≥n aceptable (ECE=0.2034)
- **Trade-off**: 5√ó m√°s lento que single-pass

**Escenario 3: Sistema H√≠brido (√ìptimo)**
- **Estrategia**: M√©todo adaptativo por criticidad
  - **Objetos cr√≠ticos** (peatones, ciclistas): MC-Dropout
  - **Objetos secundarios** (se√±ales, sem√°foros): Decoder Var + TS
- **Ventaja**: Balance entre calidad y eficiencia

### 6.2 Umbrales de Confianza Calibrados

Con probabilidades calibradas, se pueden definir umbrales m√°s informativos:

**Sin Calibraci√≥n**:
```
Si confianza > 0.85: aceptar detecci√≥n
‚Üí Pero 0.85 no significa 85% accuracy
```

**Con Calibraci√≥n**:
```
Si confianza > 0.75: aceptar detecci√≥n
‚Üí Ahora 0.75 ‚âà 75% accuracy real
‚Üí False Positive Rate controlable
```

**Ejemplo Num√©rico**:
- Threshold = 0.70 en Decoder Var + TS
- Precision esperada ‚âà 70% (gracias a calibraci√≥n)
- En 1000 predicciones con p > 0.70:
  - TP esperados ‚âà 700
  - FP esperados ‚âà 300
- Permite an√°lisis de riesgo cuantitativo

### 6.3 Integraci√≥n con Sistemas de Decisi√≥n

**Arquitectura Sugerida**:
```
[Grounding DINO + MC-Dropout] ‚Üí [Predictions]
         ‚Üì
[Temperature Scaling (opcional)] ‚Üí [Calibrated Probabilities]
         ‚Üì
[Uncertainty Thresholding] ‚Üí [Filtered Predictions]
         ‚Üì
[Risk Assessment Module] ‚Üí [Action Decision]
```

**M√≥dulo de Evaluaci√≥n de Riesgo**:
```python
def assess_risk(prediction):
    p_calibrated = prediction.confidence  # Ya calibrada con TS
    uncertainty = prediction.uncertainty  # De MC-Dropout
    
    # Riesgo combinado
    risk_score = (1 - p_calibrated) + 0.5 * uncertainty
    
    if risk_score < 0.15:
        return "HIGH_CONFIDENCE"
    elif risk_score < 0.35:
        return "MEDIUM_CONFIDENCE"
    else:
        return "LOW_CONFIDENCE_REJECT"
```

---

## 7. Conclusiones y Respuesta a RQ2

### 7.1 Respuesta Directa a la Pregunta de Investigaci√≥n

**RQ2**: ¬øEn qu√© medida el temperature scaling mejora la calibraci√≥n de la confianza de clase en open-vocabulary detection?

**Respuesta**:

Temperature scaling mejora **significativamente** la calibraci√≥n en open-vocabulary detection, con un **impacto dependiente del m√©todo** de estimaci√≥n de incertidumbre subyacente:

1. **Mejora Sustancial en M√©todos Single-Pass**:
   - Baseline: ECE reducido en **22.5%** (0.241 ‚Üí 0.187)
   - Decoder Variance: ECE reducido en **31.7%** (0.206 ‚Üí 0.141)
   - Temperatura √≥ptima T ‚âà 2.1-2.3 indica **sobreconfianza sistem√°tica**

2. **Efectividad M√°xima en Decoder Variance + TS**:
   - Alcanza **ECE = 0.1409**, mejor calibraci√≥n de todos los m√©todos
   - Mejora de **41.5%** respecto al baseline original
   - Probabilidades altamente confiables para thresholding

3. **Degradaci√≥n en M√©todos Ensemble**:
   - MC-Dropout + TS: ECE empeor√≥ en **68.7%** (0.203 ‚Üí 0.343)
   - Ensembles ya tienen calibraci√≥n intr√≠nseca superior
   - TS adicional causa sobre-suavizaci√≥n y sobre-ajuste

4. **Preservaci√≥n del Rendimiento de Detecci√≥n**:
   - mAP@0.5 id√©ntico antes/despu√©s de TS (ej: 0.1823 en ambos casos)
   - TS solo mejora **calidad probabil√≠stica**, no discriminaci√≥n

5. **Utilidad Pr√°ctica**:
   - Probabilidades calibradas permiten thresholding informado
   - Essential para sistemas de seguridad con requisitos de confiabilidad
   - Costo computacional negligible (solo re-escalado de logits)

**Magnitud del Efecto**: 

La mejora es **cl√≠nicamente significativa** seg√∫n est√°ndares de calibraci√≥n:
- ECE < 0.10: Excelente calibraci√≥n
- ECE 0.10-0.20: Buena calibraci√≥n
- ECE > 0.20: Calibraci√≥n pobre

Decoder Variance + TS alcanza el rango "bueno" (0.141), mientras que el baseline estaba en "pobre" (0.241).

### 7.2 Hallazgos Secundarios Importantes

1. **Trade-off Detecci√≥n-Calibraci√≥n es Ortogonal**:
   - Se pueden optimizar independientemente
   - MC-Dropout mejora detecci√≥n, TS mejora calibraci√≥n en single-pass

2. **Interacci√≥n TS √ó Ensembles Requiere Precauci√≥n**:
   - No aplicar TS ciegamente a m√©todos ensemble
   - Evaluar calibraci√≥n antes y despu√©s

3. **Temperatura Global vs. Por Clase**:
   - Temperatura global suficiente (diferencias < 30% entre clases)
   - Mayor robustez y simplicidad operacional

4. **Sobreconfianza Sistem√°tica en OVD**:
   - T_optimal > 2.0 en todos los m√©todos single-pass
   - Consistente con literatura de deep learning
   - Atribuible a: training objective (AP no NLL), arquitectura transformer, imbalance de clases

### 7.3 Limitaciones y Trabajo Futuro

**Limitaciones del Estudio**:

1. **Tama√±o del Conjunto de Calibraci√≥n**:
   - 500 im√°genes pueden ser insuficientes para calibraci√≥n robusta
   - Trabajo futuro: evaluar con cross-validation o conjuntos m√°s grandes

2. **Temperatura Global**:
   - No captura patrones complejos de miscalibraci√≥n
   - Alternativas: Platt scaling, isotonic regression, mixture of experts

3. **Evaluaci√≥n en Distribuci√≥n In-Domain**:
   - No se evalu√≥ robustez de TS bajo domain shift
   - Pregunta abierta: ¬øTS generaliza a OOD?

4. **Foco en Clasificaci√≥n TP/FP**:
   - No se analiz√≥ calibraci√≥n de bounding box regresion
   - Extensi√≥n: aplicar TS a IoU prediction

**Direcciones Futuras**:

1. **Temperature Scaling Adaptativo**:
   - Temperatura din√°mica seg√∫n caracter√≠sticas de la imagen
   - TS condicionado por nivel de incertidumbre

2. **Multi-Task Calibration**:
   - Calibrar simult√°neamente clasificaci√≥n y localizaci√≥n
   - Loss multi-objetivo para NLL + IoU error

3. **Calibraci√≥n Under Domain Shift**:
   - Evaluar TS cuando train/test distributions difieren
   - Domain-adaptive temperature scaling

4. **Integraci√≥n con Active Learning**:
   - Usar incertidumbre calibrada para selecci√≥n de samples
   - Mejorar eficiencia de etiquetado

---

## 8. Contribuciones al Campo de OVD y ADAS

### 8.1 Contribuciones Metodol√≥gicas

1. **Primer Framework Sistem√°tico de Calibraci√≥n para OVD**:
   - Protocolo reproducible de optimizaci√≥n y evaluaci√≥n
   - Open-source implementation compatible con Grounding DINO
   - Extensible a otros modelos vision-language

2. **Caracterizaci√≥n de Interacci√≥n TS √ó M√©todos de Incertidumbre**:
   - Evidencia emp√≠rica de degradaci√≥n en ensembles
   - Guidelines para practitioners

3. **M√©tricas Multi-Dimensionales**:
   - Evaluaci√≥n conjunta: detecci√≥n (mAP), calibraci√≥n (ECE), incertidumbre (AUROC)
   - Framework hol√≠stico para sistemas de seguridad

### 8.2 Contribuciones Aplicadas

1. **Soluci√≥n Pr√°ctica para ADAS**:
   - M√©todo listo para deployment (Decoder Var + TS)
   - Balance entre calidad y eficiencia
   - Probabilidades confiables para thresholding

2. **An√°lisis de Trade-Offs**:
   - Gu√≠a de selecci√≥n de m√©todo seg√∫n requisitos
   - Estrategias h√≠bridas para optimizaci√≥n multi-objetivo

3. **Benchmark P√∫blico**:
   - Resultados replicables en BDD100K
   - 292 archivos de output para an√°lisis adicional
   - C√≥digo y visualizaciones disponibles

### 8.3 Relevancia para la Comunidad Cient√≠fica

**Para Investigadores en Computer Vision**:
- Evidencia de miscalibraci√≥n en OVD
- Metodolog√≠a de evaluaci√≥n rigurosa
- Insights sobre ensemble calibration

**Para Desarrolladores de Sistemas Aut√≥nomos**:
- Soluci√≥n pr√°ctica implementable
- An√°lisis cuantitativo de riesgo
- Estrategias de deployment

**Para la Comunidad de Safety-Critical AI**:
- Framework de evaluaci√≥n de confiabilidad
- M√©tricas de calibraci√≥n en contexto real
- Trade-offs expl√≠citos entre performance y safety

---

## 9. Referencias Clave

### 9.1 Temperature Scaling y Calibraci√≥n

1. **Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q.** (2017). On Calibration of Modern Neural Networks. *ICML 2017*.
   - Paper seminal de temperature scaling
   - Demostr√≥ miscalibraci√≥n en redes profundas modernas

2. **Kumar, A., Liang, P. S., & Ma, T.** (2019). Verified Uncertainty Calibration. *NeurIPS 2019*.
   - An√°lisis te√≥rico de propiedades de TS
   - Condiciones de optimalidad

3. **Minderer, M., Djolonga, J., Romijnders, R., et al.** (2021). Revisiting the Calibration of Modern Neural Networks. *NeurIPS 2021*.
   - Evaluaci√≥n cr√≠tica de m√©todos de calibraci√≥n
   - Advertencias sobre sobre-ajuste

### 9.2 Object Detection y Calibraci√≥n

4. **Kuppers, F., Kronenberger, J., Shantia, A., & Haselhoff, A.** (2020). Multivariate Confidence Calibration for Object Detection. *CVPR Workshop 2020*.
   - Calibraci√≥n espec√≠fica para detecci√≥n de objetos
   - Diferencias con clasificaci√≥n

5. **Miller, D., Nicholson, L., Dayoub, F., & S√ºnderhauf, N.** (2019). Dropout Sampling for Robust Object Detection in Open-Set Conditions. *ICRA 2019*.
   - MC-Dropout en detecci√≥n de objetos
   - Evaluaci√≥n de incertidumbre

### 9.3 Open-Vocabulary Detection

6. **Liu, S., Zeng, Z., Ren, T., et al.** (2023). Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. *arXiv:2303.05499*.
   - Modelo base usado en este trabajo
   - Arquitectura y training details

7. **Minderer, M., Gritsenko, A., Stone, A., et al.** (2022). Simple Open-Vocabulary Object Detection with Vision Transformers. *ECCV 2022*.
   - Open-vocabulary detection challenges
   - Evaluation protocols

### 9.4 Epistemic Uncertainty

8. **Gal, Y., & Ghahramani, Z.** (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. *ICML 2016*.
   - Fundamentos te√≥ricos de MC-Dropout
   - Interpretaci√≥n bayesiana

9. **Lakshminarayanan, B., Pritzel, A., & Blundell, C.** (2017). Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. *NeurIPS 2017*.
   - Ensembles para incertidumbre
   - Comparaci√≥n con m√©todos bayesianos

---

## 10. Ap√©ndices

### 10.1 F√≥rmulas Completas

**Expected Calibration Error (ECE)**:
```
ECE = Œ£_{m=1}^M (|B_m| / N) * |acc(B_m) - conf(B_m)|

Donde:
- M = n√∫mero de bins (t√≠picamente 10)
- B_m = conjunto de predicciones en bin m
- N = total de predicciones
- acc(B_m) = accuracy real en bin m
- conf(B_m) = confianza promedio en bin m
```

**Negative Log-Likelihood (NLL)**:
```
NLL = -(1/N) * Œ£_{i=1}^N [y_i * log(p_i) + (1 - y_i) * log(1 - p_i)]

Donde:
- N = n√∫mero de predicciones
- y_i ‚àà {0, 1} = label verdadero (1=TP, 0=FP)
- p_i ‚àà [0, 1] = probabilidad predicha
```

**Brier Score**:
```
Brier = (1/N) * Œ£_{i=1}^N (p_i - y_i)¬≤

Donde:
- Menor es mejor (perfecta calibraci√≥n: Brier=0)
- Rango: [0, 1]
```

### 10.2 Configuraci√≥n Experimental Completa

**Hiperpar√°metros de Optimizaci√≥n**:
```python
optimization_config = {
    'method': 'L-BFGS-B',
    'bounds': [(0.01, 10.0)],
    'initial_guess': 1.0,
    'max_iterations': 100,
    'tolerance': 1e-6,
    'objective': 'negative_log_likelihood'
}
```

**Par√°metros de Inferencia**:
```python
inference_config = {
    'conf_threshold': 0.25,
    'nms_threshold': 0.65,
    'iou_matching': 0.5,
    'K_mc': 5,              # Forward passes para MC-Dropout
    'n_bins': 10,           # Bins para ECE
    'device': 'cuda',
    'seed': 42
}
```

**Dataset Splits**:
```
Total val_eval: 2,000 images
‚îú‚îÄ Calibraci√≥n: 500 images (primeras del split)
‚îÇ  ‚îî‚îÄ ~8,000 detecciones
‚îî‚îÄ Evaluaci√≥n: 1,500 images (restantes)
   ‚îî‚îÄ ~25,000 detecciones
```

### 10.3 Resultados Completos por Clase

**Temperaturas √ìptimas por Clase (Baseline)**:
```
person:         T = 2.18  (sobreconfianza moderada)
car:            T = 2.51  (sobreconfianza alta)
truck:          T = 1.89  (sobreconfianza baja)
bus:            T = 2.02  (sobreconfianza moderada)
motorcycle:     T = 1.95  (sobreconfianza baja-moderada)
bicycle:        T = 2.12  (sobreconfianza moderada)
rider:          T = 2.24  (sobreconfianza moderada-alta)
train:          T = 1.76  (sobreconfianza baja)
traffic_light:  T = 2.67  (sobreconfianza muy alta)
traffic_sign:   T = 2.45  (sobreconfianza alta)

Media: T = 2.179
Desviaci√≥n est√°ndar: 0.285
```

**Interpretaci√≥n**: Clases de se√±alizaci√≥n (traffic light, traffic sign) muestran mayor sobreconfianza, posiblemente debido a menor variabilidad visual y mayor certeza perceptual del modelo.

### 10.4 Archivos de Salida Generados

**Fase 4 (Temperature Scaling Baseline)**:
- `temperature.json`: Temperatura global optimizada
- `temperature_per_class.json`: Temperaturas por categor√≠a
- `calib_detections.csv`: 7,994 detecciones con labels TP/FP
- `eval_detections.csv`: Detecciones en val_eval
- `calibration_metrics.json`: ECE, NLL, Brier antes/despu√©s
- `reliability_diagram.png`: Visualizaci√≥n de calibraci√≥n
- `confidence_distribution.png`: Histogramas TP vs FP
- `risk_coverage.png`: Curvas de predicci√≥n selectiva

**Fase 5 (Comparaci√≥n de 6 M√©todos)**:
- `temperatures.json`: Temperaturas de los 3 m√©todos
- `detection_metrics.json`: mAP, AP50, AP75 por m√©todo
- `calibration_metrics.json`: ECE, NLL, Brier por m√©todo
- `uncertainty_auroc.json`: AUROC de discriminaci√≥n TP/FP
- `risk_coverage_auc.json`: AUC-RC por m√©todo
- `final_report.json`: Reporte consolidado completo
- `final_comparison_summary.png`: Panel visual 3√ó2
- `reliability_diagrams.png`: 6 reliability diagrams
- `risk_coverage_curves.png`: 6 curvas superpuestas
- `uncertainty_analysis.png`: Distribuciones de incertidumbre
- 6 archivos CSV: `eval_{method}.csv` con todas las predicciones

**Total**: 292 archivos generados en ambas fases

---

## Resumen Final

Este trabajo ha demostrado de manera exhaustiva que **temperature scaling es una herramienta efectiva para mejorar la calibraci√≥n en open-vocabulary detection**, con beneficios particularmente pronunciados en m√©todos single-pass (reducci√≥n de ECE del 22.5% al 41.5%). Sin embargo, la efectividad es **altamente dependiente del m√©todo de incertidumbre subyacente**, con degradaci√≥n observada en m√©todos ensemble como MC-Dropout.

La contribuci√≥n clave es el **framework sistem√°tico de evaluaci√≥n** que permite caracterizar el trade-off entre detecci√≥n, calibraci√≥n e incertidumbre, proporcionando guidance pr√°ctica para deployment en aplicaciones de seguridad cr√≠tica como ADAS.

**Recomendaci√≥n Final para RQ2**:

> "Temperature scaling mejora significativamente la calibraci√≥n de confianza en OVD (reducci√≥n de ECE de 22.5%-41.5%), pero debe aplicarse selectivamente: es altamente efectivo en m√©todos single-pass (baseline, decoder variance) pero puede degradar la calibraci√≥n en m√©todos ensemble (MC-Dropout). La mejor calibraci√≥n absoluta se alcanza con Decoder Variance + TS (ECE=0.141), mientras que la mejor detecci√≥n con MC-Dropout sin TS (mAP=0.1823). Para sistemas ADAS, se recomienda una estrategia h√≠brida que optimice ambos objetivos seg√∫n la criticidad del objeto."
